{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LkLwang99/FYP_Python_MLP_GNN_Model/blob/main/FYP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 Importing Libraries And Webscraping\n"
      ],
      "metadata": {
        "id": "L7FduxPEkl5z"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhHeCguqBZRq"
      },
      "source": [
        "##**1.1 Importing the necessary libraries on google colab**\n",
        "\n",
        "---\n",
        "The libraries that we have used is the openai,requests, pandas,numpy, beautifulsoup4 and yfinance libraries.\n",
        "The first step would be to import or install these libraries to be able to use  them in the project later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zF8aO1X9m1Ij"
      },
      "outputs": [],
      "source": [
        "### installing openai and importing openai\n",
        "!pip install -q openai\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "#regex\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "!pip install beautifulsoup4\n",
        "import requests ### using requests to make connections to apis\n",
        "from bs4 import BeautifulSoup ###\n",
        "!pip install tiktoken\n",
        "import tiktoken\n",
        "!pip install yfinance --upgrade --no-cache-dir\n",
        "import yfinance as yf\n",
        "import requests\n",
        "\n",
        "!pip install torch\n",
        "!pip install torch-geometric\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data,DataLoader\n",
        "from torch_geometric.nn import GCNConv,GATConv\n",
        "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split,KFold\n",
        "from torch.utils.data import random_split\n",
        "import itertools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nu588GWdw8uH"
      },
      "source": [
        "##**<h1>1.2 Webscraping For IPOs</h1>**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**<h2>1.2.1 IPO Data From Stockanalysis.com/ipos</h2>**\n",
        "----------------------------------------------------------\n",
        "The steps below show webscraping from the stockanalysis website with the use of beautiful soup and since the website itself separates the ipos based on years, we are able to find the ipos on years with the method below. We then store it as an np array and finally convert it to a dataframe.\n"
      ],
      "metadata": {
        "id": "SwVIK2jUkyXA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMokAN4Y715r",
        "outputId": "9c45bd9c-742e-496f-def3-5d62e1b2ea73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching data from: https://stockanalysis.com/ipos/2020/\n",
            "Fetching data from: https://stockanalysis.com/ipos/2021/\n",
            "Fetching data from: https://stockanalysis.com/ipos/2022/\n",
            "Fetching data from: https://stockanalysis.com/ipos/2023/\n",
            "Fetching data from: https://stockanalysis.com/ipos/2024/\n",
            "              Date Ticker                                    Company Name  \\\n",
            "0     Dec 29, 2020    MRM            MEDIROM Healthcare Technologies Inc.   \n",
            "1     Dec 24, 2020   VTAQ                   Ventoux CCM Acquisition Corp.   \n",
            "2     Dec 23, 2020   HCAR     Healthcare Services Acquisition Corporation   \n",
            "3     Dec 23, 2020   CFIV                         CF Acquisition Corp. IV   \n",
            "4     Dec 23, 2020    VII                         7GC & Co. Holdings Inc.   \n",
            "...            ...    ...                                             ...   \n",
            "1966  Jan 18, 2024   PSBD                  Palmer Square Capital BDC Inc.   \n",
            "1967  Jan 18, 2024   CCTG  CCSC Technology International Holdings Limited   \n",
            "1968  Jan 12, 2024   SYNX                                  Silynxcom Ltd.   \n",
            "1969  Jan 11, 2024   SDHC                       Smith Douglas Homes Corp.   \n",
            "1970   Jan 9, 2024   ROMA                      Roma Green Finance Limited   \n",
            "\n",
            "     IPO Price Current Price Price Change  \n",
            "0       $15.00         $2.74      -81.56%  \n",
            "1       $10.00         $5.08      -49.20%  \n",
            "2       $10.00        $10.11        1.10%  \n",
            "3       $10.00        $10.58        5.80%  \n",
            "4       $10.00        $10.96        9.60%  \n",
            "...        ...           ...          ...  \n",
            "1966    $16.45        $16.33       -0.79%  \n",
            "1967     $6.00         $1.60      -73.33%  \n",
            "1968     $4.00         $3.19      -18.50%  \n",
            "1969    $21.00        $33.98       64.10%  \n",
            "1970     $4.00         $0.54      -86.50%  \n",
            "\n",
            "[1971 rows x 6 columns] [['Dec 29, 2020' 'MRM' 'MEDIROM Healthcare Technologies Inc.' '$15.00'\n",
            "  '$2.74' '-81.56%']\n",
            " ['Dec 24, 2020' 'VTAQ' 'Ventoux CCM Acquisition Corp.' '$10.00' '$5.08'\n",
            "  '-49.20%']\n",
            " ['Dec 23, 2020' 'HCAR' 'Healthcare Services Acquisition Corporation'\n",
            "  '$10.00' '$10.11' '1.10%']\n",
            " ...\n",
            " ['Jan 12, 2024' 'SYNX' 'Silynxcom Ltd.' '$4.00' '$3.19' '-18.50%']\n",
            " ['Jan 11, 2024' 'SDHC' 'Smith Douglas Homes Corp.' '$21.00' '$33.98'\n",
            "  '64.10%']\n",
            " ['Jan 9, 2024' 'ROMA' 'Roma Green Finance Limited' '$4.00' '$0.54'\n",
            "  '-86.50%']]\n"
          ]
        }
      ],
      "source": [
        "base_url = \"https://stockanalysis.com/ipos/{}/\"\n",
        "response = requests.get(base_url)\n",
        "soup = BeautifulSoup(response.text,'html')\n",
        "\n",
        "start_year = 2020\n",
        "end_year = 2024\n",
        "stock_list = []\n",
        "\n",
        "\n",
        "for year in range(start_year, end_year+1):\n",
        "  url = base_url.format(year)\n",
        "  print(f\"Fetching data from: {url}\")\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text,'html.parser')\n",
        "    tbody = soup.find('tbody')\n",
        "    if tbody:\n",
        "      rows = tbody.find_all('tr')\n",
        "      for row in rows:\n",
        "        cells = row.find_all('td')\n",
        "        data = [cell.text.strip() for cell in cells]\n",
        "        if data:\n",
        "          stock_list.append(data)\n",
        "    else:\n",
        "      print(f\"No tbody found for year:{year}\")\n",
        "  else:\n",
        "    print(f\"Failed to fetch response from:{url}\")\n",
        "\n",
        "\n",
        "Stocks = np.array(stock_list,dtype=str)\n",
        "\n",
        "df = pd.DataFrame(Stocks,columns=['Date','Ticker','Company Name',\"IPO Price\",\"Current Price\",\"Price Change\"])\n",
        "\n",
        "print(df,Stocks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**<h2>1.2.2 Webscraping For IPOs using Yahoo Finance</h2>**\n",
        "\n",
        "----------------------------------------------------------\n",
        "The steps below are data retrieved from Yahoo Finance API for the Sector/Industry of the Companies.\n",
        "\n",
        "1) initalise two empty lists to and gather the company info from the yahoo api and return the sector and industry and push them into the list of sectorRow and IndustryRow. We also keep an NP Array of the data frame objects for easier processing later on if we fail to transform the dataframe or other errors occur."
      ],
      "metadata": {
        "id": "D79LyC-LQv_A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JPoz5ho_q0ug"
      },
      "outputs": [],
      "source": [
        "SectorRow = []\n",
        "IndustryRow = []\n",
        "\n",
        "for i in df['Ticker']:\n",
        "  ticker=i\n",
        "  print(ticker)\n",
        "  company = yf.Ticker(ticker)\n",
        "  company_info = company.info\n",
        "  sector = company_info.get(\"sector\",\"Sector not found\")\n",
        "  industry = company_info.get(\"industry\",\"Industry not found\")\n",
        "  SectorRow.append(sector)\n",
        "  IndustryRow.append(industry)\n",
        "print(SectorRow,IndustryRow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yE0S_gDvaGHk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2667613d-3133-4a0d-8df6-37650c54d2fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Date Ticker                                    Company Name  \\\n",
            "0     Dec 29, 2020    MRM            MEDIROM Healthcare Technologies Inc.   \n",
            "1     Dec 24, 2020   VTAQ                   Ventoux CCM Acquisition Corp.   \n",
            "2     Dec 23, 2020   HCAR     Healthcare Services Acquisition Corporation   \n",
            "3     Dec 23, 2020   CFIV                         CF Acquisition Corp. IV   \n",
            "4     Dec 23, 2020    VII                         7GC & Co. Holdings Inc.   \n",
            "...            ...    ...                                             ...   \n",
            "1966  Jan 18, 2024   PSBD                  Palmer Square Capital BDC Inc.   \n",
            "1967  Jan 18, 2024   CCTG  CCSC Technology International Holdings Limited   \n",
            "1968  Jan 12, 2024   SYNX                                  Silynxcom Ltd.   \n",
            "1969  Jan 11, 2024   SDHC                       Smith Douglas Homes Corp.   \n",
            "1970   Jan 9, 2024   ROMA                      Roma Green Finance Limited   \n",
            "\n",
            "     IPO Price Current Price Price Change             Sector  \\\n",
            "0       $15.00         $2.74      -81.56%  Consumer Cyclical   \n",
            "1       $10.00         $5.08      -49.20%   Sector not found   \n",
            "2       $10.00        $10.11        1.10%   Sector not found   \n",
            "3       $10.00        $10.58        5.80%   Sector not found   \n",
            "4       $10.00        $10.96        9.60%   Sector not found   \n",
            "...        ...           ...          ...                ...   \n",
            "1966    $16.45        $16.33       -0.79%   Sector not found   \n",
            "1967     $6.00         $1.60      -73.33%        Industrials   \n",
            "1968     $4.00         $3.19      -18.50%         Technology   \n",
            "1969    $21.00        $33.98       64.10%        Real Estate   \n",
            "1970     $4.00         $0.54      -86.50%        Industrials   \n",
            "\n",
            "                          Industry  \n",
            "0                Personal Services  \n",
            "1               Industry not found  \n",
            "2               Industry not found  \n",
            "3               Industry not found  \n",
            "4               Industry not found  \n",
            "...                            ...  \n",
            "1966            Industry not found  \n",
            "1967  Electrical Equipment & Parts  \n",
            "1968       Communication Equipment  \n",
            "1969       Real Estate—Development  \n",
            "1970           Consulting Services  \n",
            "\n",
            "[1971 rows x 8 columns]\n"
          ]
        }
      ],
      "source": [
        "df.insert(loc=len(df.columns), column='Sector', value=SectorRow)\n",
        "df.insert(loc=len(df.columns),column=\"Industry\",value=IndustryRow)\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke0E-NVN3NHu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce373862-6ae5-4465-9c7d-ad42527f4a18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Dec 29, 2020' 'MRM' 'MEDIROM Healthcare Technologies Inc.' '$15.00'\n",
            " '$2.74' '-81.56%' 'Consumer Cyclical' 'Personal Services']\n"
          ]
        }
      ],
      "source": [
        "SectorRowNPArray = np.array(SectorRow,dtype=object).reshape(-1,1)\n",
        "IndustryRowNPArray = np.array(IndustryRow,dtype=object).reshape(-1,1)\n",
        "Stocks= np.hstack((Stocks,SectorRowNPArray))\n",
        "Stocks = np.hstack((Stocks,IndustryRowNPArray))\n",
        "print(Stocks[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###****<h2>1.2.3 Webscraping For IPOs using NASDAQ API</h2>****\n",
        "\n",
        "----------------------------------------------------------\n",
        "From the codes below, it shows an extraction of the NASDAQ API to ensure that the data we have retrieved is true and also double check that the IPO_Price is true as Nasdaq is the stock exchange which listed the stock and is trustable. After which, we then use the Yahoo Finance API to find out the stocks' total asset, equities and earnings/losses."
      ],
      "metadata": {
        "id": "QE3tKAFYRcPW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EpyGswAIx5LB"
      },
      "outputs": [],
      "source": [
        "\n",
        "url = 'https://api.nasdaq.com/api/ipo/calendar'\n",
        "headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/113.0'}\n",
        "start_date = '2020-1-1'\n",
        "end_date = '2023-5-31'\n",
        "periods = pd.period_range(start_date, end_date, freq='M')\n",
        "dfs = []\n",
        "for period in periods:\n",
        "    data = requests.get(url, headers=headers, params={'date': period}).json()\n",
        "    df2 = pd.json_normalize(data['data']['priced'], 'rows')\n",
        "    dfs.append(df2)\n",
        "df2 = pd.concat(dfs, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtNwt6qC01to",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e33d31c-97bc-41c5-dd2d-b9ae2f3f3cd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              dealID proposedTickerSymbol  \\\n",
            "0      1099162-91392                 ARQT   \n",
            "1      1101484-91092                 REYN   \n",
            "2       748340-91387                 ONEM   \n",
            "3      1099989-90949                 ANPC   \n",
            "4      1019150-91385                 BDTX   \n",
            "...              ...                  ...   \n",
            "1772  1210259-102635                  SGE   \n",
            "1773  1239799-104989                ALCYU   \n",
            "1774  1254469-106197                 SLRN   \n",
            "1775  1243360-105271                 KVUE   \n",
            "1776  1190851-101486                GODNU   \n",
            "\n",
            "                                 companyName      proposedExchange  \\\n",
            "0              Arcutis Biotherapeutics, Inc.  NASDAQ Global Select   \n",
            "1            Reynolds Consumer Products Inc.  NASDAQ Global Select   \n",
            "2                       1life Healthcare Inc  NASDAQ Global Select   \n",
            "3                           Fresh2 Group Ltd         NASDAQ Global   \n",
            "4           Black Diamond Therapeutics, Inc.  NASDAQ Global Select   \n",
            "...                                      ...                   ...   \n",
            "1772       Strong Global Entertainment, Inc.              NYSE MKT   \n",
            "1773  Alchemy Investments Acquisition Corp 1         NASDAQ Global   \n",
            "1774                          ACELYRIN, Inc.  NASDAQ Global Select   \n",
            "1775                             Kenvue Inc.                  NYSE   \n",
            "1776            Golden Star Acquisition Corp         NASDAQ Global   \n",
            "\n",
            "     proposedSharePrice sharesOffered pricedDate dollarValueOfSharesOffered  \\\n",
            "0                 17.00     9,375,000  1/31/2020               $159,375,000   \n",
            "1                 26.00    47,170,000  1/31/2020             $1,226,420,000   \n",
            "2                 14.00    17,500,000  1/31/2020               $245,000,000   \n",
            "3                 12.00     1,333,360  1/30/2020                $16,000,320   \n",
            "4                 19.00    10,586,316  1/30/2020               $201,140,004   \n",
            "...                 ...           ...        ...                        ...   \n",
            "1772               4.00     1,000,000  5/16/2023                 $4,000,000   \n",
            "1773              10.00    10,000,000  5/05/2023               $100,000,000   \n",
            "1774              18.00    30,000,000  5/05/2023               $540,000,000   \n",
            "1775              22.00   172,812,560  5/04/2023             $3,801,876,320   \n",
            "1776              10.00     6,000,000  5/02/2023                $60,000,000   \n",
            "\n",
            "     dealStatus  \n",
            "0        Priced  \n",
            "1        Priced  \n",
            "2        Priced  \n",
            "3        Priced  \n",
            "4        Priced  \n",
            "...         ...  \n",
            "1772     Priced  \n",
            "1773     Priced  \n",
            "1774     Priced  \n",
            "1775     Priced  \n",
            "1776     Priced  \n",
            "\n",
            "[1777 rows x 9 columns]\n",
            "Empty DataFrame\n",
            "Columns: [dealID, proposedTickerSymbol, companyName, proposedExchange, proposedSharePrice, sharesOffered, pricedDate, dollarValueOfSharesOffered, dealStatus]\n",
            "Index: []\n",
            "            dealID proposedTickerSymbol                           companyName  \\\n",
            "403  1135217-94816                  MRM  Medirom Healthcare Technologies Inc.   \n",
            "\n",
            "    proposedExchange proposedSharePrice sharesOffered  pricedDate  \\\n",
            "403   NASDAQ Capital              15.00       800,000  12/29/2020   \n",
            "\n",
            "    dollarValueOfSharesOffered dealStatus  \n",
            "403                $12,000,000     Priced  \n",
            "False\n",
            "True\n",
            "15.00\n",
            "            Date Ticker                  Company Name IPO Price Current Price  \\\n",
            "28  Dec 15, 2020   BLUW  Blue Water Acquisition Corp.    $10.00         $9.63   \n",
            "\n",
            "   Price Change            Sector            Industry  \n",
            "28       -3.70%  Sector not found  Industry not found  \n"
          ]
        }
      ],
      "source": [
        "print(df2)\n",
        "\n",
        "mrm_row = df2[df2['proposedTickerSymbol'] == 'MRM']\n",
        "mrm_row_price = mrm_row['proposedSharePrice']\n",
        "VTAQ_row = df2[df2['proposedTickerSymbol']==\"VTAQ\"]\n",
        "\n",
        "print(VTAQ_row)\n",
        "print(mrm_row)\n",
        "print(mrm_row.empty)\n",
        "print(VTAQ_row.empty)\n",
        "print(mrm_row_price.values[0])\n",
        "print(df.loc[df['Ticker'] == 'BLUW'])\n",
        "df.loc[30,\"Ticker\"]=\"BLUW\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWQ-mCjTsJ7M"
      },
      "outputs": [],
      "source": [
        "df['sharesOffered'] = \"0\"\n",
        "for ticker in df['Ticker']:\n",
        "  ticker_row = df2[df2['proposedTickerSymbol'] == ticker]\n",
        "  if not ticker_row.empty:\n",
        "    ticker_row_price = ticker_row['proposedSharePrice']\n",
        "    ticker_sharesOffered = ticker_row['sharesOffered']\n",
        "    df.loc[df['Ticker'] == ticker, 'IPO Price'] = ticker_row_price.values[0]\n",
        "    df.loc[df['Ticker'] == ticker,'sharesOffered'] = ticker_sharesOffered.values[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyxkLIlTpsqY"
      },
      "outputs": [],
      "source": [
        "df['TotalRevenue']= 0\n",
        "\n",
        "for i in df['Ticker']:\n",
        "  ticker=i\n",
        "  last_position = -1\n",
        "  company = yf.Ticker(ticker)\n",
        "  Financials = company.financials\n",
        "  if Financials.empty:\n",
        "    print(f\"No financial data found for {ticker}\")\n",
        "    continue\n",
        "  if 'Total Revenue' in Financials.index:\n",
        "    while math.isnan(Financials.loc[\"Total Revenue\"][last_position]):\n",
        "      last_position -= 1\n",
        "    total_revenue_row = Financials.loc[\"Total Revenue\"].iloc[last_position]\n",
        "    df.loc[df['Ticker'] == ticker, 'TotalRevenue'] = '{:,.0f}'.format(total_revenue_row)\n",
        "  else:\n",
        "    print(f\"'Total Revenue' not found for {ticker}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVLQQ3vw3sjb"
      },
      "outputs": [],
      "source": [
        "df['TotalAssets']= 0\n",
        "\n",
        "for i in df['Ticker']:\n",
        "  ticker=i\n",
        "  last_position = -1\n",
        "  company = yf.Ticker(ticker)\n",
        "  Financials = company.balancesheet\n",
        "  if Financials.empty:\n",
        "    print(f\"No financial data found for {ticker}\")\n",
        "    continue\n",
        "  if 'Cash And Cash Equivalents' in Financials.index:\n",
        "    while math.isnan(Financials.loc[\"Cash And Cash Equivalents\"][last_position]):\n",
        "      last_position -= 1\n",
        "    total_assets_row = Financials.loc[\"Cash And Cash Equivalents\"].iloc[last_position]\n",
        "    df.loc[df['Ticker'] == ticker, 'TotalAssets'] = '{:,.0f}'.format(total_assets_row)\n",
        "  else:\n",
        "    print(f\"'Total Revenue' not found for {ticker}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fu56hsZsSKTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf693ea-5f55-45cb-f670-df9157d20c2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Date Ticker                                    Company Name  \\\n",
            "0     Dec 29, 2020    MRM            MEDIROM Healthcare Technologies Inc.   \n",
            "1     Dec 24, 2020   VTAQ                   Ventoux CCM Acquisition Corp.   \n",
            "2     Dec 23, 2020   HCAR     Healthcare Services Acquisition Corporation   \n",
            "3     Dec 23, 2020   CFIV                         CF Acquisition Corp. IV   \n",
            "4     Dec 23, 2020    VII                         7GC & Co. Holdings Inc.   \n",
            "...            ...    ...                                             ...   \n",
            "1966  Jan 18, 2024   PSBD                  Palmer Square Capital BDC Inc.   \n",
            "1967  Jan 18, 2024   CCTG  CCSC Technology International Holdings Limited   \n",
            "1968  Jan 12, 2024   SYNX                                  Silynxcom Ltd.   \n",
            "1969  Jan 11, 2024   SDHC                       Smith Douglas Homes Corp.   \n",
            "1970   Jan 9, 2024   ROMA                      Roma Green Finance Limited   \n",
            "\n",
            "     IPO Price Current Price Price Change             Sector  \\\n",
            "0        15.00         $2.74      -81.56%  Consumer Cyclical   \n",
            "1       $10.00         $5.08      -49.20%   Sector not found   \n",
            "2       $10.00        $10.11        1.10%   Sector not found   \n",
            "3       $10.00        $10.58        5.80%   Sector not found   \n",
            "4       $10.00        $10.96        9.60%   Sector not found   \n",
            "...        ...           ...          ...                ...   \n",
            "1966    $16.45        $16.33       -0.79%   Sector not found   \n",
            "1967     $6.00         $1.60      -73.33%        Industrials   \n",
            "1968     $4.00         $3.19      -18.50%         Technology   \n",
            "1969    $21.00        $33.98       64.10%        Real Estate   \n",
            "1970     $4.00         $0.54      -86.50%        Industrials   \n",
            "\n",
            "                          Industry sharesOffered   TotalRevenue    TotalAssets  \n",
            "0                Personal Services       800,000  3,341,617,000  1,439,733,000  \n",
            "1               Industry not found             0              0              0  \n",
            "2               Industry not found             0              0              0  \n",
            "3               Industry not found             0              0              0  \n",
            "4               Industry not found             0              0              0  \n",
            "...                            ...           ...            ...            ...  \n",
            "1966            Industry not found             0     20,924,886        682,579  \n",
            "1967  Electrical Equipment & Parts             0     22,608,447      2,642,918  \n",
            "1968       Communication Equipment             0      9,581,000      1,561,000  \n",
            "1969       Real Estate—Development             0    518,863,000     25,340,000  \n",
            "1970           Consulting Services             0     13,677,261        394,471  \n",
            "\n",
            "[1971 rows x 11 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df)\n",
        "\n",
        "#### saving the dataframe as a csv file\n",
        "df.to_csv('dataset.csv',\n",
        "    index=False,\n",
        "    sep=',',\n",
        "    header=True,\n",
        "    na_rep='NA')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2<h1> Generating Synthetic Data</h1>"
      ],
      "metadata": {
        "id": "ZYTM869GlEjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##****<h2>2.1 Feeding the data retrieved to GPT API</h2>****\n",
        "\n",
        "----------------------------------------------------------\n",
        "Now that we have the full set of Data Frame of IPO Prices and companies dated from 2020-2024, we want to ensure that we can send them as a list to GPT so that GPT can produce synthetic data with  similar statistic properties. We have to send them as a message content which we need to process the original dataframe which splits up into several strings so that gpt can receive these messages. The reason for splitting is  so that we don't exceed the tokens which the gpt api can process. Therefore, the steps below shows how we split the company_data into lists of objects and finally creating a single string array which consists of hundreds of the companies' data."
      ],
      "metadata": {
        "id": "Aqq_QRdHSi89"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVu7w-Orj9kF"
      },
      "outputs": [],
      "source": [
        "company_data = list(df.itertuples(index=False, name=None))\n",
        "num_lists = 6\n",
        "## converting the company details to names\n",
        "full_texts= []\n",
        "\n",
        "for i in company_data:\n",
        "  joined_text = \",\".join(map(str,i))+\"\\n\"\n",
        "  full_texts.append(joined_text)\n",
        "\n",
        "\n",
        "list_length = len(company_data)\n",
        "chunk_size = math.ceil(list_length/num_lists)\n",
        "\n",
        "### splitting the list of companies' details to 6 different list to parse into gpt to minimize the size when feeding into gpt\n",
        "split_lists = []\n",
        "for i in range(num_lists):\n",
        "  start = i*chunk_size\n",
        "  end = (i+1) * chunk_size\n",
        "  split_lists.append(full_texts[start:end])\n",
        "\n",
        "single_string_array = []\n",
        "for i in range(len(split_lists)):\n",
        "  single_string = \" \".join(split_lists[i])\n",
        "  single_string_array.append(single_string)\n",
        "\n",
        "single_string_array_combined = single_string_array[0]+single_string_array[1]+single_string_array[2]+single_string_array[3]+single_string_array[4]+single_string_array[5]\n",
        "\n",
        "#print(\"can you provide similar examples of the company in the following format Dates,Ticker,Full Name Of Company, IPO Price, Current Price, Percentage Increase,Industry,Sector,Shares Offered,Total Revenue, Total Asset which is seen in the examples below: \\n\"+single_string_array[0]+\" and return it with no explanation and separate each company without numbering it and no spaces inbetween.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuVrynJtndyB"
      },
      "outputs": [],
      "source": [
        "encoding = tiktoken.encoding_for_model(\"gpt-4o-latest\")\n",
        "\n",
        "client = OpenAI(\n",
        "    # This is the default and can be omitted\n",
        "    api_key='Your_API_Key',\n",
        ")\n",
        "\n",
        "syntheticDataset =[]\n",
        "not_done = True\n",
        "i = 0\n",
        "\n",
        "while not_done or len(syntheticDataset)<=10000 :\n",
        "  message_content = (\n",
        "    \"I will now feed you 389 data points about these companies' Financials and IPO Data. \"\n",
        "    \"Can you provide me a synthetic dataset with 2,000 datapoints of similar properties in comparison with these companies in the following format: \"\n",
        "    \"1) Dates, 2) Ticker, 3) Full Name Of Company, 4) IPO Price, 5) Current Price, 6) Percentage Increase, 7) Industry, 8) Sector, 9) Shares Offered, \"\n",
        "    \"10) Total Revenue with $ in the beginning, 11) Total Asset with $ in the beginning. Please use the examples provided below: \\n\" + single_string_array_combined +\n",
        "    \". Return the synthetic dataset with no explanations and separate each company without numbering and without spaces in between. \"\n",
        "    \"The companies were webscraped, and some information might not be complete, such as total revenue, total assets, and earnings, which may not be accurate. \"\n",
        "    \"If any values are 0, replace them with values that are suitable for the industry. Return the synthetic dataset without any other explanations. \"\n",
        "    \"First, return the number of data points at the start that have been fed to you, then return the synthetic dataset that you have generated. \"\n",
        "    \"Finally, return strictly '-notdone' at the end if you have not finished generating the synthetic dataset.\"\n",
        ")\n",
        "  print(f\"Processing chunk {i}, current synthetic dataset length: {len(syntheticDataset)}\")\n",
        "  chat_completion = client.chat.completions.create(\n",
        "      max_tokens=16384,\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": message_content\n",
        "          }\n",
        "      ],\n",
        "      model=\"chatgpt-4o-latest\",\n",
        "    )\n",
        "  print(chat_completion.choices[0].message.content)\n",
        "  crd = chat_completion.choices[0].message.content\n",
        "  if '-notdone' in crd:\n",
        "        not_done = True\n",
        "        crd = crd.replace('-notdone','').strip()\n",
        "  else:\n",
        "      not_done = False\n",
        "  if i == 6:\n",
        "    i = -1\n",
        "  i += 1\n",
        "  finalchatcompletion = crd.split('\\n')\n",
        "  syntheticDataset.extend(finalchatcompletion)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##****<h2>2.2 Retrieving Data and Conversion to Dataframe</h2>****\n",
        "\n",
        "----------------------------------------------------------\n",
        "After we retrieve the full dataset as the variable called syntheticDataset, we need to remove some items which are outliers as they are not returned in the format which we have specified to the GPT API. Therefore, the data cleaning process begins from this section onwards which i found the KLDQ contains weird formatting and removed it from the dataframe and we use regex to split them into objects."
      ],
      "metadata": {
        "id": "VqaRP6SmTpRl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa213-ov7gAu",
        "outputId": "c295c5d8-33f6-42a8-8a99-abb5f1f704c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10094\n",
            "9289\n",
            "9289\n"
          ]
        }
      ],
      "source": [
        "print(len(syntheticDataset))\n",
        "date_pattern = r'^[A-Za-z]{3} \\d{1,2}, \\d{4}'\n",
        "filtered_syntheticDataset = [item for item in syntheticDataset if re.match(date_pattern, item)]\n",
        "print(len(filtered_syntheticDataset))\n",
        "filtered_syntheticDataset = [item for item in filtered_syntheticDataset if \"KLDQ\" not in item]\n",
        "print(len(filtered_syntheticDataset))\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##****<h2>2.3 Data Cleaning</h2>****\n",
        "\n",
        "----------------------------------------------------------\n",
        "These steps below are the ones which i have used to split them into lists of the companies which we use regex to split them via \",\" and other factors such as the dollarsign to retrieve the individual properties of the company and then finally merging these lists[0] up to create a company object. We then have the full lists of companies with 11 properties and saving them as a csv file so we do not have to incur more costs when reloading the scripts above which uses the gpt api to generate synthetic data again."
      ],
      "metadata": {
        "id": "2P1T4chsULFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processing_syntheticDatasetDate = []\n",
        "processing_syntheticDatasetTicker = []\n",
        "processing_syntheticDatasetName= []\n",
        "processing_IPO_Price = []\n",
        "processing_IPO_Current_Price = []\n",
        "processing_Percentage_Increase = []\n",
        "processing_Industry = []\n",
        "processing_Sector = []\n",
        "processing_Shares_Offered = []\n",
        "processing_Total_Revenue = []\n",
        "processing_Total_Asset = []\n",
        "\n",
        "\n",
        "for data in filtered_syntheticDataset:\n",
        "  # Step 1: Split the string by the first comma to separate the date\n",
        "  first_split = data.split(',', 1)\n",
        "\n",
        "  combined_date = first_split[0] + ',' + first_split[1].split(',', 1)[0]\n",
        "\n",
        "  remaining_string = first_split[1].split(',', 1)[1]\n",
        "\n",
        "  remaining_parts = remaining_string.split(',')\n",
        "\n",
        "  dollarsignsplit_dataparts = data.rsplit('$', 2)\n",
        "\n",
        "  split_parts_totalShares = dollarsignsplit_dataparts[0].split(',')\n",
        "  parts_after_ninth = split_parts_totalShares[9:]\n",
        "  total_shares = ','.join(parts_after_ninth)\n",
        "  reverse_string = dollarsignsplit_dataparts[0][::-1]\n",
        "  match = re.search(r'[A-Za-z]', reverse_string)\n",
        "  if match:\n",
        "    position = len(dollarsignsplit_dataparts[0]) - match.start() + 1\n",
        "\n",
        "    left_part = dollarsignsplit_dataparts[0][:position]\n",
        "    right_part = dollarsignsplit_dataparts[0][position:]\n",
        "\n",
        "\n",
        "  else:\n",
        "    print(\"No alphabet character found in the string.\")\n",
        "\n",
        "  right_part = right_part.replace(\" \",\"\")\n",
        "  right_part = right_part.replace(\".\",\"\")\n",
        "  right_part = right_part.replace(\",\",\"\")\n",
        "  right_part = right_part.replace(\"$\",\"\")\n",
        "  total_shares=right_part\n",
        "\n",
        "  if len(remaining_parts)>=7 and len(dollarsignsplit_dataparts)==3 and len(total_shares)>=5:\n",
        "        split_parts = [combined_date, remaining_parts]\n",
        "        processing_syntheticDatasetDate.append(combined_date)\n",
        "        processing_syntheticDatasetTicker.append(remaining_parts[0])\n",
        "        processing_syntheticDatasetName.append(remaining_parts[1])\n",
        "        processing_IPO_Price.append(remaining_parts[2])\n",
        "        processing_IPO_Current_Price.append(remaining_parts[3])\n",
        "        processing_Percentage_Increase.append(remaining_parts[4])\n",
        "        processing_Industry.append(remaining_parts[5])\n",
        "        processing_Sector.append(remaining_parts[6])\n",
        "        processing_Total_Revenue.append(\"$\"+dollarsignsplit_dataparts[1])\n",
        "        processing_Total_Asset.append(\"$\"+dollarsignsplit_dataparts[2])\n",
        "        processing_Shares_Offered.append(total_shares)\n",
        "\n",
        "\n",
        "\n",
        "df_syntheticDataset = pd.DataFrame({\n",
        "    'Date': processing_syntheticDatasetDate,\n",
        "    'Ticker': processing_syntheticDatasetTicker,\n",
        "    'Company_Name': processing_syntheticDatasetName,\n",
        "    'IPO_Price': processing_IPO_Price,\n",
        "    'Current_Price': processing_IPO_Current_Price,\n",
        "    'Percentage_Increase': processing_Percentage_Increase,\n",
        "    'Industry': processing_Industry,\n",
        "    'Sector': processing_Sector,\n",
        "    'Shares_Offered': processing_Shares_Offered,\n",
        "    'Total_Revenue': processing_Total_Revenue,\n",
        "    'Total_Asset': processing_Total_Asset\n",
        "})\n",
        "\n",
        "print(df_syntheticDataset)\n",
        "# Save DataFrame to CSV\n",
        "df_syntheticDataset.to_csv('synthetic_dataset_Final.csv', index=False)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SRVioZTLzGYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.3.1 Further Data Cleaning\n",
        "\n",
        "\n",
        "----------------------------------------------------------\n",
        "Further Data Cleaning so that the information feeded to the machine learning model can interpret what information we have produced based on the data that GPT has created for us. The steps below show the removal of text words in the columns which should not consist of text words and also punctuations and other not required elements.\n",
        "\n",
        "Note:\n",
        "Because i'm using google colab, sometimes it disconnect and i have to read the csv file from save as when it disconnects, all the variables are lost."
      ],
      "metadata": {
        "id": "KnzwRxmnUtof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/synthetic_dataset_Final (1).csv')\n",
        "\n",
        "## upon inspecting the initial csv file in the csv, i have found the chatgpt outputs to have some errors in some runs when the gpt api returned a response\n",
        "## to avoid errors later on, we remove some of the rows that contain the error as sector and industry should\n",
        "## only consist of alphabets instead of numbers so we use regex to find if they contain numbers.\n",
        "## we then clean the IPO_Price as a start to clean the data and convert them to floats.\n",
        "\n",
        "\n",
        "## removing rows that has numbers in the sector column\n",
        "df_cleaned = df[~df['Sector'].str.contains(r'\\d')]\n",
        "## saving and viewing to see if the rows have been removed.\n",
        "df_cleaned.to_csv('synthetic_dataset_cleanedFinal.csv', index=False)\n",
        "\n",
        "##removing rows that has numbers in the industry column\n",
        "df_cleaned = df_cleaned[~df_cleaned['Industry'].str.contains(r'\\d')]\n",
        "\n",
        "## saving and viewing to see if the rows have been removed\n",
        "df_cleaned.to_csv('synthetic_dataset_cleaned2Final.csv', index=False)\n",
        "\n",
        "\n",
        "##replacing the rows' string to ensure.E.g($,alphabets) that it only consist of numbers\n",
        "df_cleaned['IPO_Price'] = df_cleaned[\"IPO_Price\"].str.replace(r'[^0-9.]','',regex=True)\n",
        "#replacing empty as nan\n",
        "df_cleaned['IPO_Price'] = df_cleaned['IPO_Price'].replace('', np.nan)\n",
        "#changing the column to number instead of string\n",
        "df_cleaned['IPO_Price'] = pd.to_numeric(df_cleaned['IPO_Price'], errors='coerce')\n",
        "\n",
        "### After we get the IPO_Price, the next thing we want to check is the shares_offered, TOtal_Revenue,Total_assets to ensure that our machine learning model works.\n",
        "df_cleaned['Shares_Offered'] = df_cleaned['Shares_Offered'].str.replace(r'\\D+', '', regex=True)\n",
        "df_cleaned['Shares_Offered'] = df_cleaned['Shares_Offered'].replace('', np.nan)\n",
        "\n",
        "\n",
        "\n",
        "df_cleaned = df_cleaned.dropna(subset=['Shares_Offered'])\n",
        "print(df_cleaned[df_cleaned['Shares_Offered'].isnull()])\n",
        "\n",
        "\n",
        "\n",
        "print(\"=================shares_offered Cleaned========================\\n\")\n",
        "\n",
        "df_cleaned['Total_Revenue'] = df_cleaned[\"Total_Revenue\"].str.replace(r'[^0-9.]','',regex=True)\n",
        "df_cleaned['Total_Revenue'] = df_cleaned['Total_Revenue'].replace('', np.nan)\n",
        "\n",
        "print(df_cleaned[df_cleaned['Total_Revenue'].isnull()])\n",
        "df_cleaned = df_cleaned.dropna(subset=['Total_Revenue'])\n",
        "print(df_cleaned[df_cleaned['Total_Revenue'].isnull()])\n",
        "print(\"=================Total_Revenue Cleaned========================\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_cleaned['Total_Asset'] = df_cleaned[\"Total_Asset\"].str.replace(r'[^0-9.]','',regex=True)\n",
        "df_cleaned['Total_Asset'] = df_cleaned['Total_Asset'].replace('', np.nan)\n",
        "print(df_cleaned[df_cleaned['Total_Asset'].isnull()])\n",
        "df_cleaned = df_cleaned.dropna(subset=['Total_Asset'])\n",
        "print(df_cleaned[df_cleaned['Total_Revenue'].isnull()])\n",
        "print(\"=================Total_Asset Cleaned========================\\n\")\n",
        "\n",
        "print(df_cleaned)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3seyRLHUT6Y",
        "outputId": "21c37087-6dee-4c43-eebd-9dbe92832f68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Date, Ticker, Company_Name, IPO_Price, Current_Price, Percentage_Increase, Industry, Sector, Shares_Offered, Total_Revenue, Total_Asset]\n",
            "Index: []\n",
            "=================shares_offered Cleaned========================\n",
            "\n",
            "              Date Ticker                 Company_Name  IPO_Price  \\\n",
            "6176  Jun 12, 2023    FRX       Fraxon Pharmaceuticals       18.0   \n",
            "6189  Jul 08, 2024    NXG       Nexius Pharmaceuticals       14.0   \n",
            "6190  Apr 13, 2023    STR  Stratagenix Pharmaceuticals       20.0   \n",
            "6191  Nov 21, 2024    ZYL       Zylant Pharmaceuticals       19.0   \n",
            "6192  Sep 17, 2024    BIH       BioHealth Technologies       12.0   \n",
            "...            ...    ...                          ...        ...   \n",
            "6253  Apr 04, 2023    EXZ     ExonLabs Pharmaceuticals       25.0   \n",
            "6254  Jan 15, 2024    SXP       Synpro Pharmaceuticals       18.0   \n",
            "6255  Aug 27, 2024    NXT      NexRite Pharmaceuticals       16.0   \n",
            "6256  Dec 16, 2024    MLG     Melgenix Pharmaceuticals       17.5   \n",
            "6257  Sep 28, 2023    LGT        Ligen Pharmaceuticals       12.0   \n",
            "\n",
            "     Current_Price Percentage_Increase    Industry         Sector  \\\n",
            "6176         $9.33             -48.17%  Healthcare  Biotechnology   \n",
            "6189        $23.22              65.86%  Healthcare  Biotechnology   \n",
            "6190        $17.31             -13.45%  Healthcare  Biotechnology   \n",
            "6191        $22.99              21.00%  Healthcare  Biotechnology   \n",
            "6192         $7.55             -37.08%  Healthcare  Biotechnology   \n",
            "...            ...                 ...         ...            ...   \n",
            "6253        $12.44             -50.24%  Healthcare  Biotechnology   \n",
            "6254        $10.22             -43.22%  Healthcare  Biotechnology   \n",
            "6255        $10.89             -31.94%  Healthcare  Biotechnology   \n",
            "6256        $24.12              37.83%  Healthcare  Biotechnology   \n",
            "6257         $9.78             -18.50%  Healthcare  Biotechnology   \n",
            "\n",
            "         Shares_Offered Total_Revenue     Total_Asset  \n",
            "6176   5800000130000000           NaN   $89,000,000    \n",
            "6189  12500000598000000           NaN  $450,000,000    \n",
            "6190   9400000630000000           NaN  $280,000,000    \n",
            "6191  11100000360000000           NaN  $285,000,000    \n",
            "6192   7250000200000000           NaN   $90,000,000    \n",
            "...                 ...           ...             ...  \n",
            "6253   6440000280000000           NaN  $190,000,000    \n",
            "6254   8600000280000000           NaN  $190,000,000    \n",
            "6255   7200000230000000           NaN  $120,000,000    \n",
            "6256   8550000350000000           NaN  $245,000,000    \n",
            "6257   5700000268000000           NaN  $184,000,000    \n",
            "\n",
            "[66 rows x 11 columns]\n",
            "Empty DataFrame\n",
            "Columns: [Date, Ticker, Company_Name, IPO_Price, Current_Price, Percentage_Increase, Industry, Sector, Shares_Offered, Total_Revenue, Total_Asset]\n",
            "Index: []\n",
            "=================Total_Revenue Cleaned========================\n",
            "\n",
            "              Date Ticker             Company_Name  IPO_Price Current_Price  \\\n",
            "6174  May 28, 2024    MTR   Metron Pharmaceuticals       15.0        $21.18   \n",
            "6182  May 31, 2024    KMT  Kimatro Pharmaceuticals       16.0        $12.46   \n",
            "6183  Aug 29, 2024    CGX  Cygenix Pharmaceuticals       13.0        $15.57   \n",
            "6187  Feb 26, 2024    TSF  Trusfar Pharmaceuticals       18.0         $9.43   \n",
            "6188  May 13, 2024    CYT  Cytogen Pharmaceuticals       14.0         $9.32   \n",
            "\n",
            "     Percentage_Increase    Industry         Sector    Shares_Offered  \\\n",
            "6174              41.20%  Healthcare  Biotechnology  8500000320000000   \n",
            "6182             -22.13%  Healthcare  Biotechnology  8900000330000000   \n",
            "6183              19.77%  Healthcare  Biotechnology  5400000280000000   \n",
            "6187             -47.61%  Healthcare  Biotechnology  9840000500000000   \n",
            "6188             -33.43%  Healthcare  Biotechnology  8000000240000000   \n",
            "\n",
            "     Total_Revenue Total_Asset  \n",
            "6174     192000000         NaN  \n",
            "6182     189000000         NaN  \n",
            "6183     135000000         NaN  \n",
            "6187     234000000         NaN  \n",
            "6188      98000000         NaN  \n",
            "Empty DataFrame\n",
            "Columns: [Date, Ticker, Company_Name, IPO_Price, Current_Price, Percentage_Increase, Industry, Sector, Shares_Offered, Total_Revenue, Total_Asset]\n",
            "Index: []\n",
            "=================Total_Asset Cleaned========================\n",
            "\n",
            "              Date Ticker                  Company_Name  IPO_Price  \\\n",
            "0     Jan 10, 2022   FJMS  First Financial Holdings Ltd      20.00   \n",
            "1      Feb 8, 2021   FSTD             First Studio Inc.      25.00   \n",
            "2     Mar 15, 2021   MJRX           Medica Therapeutics       7.00   \n",
            "3     Apr 14, 2020   HNMT  Harvest Natural Medical Tech      18.00   \n",
            "4     May 22, 2020   EENG                  Energic Inc.      12.00   \n",
            "...            ...    ...                           ...        ...   \n",
            "8701   Oct 5, 2023   YKSR          Yakser Holdings Ltd.      13.50   \n",
            "8702   Oct 4, 2023   XLLT                  Xellent Ltd.      20.50   \n",
            "8703   Oct 3, 2023   NNCX       Nanocore Biopharma Ltd.      15.75   \n",
            "8704   Oct 2, 2023   VRGT                  Verigut Inc.      12.25   \n",
            "8705   Oct 1, 2023   CLRX   Clarix Pharmaceuticals Ltd.      14.25   \n",
            "\n",
            "     Current_Price Percentage_Increase                Industry  \\\n",
            "0           $20.48               2.40%      Financial Services   \n",
            "1           $76.65             206.60%  Communication Services   \n",
            "2           $13.35              90.71%              Healthcare   \n",
            "3           $24.11              33.95%              Healthcare   \n",
            "4           $38.45             220.42%                  Energy   \n",
            "...            ...                 ...                     ...   \n",
            "8701        $15.75              16.67%             Industrials   \n",
            "8702        $24.20              18.05%              Technology   \n",
            "8703        $14.50              -7.94%              Healthcare   \n",
            "8704        $11.50              -6.12%              Technology   \n",
            "8705        $12.75             -10.53%              Healthcare   \n",
            "\n",
            "                              Sector Shares_Offered Total_Revenue Total_Asset  \n",
            "0                    Shell Companies       10000000    1050505500   622732000  \n",
            "1                      Entertainment        3500000       5300000     2000000  \n",
            "2                      Biotechnology        4800000     145600000    73200000  \n",
            "3                    Medical Devices        5000000     456000000   289700000  \n",
            "4     Oil & Gas Equipment & Services        6250000     820131000   392876000  \n",
            "...                              ...            ...           ...         ...  \n",
            "8701      Engineering & Construction       17500000     400000000   340000000  \n",
            "8702         Software—Infrastructure       11250000     145000000   110000000  \n",
            "8703                   Biotechnology       10500000     120000000    90000000  \n",
            "8704            Software—Application        9500000      65000000    42000000  \n",
            "8705                   Biotechnology       12000000     100000000    78000000  \n",
            "\n",
            "[7263 rows x 11 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2.4 Label Encoding And Scaling\n",
        "\n",
        "----------------------------------------------------------\n",
        "Label encoding uses the label encoder object which we have imported from the library to encode categorical values into numerical values so that the machine learning model can better interpret these categories and scaling standardizes numerical data to ensure features contribute equally to the model."
      ],
      "metadata": {
        "id": "xFez9lRIl0t_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "scaler = StandardScaler()\n",
        "\n",
        "###using Label_encoder to transform text features into numerical features\n",
        "df2 = df_cleaned.copy()\n",
        "df2['Sector_encoded'] = label_encoder.fit_transform(df_cleaned['Sector'])\n",
        "df2['Industry_encoded'] = label_encoder.fit_transform(df_cleaned['Industry'])\n",
        "\n",
        "df2['Shares_Offered_encoded'] = df2['Shares_Offered'].astype(int)\n",
        "lower_percentile = df2['Shares_Offered_encoded'].quantile(0.01)\n",
        "upper_percentile = df2['Shares_Offered_encoded'].quantile(0.99)\n",
        "###removing extreme outliers\n",
        "df2 = df2[(df2['Shares_Offered_encoded'] >= lower_percentile) & (df2['Shares_Offered_encoded'] <= upper_percentile)]\n",
        "df2['Shares_Offered_encoded'] = scaler.fit_transform(df2[['Shares_Offered_encoded']])\n",
        "\n",
        "##experienced an error where some of it isn't int and total revenue should be in int as it is whole numbers. Therefore, we remove all that isn't int\n",
        "df2['Total_Revenue_encoded'] = pd.to_numeric(df2['Total_Revenue'], errors='coerce')\n",
        "df2 = df2[df2['Total_Revenue_encoded'] == df2['Total_Revenue_encoded'].astype(int)]\n",
        "df2['Total_Revenue_encoded'] = df2['Total_Revenue_encoded'].astype(int)\n",
        "df2['Total_Revenue_encoded'] = scaler.fit_transform(df2[['Total_Revenue_encoded']])\n",
        "\n",
        "\n",
        "\n",
        "df2['Total_Asset_encoded'] = df2['Total_Asset'].astype(int)\n",
        "df2['Total_Asset_encoded'] = scaler.fit_transform(df2[['Total_Asset_encoded']])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df2 = df2.dropna(subset=['IPO_Price'])\n",
        "\n",
        "print(df2)\n",
        "print(df2['IPO_Price'].min())\n",
        "print(df2['IPO_Price'].max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoVTJ_JvqNZf",
        "outputId": "74164d60-fd9f-47d5-8907-35566862dee6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              Date Ticker                  Company_Name  IPO_Price  \\\n",
            "0     Jan 10, 2022   FJMS  First Financial Holdings Ltd      20.00   \n",
            "1      Feb 8, 2021   FSTD             First Studio Inc.      25.00   \n",
            "2     Mar 15, 2021   MJRX           Medica Therapeutics       7.00   \n",
            "3     Apr 14, 2020   HNMT  Harvest Natural Medical Tech      18.00   \n",
            "4     May 22, 2020   EENG                  Energic Inc.      12.00   \n",
            "...            ...    ...                           ...        ...   \n",
            "8701   Oct 5, 2023   YKSR          Yakser Holdings Ltd.      13.50   \n",
            "8702   Oct 4, 2023   XLLT                  Xellent Ltd.      20.50   \n",
            "8703   Oct 3, 2023   NNCX       Nanocore Biopharma Ltd.      15.75   \n",
            "8704   Oct 2, 2023   VRGT                  Verigut Inc.      12.25   \n",
            "8705   Oct 1, 2023   CLRX   Clarix Pharmaceuticals Ltd.      14.25   \n",
            "\n",
            "     Current_Price Percentage_Increase                Industry  \\\n",
            "0           $20.48               2.40%      Financial Services   \n",
            "1           $76.65             206.60%  Communication Services   \n",
            "2           $13.35              90.71%              Healthcare   \n",
            "3           $24.11              33.95%              Healthcare   \n",
            "4           $38.45             220.42%                  Energy   \n",
            "...            ...                 ...                     ...   \n",
            "8701        $15.75              16.67%             Industrials   \n",
            "8702        $24.20              18.05%              Technology   \n",
            "8703        $14.50              -7.94%              Healthcare   \n",
            "8704        $11.50              -6.12%              Technology   \n",
            "8705        $12.75             -10.53%              Healthcare   \n",
            "\n",
            "                              Sector Shares_Offered Total_Revenue Total_Asset  \\\n",
            "0                    Shell Companies       10000000    1050505500   622732000   \n",
            "1                      Entertainment        3500000       5300000     2000000   \n",
            "2                      Biotechnology        4800000     145600000    73200000   \n",
            "3                    Medical Devices        5000000     456000000   289700000   \n",
            "4     Oil & Gas Equipment & Services        6250000     820131000   392876000   \n",
            "...                              ...            ...           ...         ...   \n",
            "8701      Engineering & Construction       17500000     400000000   340000000   \n",
            "8702         Software—Infrastructure       11250000     145000000   110000000   \n",
            "8703                   Biotechnology       10500000     120000000    90000000   \n",
            "8704            Software—Application        9500000      65000000    42000000   \n",
            "8705                   Biotechnology       12000000     100000000    78000000   \n",
            "\n",
            "      Sector_encoded  Industry_encoded  Shares_Offered_encoded  \\\n",
            "0                461                24                0.042880   \n",
            "1                173                10               -1.116260   \n",
            "2                 58                28               -0.884432   \n",
            "3                327                28               -0.848766   \n",
            "4                358                20               -0.625854   \n",
            "...              ...               ...                     ...   \n",
            "8701             171                32                1.380349   \n",
            "8702             483                43                0.265792   \n",
            "8703              58                28                0.132045   \n",
            "8704             481                43               -0.046285   \n",
            "8705              58                28                0.399538   \n",
            "\n",
            "      Total_Revenue_encoded  Total_Asset_encoded  \n",
            "0                  0.972504             0.655143  \n",
            "1                 -0.722362            -0.647522  \n",
            "2                 -0.494857            -0.498102  \n",
            "3                  0.008476            -0.043756  \n",
            "4                  0.598937             0.172769  \n",
            "...                     ...                  ...  \n",
            "8701              -0.082331             0.061803  \n",
            "8702              -0.495830            -0.420874  \n",
            "8703              -0.536369            -0.462846  \n",
            "8704              -0.625555            -0.563578  \n",
            "8705              -0.568800            -0.488029  \n",
            "\n",
            "[7106 rows x 16 columns]\n",
            "2.0\n",
            "157.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 Prototyping"
      ],
      "metadata": {
        "id": "oeyEyrg2mlNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 Understanding The target Values with Scatter Plot\n",
        "---------------------------------------------------------\n",
        "We first begin with understanding the dataset with the scatter plot as shown below which shows it has a range of 0-160 for the companies' initial IPO Prices."
      ],
      "metadata": {
        "id": "RrpECaTAmuJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df2.columns)\n",
        "print(df2['IPO_Price'].describe())\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reset the index to make it a regular column\n",
        "df2_reset = df2.reset_index()\n",
        "\n",
        "# Create the scatter plot\n",
        "plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "plt.scatter(df2_reset.index, df2_reset['IPO_Price'])\n",
        "plt.xlabel('Index')\n",
        "plt.ylabel('IPO_Price')\n",
        "plt.title('Scatter Plot of Index vs. IPO_Price')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JrnawX2KCvMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 Defining The Baseline Model\n",
        "---------------------------------------------------------\n",
        "The baseline model for this regression task would be the mean of the IPO_Prices which would be 17.63258. We then measure the metrics for the actual vs predicted results with the actual being the df2['IPO_Price'] Values and the predicted values being mean values of the IPO_Prices. The MSE,MAE,RMSE and MAPE are shown as below which are 84.45,6.40,9.19,0.45 respectively. For the machine learning model to prove its worth, it would have to be better results than the baseline model."
      ],
      "metadata": {
        "id": "Txg7y-9tnIV0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHgxf1uo3763"
      },
      "outputs": [],
      "source": [
        "df2 = df2.reset_index(drop=True)\n",
        "features = df2[['Sector_encoded', 'Industry_encoded','Shares_Offered_encoded','Total_Revenue_encoded','Total_Asset_encoded']].values\n",
        "targets = df2['IPO_Price'].values\n",
        "x = torch.tensor(features, dtype=torch.float)\n",
        "y = torch.tensor(targets, dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "mean_IPO_Price = df2['IPO_Price'].mean()\n",
        "print(mean_IPO_Price)\n",
        "baseline_predictions = [mean_IPO_Price] * len(df2)\n",
        "print(baseline_predictions)\n",
        "actuals = df2['IPO_Price'].values\n",
        "predictions = baseline_predictions\n",
        "\n",
        "mse = mean_squared_error(actuals, predictions)\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "rmse = mean_squared_error(actuals, predictions, squared=False)\n",
        "mape = mean_absolute_percentage_error(actuals, predictions)\n",
        "print(mse,mae,rmse,mape)"
      ],
      "metadata": {
        "id": "d4pYUooK56T1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.3 Defining the train,test sets with edges\n",
        "-----------------------------------------------\n",
        "We first split the dataframe into sizes 0.8 and 0.2 for the training set and test set for the graph neural network to maintain a holdout set so that the test set is unseen to the model and would not recognize hidden patterns in the test set to check if the model is accurate in estimating the IPO_Price.\n",
        "\n",
        "The edges are defined by the indexes of the companies. For example, if company A is in the first position of the list, then it would be linked to the next company in the second position of the list so [0] -> [1] -> [2] and so on.\n",
        "\n"
      ],
      "metadata": {
        "id": "UUrn5sFPn687"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNQrrSNQ9yKM"
      },
      "outputs": [],
      "source": [
        "# Check tensors for NaN values\n",
        "print(\"NaN values in tensor x:\", torch.isnan(x).sum().item())\n",
        "print(\"NaN values in tensor y:\", torch.isnan(y).sum().item())\n",
        "print(x.size())\n",
        "print(y.size())\n",
        "\n",
        "train_df, test_df = train_test_split(df2, test_size=0.2, random_state=42)\n",
        "\n",
        "def df_to_pyg_data(dtf):\n",
        "    # Convert node features and target to torch tensors\n",
        "    x = torch.tensor(dtf[['Sector_encoded', 'Industry_encoded','Shares_Offered_encoded','Total_Revenue_encoded','Total_Asset_encoded']].values, dtype=torch.float)\n",
        "    y = torch.tensor(dtf['IPO_Price'].values, dtype=torch.long)\n",
        "\n",
        "    # Convert edges to torch tensor\n",
        "    edge_index = torch.tensor([[i, i+1] for i in range(len(dtf)-1)] + [[i+1, i] for i in range(len(dtf)-1)], dtype=torch.long).t().contiguous()\n",
        "\n",
        "    # Create PyTorch Geometric Data object\n",
        "    data = Data(x=x, edge_index=edge_index, y=y)\n",
        "    return data\n",
        "\n",
        "\n",
        "train_data = df_to_pyg_data(train_df)\n",
        "test_data = df_to_pyg_data(test_df)\n",
        "\n",
        "train_loader = DataLoader([train_data], batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader([test_data], batch_size=32, shuffle=False)\n",
        "print(len(train_loader))\n",
        "print(len(test_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_dataloader(loader):\n",
        "    for i, batch in enumerate(loader):\n",
        "        print(f\"Batch {i+1}\")\n",
        "        print(\"Number of nodes:\", batch.num_nodes)\n",
        "        print(\"Number of edges:\", batch.num_edges)\n",
        "        print(\"Node features shape:\", batch.x.shape)\n",
        "        print(\"Edge indices shape:\", batch.edge_index.shape)\n",
        "        print(\"Target labels shape:\", batch.y.shape)\n",
        "        print(\"Edge index tensor:\")\n",
        "        print(batch.edge_index)\n",
        "        print(\"Node features tensor:\")\n",
        "        print(batch.x)\n",
        "        print(\"Target labels tensor:\")\n",
        "        print(batch.y)\n",
        "        print(\"-\" * 40)\n",
        "        # Stop after checking one batch to avoid printing too much\n",
        "        break\n",
        "check_dataloader(test_loader)"
      ],
      "metadata": {
        "id": "pkp-XKtLBnMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.4 Initalizing the GNN Model\n",
        "-----------------------------------------------\n",
        "We then setup the basic GNN model to check if it does better than the baseline model with the pytorch library and since we want the output to be a float, we add a linear layer to it which converts the neurons into a single output which is seen below.\n"
      ],
      "metadata": {
        "id": "31P8yyu4o194"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfD1uaPc96OL"
      },
      "outputs": [],
      "source": [
        "#Define the GNN model\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNN, self).__init__()\n",
        "        ###convolutional layer 1 to extract richer features from the data\n",
        "        self.conv1 = GCNConv(in_channels=x.size(1), out_channels=16)\n",
        "        ### convolutional layer 2 further refining these features into a more compact form.\n",
        "        self.conv2 = GCNConv(in_channels=16, out_channels=8)\n",
        "        self.fc = torch.nn.Linear(8, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        embeddings = x\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Training the Model\n",
        "-------------------------------\n",
        "The model are defined with a learning rate of 0.005 and we train it for 1000 epochs and finally we test the results to see if we have performed better than the baseline model by using the gnn model to test against the test set to see if there's better performance."
      ],
      "metadata": {
        "id": "VOqW_9EwqJwv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGDihb_Z-F9j"
      },
      "outputs": [],
      "source": [
        "# Create data loader\n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = GNN()\n",
        "criterion = torch.nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
        "\n",
        "# Training loop with additional checks\n",
        "model.train()\n",
        "for epoch in range(1000):\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch)\n",
        "        loss = criterion(out, batch.y.view(-1, 1).float())\n",
        "        if torch.isnan(loss).sum() > 0:\n",
        "            print(\"NaN loss encountered at epoch:\", epoch)\n",
        "            break\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Testing the Model\n",
        "----------------------------------\n",
        "The model shows a result of mean squared error of 91.0679, Mean Absolute Error of 6.5011 and Root Mean Squared Error of 9.5429 and Mean Absolute Percentage Error of 0.45% which shows that it is not performing better than the baseline model and we move on to developing the model further by adding in more layers of graph convolutional layer and checking if the results will improve in the next few sections."
      ],
      "metadata": {
        "id": "4lS0ynbXqEsD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Fj2pIN2-I8i"
      },
      "outputs": [],
      "source": [
        "# Prediction\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        out = model(batch)\n",
        "        predictions.extend(out.squeeze().cpu().numpy().tolist())\n",
        "\n",
        "print(f'Predicted valuations: {predictions}')\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "\n",
        "# Convert predictions and targets to numpy arrays for evaluation\n",
        "actuals = test_data.y.numpy()\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(actuals, predictions)\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "rmse = mean_squared_error(actuals, predictions, squared=False)\n",
        "mape = mean_absolute_percentage_error(actuals, predictions)\n",
        "\n",
        "\n",
        "print(f'Mean Squared Error: {mse:.4f}')\n",
        "print(f'Mean Absolute Error: {mae:.4f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {mape:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4 Implementation and Improving Of Models\n",
        "-----------------------------------------------\n",
        "For the training of the graph neural network (GNN) model, the loss function used will be the Mean Absolute Error (MAE), which measures the average absolute difference between the actual and predicted values. MAE is an important metric for evaluating model performance in regression tasks because it provides a straightforward measure of how well the model's predictions match the true values. By comparing the MAE of the GNN model to that of a baseline model, we can determine if the GNN model provides better predictive accuracy"
      ],
      "metadata": {
        "id": "zFDjgl-uq9ML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.1 Improving the Edges and splitting the training set\n",
        "----------------------------------------------------\n",
        "We change the edges so that Companies are linked to each other by their sector and industry which we append edge index both ways which is seen from the example below to show the relation between the companies with the industry and sector:\n",
        "\n",
        "[0] -> [1] and [1]->[0]\n",
        "\n",
        "We also split the training set up into training set and validation set to validate the results of the training process and implement early stopping if required."
      ],
      "metadata": {
        "id": "I3ZBD6_BrGNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_pyg_data(df):\n",
        "    # Convert node features and target to torch tensors\n",
        "    x = torch.tensor(df[['Sector_encoded', 'Industry_encoded','Shares_Offered_encoded','Total_Revenue_encoded','Total_Asset_encoded']].values, dtype=torch.float)\n",
        "    y = torch.tensor(df['IPO_Price'].values, dtype=torch.float)\n",
        "    edge_index= []\n",
        "    industry_groups = df.groupby('Industry_encoded').groups\n",
        "    sector_groups = df.groupby('Sector_encoded').groups\n",
        "    for _, nodes in industry_groups.items():\n",
        "      nodes = list(nodes)\n",
        "      for i in range(len(nodes)):\n",
        "        for j in range(i + 1, len(nodes)):\n",
        "            edge_index.append((nodes[i], nodes[j]))\n",
        "            edge_index.append((nodes[j], nodes[i]))\n",
        "    for _, nodes in sector_groups.items():\n",
        "      nodes = list(nodes)\n",
        "      for i in range(len(nodes)):\n",
        "        for j in range(i + 1, len(nodes)):\n",
        "            edge_index.append((nodes[i], nodes[j]))\n",
        "            edge_index.append((nodes[j], nodes[i]))\n",
        "    # Convert edges to torch tensor\n",
        "    edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "\n",
        "    # Create PyTorch Geometric Data object\n",
        "    data = Data(x=x, edge_index=edge_index, y=y)\n",
        "    return data\n",
        "\n",
        "train_df,val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "train_data = df_to_pyg_data(train_df)\n",
        "val_data = df_to_pyg_data(val_df)\n",
        "test_data = df_to_pyg_data(test_df)\n",
        "train_loader2 = DataLoader([train_data], batch_size=32, shuffle=True)\n",
        "val_loader2 = DataLoader([val_data],batch_size=32,shuffle=False)\n",
        "test_loader2 = DataLoader([test_data], batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "3EUDgKX8lQTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429e5f3a-9b71-4765-e842-82a8a507cf8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Testing And Training the model defined in the prototype and Evaluating"
      ],
      "metadata": {
        "id": "uGZkML6CsJ9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model_betterEdges = GNN()\n",
        "criterion = torch.nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model_betterEdges.parameters(), lr=0.005)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 10  # Number of epochs to wait for improvement\n",
        "best_val_loss = float('inf')  # Initialize with a large value\n",
        "epochs_no_improve = 0  # Count of epochs since the last improvement\n",
        "\n",
        "\n",
        "# Training loop with additional checks\n",
        "model_betterEdges.train()\n",
        "for epoch in range(500):\n",
        "  train_loss = 0\n",
        "  for batch in train_loader2:\n",
        "      optimizer.zero_grad()\n",
        "      out = model_betterEdges(batch)\n",
        "      loss = criterion(out, batch.y.view(-1, 1).float())\n",
        "      if torch.isnan(loss).sum() > 0:\n",
        "          print(\"NaN loss encountered at epoch:\", epoch)\n",
        "          break\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model_betterEdges.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item() * batch.num_graphs\n",
        "  train_loss /= len(train_loader2.dataset)\n",
        "  print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n",
        "  model_betterEdges.eval()  # Set the model to evaluation mode\n",
        "  val_loss = 0  # Initialize validation loss\n",
        "\n",
        "  with torch.no_grad():  # No gradient computation\n",
        "      for batch in val_loader2:\n",
        "          out = model_betterEdges(batch)  # Forward pass\n",
        "          loss = criterion(out, batch.y.view(-1, 1).float())  # Compute loss\n",
        "          val_loss += loss.item() * batch.num_graphs  # Accumulate validation loss\n",
        "\n",
        "  val_loss /= len(val_loader2.dataset)  # Average validation loss\n",
        "  print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "  # Early stopping check\n",
        "  if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      epochs_no_improve = 0\n",
        "      # Save the best model\n",
        "      torch.save(model_betterEdges.state_dict(), 'best_model.pt')\n",
        "  else:\n",
        "      epochs_no_improve += 1\n",
        "\n",
        "    # Check if early stopping criterion is met\n",
        "  if epochs_no_improve >= patience:\n",
        "      print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "      break\n",
        "\n",
        "    # Set the model back to training mode for the next epoch\n",
        "  model_betterEdges.train()"
      ],
      "metadata": {
        "id": "7iuxggJslWzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "model_betterEdges.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader2:\n",
        "        out = model_betterEdges(batch)\n",
        "        predictions.extend(out.squeeze().cpu().numpy().tolist())\n",
        "\n",
        "print(f'Predicted valuations: {predictions}')\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "\n",
        "# Convert predictions and targets to numpy arrays for evaluation\n",
        "actuals = test_data.y.numpy()\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(actuals, predictions)\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "rmse = mean_squared_error(actuals, predictions, squared=False)\n",
        "mape = mean_absolute_percentage_error(actuals, predictions)\n",
        "\n",
        "\n",
        "print(f'Mean Squared Error: {mse:.4f}')\n",
        "print(f'Mean Absolute Error: {mae:.4f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {mape:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1tXoIrjFds0",
        "outputId": "b76b054d-6cd4-499f-cce5-b50174e56b17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted valuations: [16.25867462158203, 16.25867462158203, 14.796052932739258, 16.258676528930664, 9.213789939880371, 16.25867462158203, 14.796051025390625, 10.842289924621582, 14.449151992797852, 9.213791847229004, 14.796051025390625, 9.213028907775879, 9.213458061218262, 16.25867462158203, 12.710153579711914, 12.70976734161377, 16.258678436279297, 16.258668899536133, 16.25867462158203, 16.25867462158203, 10.84345817565918, 14.796051025390625, 10.84274673461914, 16.2586669921875, 16.2586727142334, 15.980449676513672, 16.25868034362793, 10.842220306396484, 16.25867462158203, 16.258676528930664, 16.2586727142334, 20.219566345214844, 16.25867462158203, 14.796051025390625, 9.21303939819336, 10.842571258544922, 14.796049118041992, 14.796051025390625, 14.44913101196289, 16.258670806884766, 16.258676528930664, 14.796049118041992, 14.796051025390625, 12.70643138885498, 14.449248313903809, 14.796051025390625, 14.796051025390625, 16.258676528930664, 14.796046257019043, 16.258676528930664, 10.846891403198242, 14.449068069458008, 14.449180603027344, 14.449244499206543, 10.846936225891113, 9.535701751708984, 16.827228546142578, 14.448872566223145, 10.841551780700684, 16.258682250976562, 20.21961212158203, 16.25867462158203, 14.796051025390625, 10.842103004455566, 16.25867462158203, 16.25867462158203, 14.796051025390625, 14.796051025390625, 16.25867462158203, 12.710227012634277, 16.258678436279297, 16.258686065673828, 16.258663177490234, 9.213067054748535, 16.25867462158203, 14.796051025390625, 16.258676528930664, 9.21261215209961, 10.841996192932129, 16.258678436279297, 9.533047676086426, 16.25867462158203, 16.827186584472656, 16.258676528930664, 8.529398918151855, 16.258670806884766, 14.796051025390625, 9.534110069274902, 9.212555885314941, 16.258676528930664, 10.842735290527344, 16.25867462158203, 14.449100494384766, 16.258676528930664, 14.449163436889648, 14.796051025390625, 14.79605484008789, 16.25867462158203, 16.25867462158203, 16.25867462158203, 16.258670806884766, 14.796051025390625, 14.796051025390625, 14.796051025390625, 16.25865936279297, 14.796051025390625, 10.843551635742188, 16.25867462158203, 14.796046257019043, 15.980780601501465, 9.535662651062012, 14.796049118041992, 16.2586669921875, 14.796051025390625, 14.796052932739258, 16.258678436279297, 14.449142456054688, 14.796052932739258, 14.796051025390625, 16.258676528930664, 9.213034629821777, 16.25867462158203, 16.827190399169922, 16.258676528930664, 16.258676528930664, 16.82794952392578, 6.335049629211426, 16.258684158325195, 14.796051025390625, 15.981085777282715, 16.258676528930664, 14.796051025390625, 14.796051025390625, 16.258676528930664, 16.2586612701416, 14.796052932739258, 9.213748931884766, 14.79604721069336, 14.796051025390625, 16.258676528930664, 14.796051025390625, 14.449240684509277, 16.258676528930664, 14.796051025390625, 16.25865936279297, 16.258682250976562, 14.796051025390625, 16.258676528930664, 16.827653884887695, 14.448919296264648, 14.449148178100586, 16.258676528930664, 10.841540336608887, 16.827251434326172, 14.448928833007812, 16.258676528930664, 14.796051025390625, 16.258670806884766, 15.980655670166016, 16.258678436279297, 16.82793617248535, 14.796052932739258, 16.2586727142334, 16.258676528930664, 16.258676528930664, 14.449224472045898, 14.796051025390625, 9.5330171585083, 9.534676551818848, 14.796052932739258, 16.258676528930664, 16.258663177490234, 14.796051025390625, 14.796049118041992, 16.2586727142334, 16.25867462158203, 16.258676528930664, 16.258676528930664, 14.796052932739258, 14.44891357421875, 16.25865936279297, 14.796051025390625, 14.796051025390625, 16.25868034362793, 10.846900939941406, 14.449172973632812, 16.25867462158203, 14.796052932739258, 14.796051025390625, 14.449054718017578, 16.25867462158203, 16.25867462158203, 14.79605484008789, 16.2586669921875, 14.796051025390625, 9.21259593963623, 14.796051025390625, 9.535698890686035, 16.258676528930664, 16.827281951904297, 20.219676971435547, 14.448927879333496, 16.258676528930664, 15.980413436889648, 10.841547966003418, 14.796049118041992, 14.796051025390625, 14.796051025390625, 10.841995239257812, 14.449210166931152, 14.796051025390625, 14.796051025390625, 14.796051025390625, 14.796051025390625, 14.448979377746582, 16.258676528930664, 16.25867462158203, 14.796052932739258, 16.82720184326172, 14.796051025390625, 10.84152603149414, 19.593738555908203, 14.796051025390625, 9.213822364807129, 14.796049118041992, 9.534662246704102, 14.796049118041992, 14.448874473571777, 14.449056625366211, 10.842751502990723, 14.796051025390625, 16.25867462158203, 14.796052932739258, 16.258676528930664, 16.25865936279297, 14.79604721069336, 14.448908805847168, 10.842238426208496, 14.796051025390625, 14.796051025390625, 12.710142135620117, 16.258663177490234, 20.612667083740234, 9.534111022949219, 16.2586612701416, 14.796051025390625, 16.258663177490234, 16.828224182128906, 14.796051025390625, 14.796052932739258, 14.796051025390625, 16.258676528930664, 16.258676528930664, 14.796051025390625, 14.796052932739258, 16.25867462158203, 14.796052932739258, 16.25867462158203, 16.8271541595459, 14.796051025390625, 16.258676528930664, 16.258676528930664, 14.796051025390625, 16.25867462158203, 16.258665084838867, 16.258665084838867, 16.258686065673828, 14.449121475219727, 14.796051025390625, 10.842206954956055, 12.706557273864746, 14.796051025390625, 16.82769012451172, 16.258663177490234, 16.25867462158203, 10.844581604003906, 16.258676528930664, 14.796051025390625, 12.706474304199219, 14.796051025390625, 20.219467163085938, 10.844642639160156, 16.25867462158203, 9.213770866394043, 14.796051025390625, 16.82711410522461, 16.258676528930664, 14.796051025390625, 14.796051025390625, 16.2586612701416, 14.449095726013184, 16.25867462158203, 14.796048164367676, 16.258676528930664, 16.258676528930664, 14.796051025390625, 14.796049118041992, 16.2586612701416, 14.449066162109375, 14.796051025390625, 14.796049118041992, 10.842212677001953, 9.534435272216797, 14.796052932739258, 14.79604721069336, 16.25867462158203, 16.258670806884766, 16.258676528930664, 14.796049118041992, 14.796051025390625, 14.449197769165039, 9.533073425292969, 14.796051025390625, 14.449159622192383, 16.258676528930664, 16.2586612701416, 16.2586669921875, 14.796051025390625, 16.25867462158203, 16.25867462158203, 14.796051025390625, 14.796049118041992, 14.796051025390625, 14.448963165283203, 14.796051025390625, 14.796049118041992, 16.25867462158203, 14.796051025390625, 14.796051025390625, 16.258676528930664, 16.25867462158203, 10.842002868652344, 10.84689998626709, 14.796051025390625, 16.82718849182129, 14.796052932739258, 14.796051025390625, 10.841556549072266, 14.796051025390625, 16.258676528930664, 10.844671249389648, 14.796052932739258, 10.841538429260254, 9.212522506713867, 14.796050071716309, 16.82720947265625, 14.796052932739258, 9.21346664428711, 10.842558860778809, 14.796051025390625, 16.258676528930664, 14.796051025390625, 14.796051025390625, 12.711798667907715, 16.82763671875, 16.258665084838867, 16.25867462158203, 16.258686065673828, 14.796051025390625, 16.25867462158203, 16.258668899536133, 14.796051025390625, 14.449031829833984, 16.25867462158203, 14.796051025390625, 14.796049118041992, 14.796051025390625, 10.841854095458984, 14.796052932739258, 14.796049118041992, 14.796051025390625, 16.258663177490234, 14.796051025390625, 16.25867462158203, 14.796051025390625, 16.258682250976562, 14.79604721069336, 14.796049118041992, 12.711893081665039, 14.796051025390625, 14.796052932739258, 16.25867462158203, 16.258678436279297, 16.82722282409668, 16.258678436279297, 14.796051025390625, 16.2586669921875, 14.796051025390625, 14.796051025390625, 14.449108123779297, 9.534112930297852, 16.258682250976562, 16.258676528930664, 16.258676528930664, 14.796052932739258, 14.79605484008789, 10.867094039916992, 14.449161529541016, 14.448925018310547, 16.258676528930664, 14.796051025390625, 14.796051025390625, 10.841526985168457, 14.796051025390625, 14.449136734008789, 14.79604721069336, 14.796052932739258, 16.25867462158203, 14.796051025390625, 9.534111976623535, 14.796051025390625, 9.212509155273438, 14.796051025390625, 16.25867462158203, 10.841529846191406, 16.25868034362793, 14.796052932739258, 12.706456184387207, 16.258651733398438, 10.842205047607422, 15.981038093566895, 14.796049118041992, 9.534676551818848, 9.534235000610352, 14.796049118041992, 14.796051025390625, 14.449256896972656, 16.25868034362793, 14.796051025390625, 16.2586669921875, 16.25867462158203, 14.796049118041992, 14.796049118041992, 11.854860305786133, 14.796052932739258, 16.258676528930664, 9.535663604736328, 14.796051025390625, 16.25867462158203, 10.842737197875977, 16.258676528930664, 14.79604721069336, 14.449176788330078, 14.796051025390625, 14.449224472045898, 16.25868034362793, 14.796051025390625, 14.449188232421875, 16.258678436279297, 10.841538429260254, 14.449097633361816, 14.796051025390625, 14.796049118041992, 16.258676528930664, 14.796060562133789, 9.213062286376953, 16.258676528930664, 10.8427734375, 16.258670806884766, 20.2195987701416, 16.25867462158203, 16.258676528930664, 14.796051025390625, 16.258678436279297, 14.79604721069336, 14.796050071716309, 16.25868034362793, 16.258678436279297, 9.53289794921875, 16.828163146972656, 16.25867462158203, 17.708606719970703, 16.25867462158203, 16.258676528930664, 16.25868034362793, 16.2586612701416, 14.79605484008789, 10.841686248779297, 14.796052932739258, 10.841517448425293, 16.258665084838867, 16.2586612701416, 16.2586669921875, 9.21320629119873, 9.21382999420166, 10.842259407043457, 20.219898223876953, 15.980562210083008, 14.796051025390625, 9.534663200378418, 16.2586612701416, 16.2586612701416, 14.796048164367676, 16.258670806884766, 16.258676528930664, 16.25867462158203, 14.796051025390625, 14.796052932739258, 20.219602584838867, 20.218082427978516, 14.796048164367676, 10.95228385925293, 10.842235565185547, 9.213677406311035, 14.796052932739258, 16.25867462158203, 14.79604721069336, 16.82765007019043, 9.532637596130371, 16.25868034362793, 16.258676528930664, 16.25867462158203, 14.796052932739258, 14.796052932739258, 16.258682250976562, 16.25867462158203, 14.796051025390625, 14.796051025390625, 16.25867462158203, 14.79605484008789, 14.449220657348633, 14.796051025390625, 16.25867462158203, 9.535685539245605, 16.25867462158203, 10.841707229614258, 16.258676528930664, 9.533849716186523, 14.796051025390625, 14.796052932739258, 10.841692924499512, 16.82721710205078, 14.796052932739258, 14.796051025390625, 14.796051025390625, 16.258678436279297, 16.258665084838867, 14.796051025390625, 14.448902130126953, 14.796051025390625, 14.449077606201172, 20.219802856445312, 14.796052932739258, 14.448919296264648, 14.449252128601074, 14.796049118041992, 16.25867462158203, 14.796051025390625, 14.449119567871094, 14.796052932739258, 10.843511581420898, 14.796051025390625, 16.25867462158203, 16.25867462158203, 10.842340469360352, 16.25867462158203, 16.258676528930664, 16.258663177490234, 16.2586727142334, 10.841737747192383, 14.796049118041992, 16.258682250976562, 10.844755172729492, 16.258676528930664, 16.258676528930664, 9.213064193725586, 16.258676528930664, 16.258678436279297, 14.796051025390625, 9.537983894348145, 16.258676528930664, 16.25867462158203, 14.44913101196289, 15.980692863464355, 14.796049118041992, 14.449119567871094, 14.796052932739258, 16.25867462158203, 14.449091911315918, 16.258686065673828, 16.25868034362793, 14.796049118041992, 16.25867462158203, 14.79604721069336, 16.258676528930664, 16.258676528930664, 14.796049118041992, 14.796051025390625, 14.796056747436523, 10.841876983642578, 16.2586669921875, 16.258676528930664, 16.2586612701416, 14.796051025390625, 11.284308433532715, 16.258676528930664, 16.258686065673828, 14.796052932739258, 16.2586612701416, 14.796046257019043, 14.796049118041992, 14.796051025390625, 12.707967758178711, 16.828142166137695, 9.534757614135742, 14.79604721069336, 16.258670806884766, 14.796051025390625, 16.25868034362793, 10.841922760009766, 14.796049118041992, 14.796051025390625, 14.796051025390625, 14.796051025390625, 14.449015617370605, 10.841527938842773, 14.796051025390625, 14.796051025390625, 16.258663177490234, 14.796051025390625, 16.258678436279297, 9.536293029785156, 9.213255882263184, 14.79604721069336, 10.841583251953125, 14.449210166931152, 14.796051025390625, 16.25868034362793, 12.71047592163086, 16.258670806884766, 10.841527938842773, 14.796051025390625, 9.212991714477539, 14.796051025390625, 14.448988914489746, 14.449201583862305, 12.710687637329102, 14.796052932739258, 12.70619010925293, 16.2586669921875, 16.258686065673828, 14.79604721069336, 9.21249771118164, 14.448904037475586, 16.25868034362793, 16.827224731445312, 14.44911003112793, 16.258665084838867, 16.258663177490234, 10.842702865600586, 16.25867462158203, 14.796052932739258, 9.214423179626465, 16.258676528930664, 16.258686065673828, 10.841545104980469, 14.79605484008789, 14.796052932739258, 14.796051025390625, 14.44892406463623, 14.796052932739258, 14.796051025390625, 14.796052932739258, 14.796051025390625, 14.448908805847168, 16.25867462158203, 12.710126876831055, 16.2586669921875, 14.796051025390625, 15.981081008911133, 14.796051025390625, 14.796051025390625, 16.258663177490234, 16.258676528930664, 16.25867462158203, 16.258676528930664, 14.796052932739258, 16.258678436279297, 14.796053886413574, 14.796051025390625, 16.258676528930664, 14.796051025390625, 16.258676528930664, 16.258678436279297, 14.796051025390625, 16.258663177490234, 14.796052932739258, 14.796049118041992, 14.796051025390625, 10.842742919921875, 14.796051025390625, 14.796052932739258, 20.219337463378906, 14.449101448059082, 16.258676528930664, 14.796051025390625, 21.84726905822754, 16.2586669921875, 12.708357810974121, 15.980266571044922, 14.796051025390625, 9.213770866394043, 16.25867462158203, 16.82720375061035, 14.796052932739258, 16.25867462158203, 16.258676528930664, 16.258663177490234, 14.796049118041992, 14.449178695678711, 16.25867462158203, 14.796049118041992, 12.711780548095703, 14.796051025390625, 10.842338562011719, 14.796052932739258, 14.796046257019043, 14.79605484008789, 16.258676528930664, 16.82724952697754, 16.258676528930664, 14.796051025390625, 14.796049118041992, 14.796051025390625, 9.53382396697998, 14.796052932739258, 14.796051025390625, 14.796051025390625, 14.796048164367676, 16.827190399169922, 9.212547302246094, 10.842011451721191, 14.796051025390625, 16.25867462158203, 16.258676528930664, 16.258686065673828, 16.258676528930664, 14.79604721069336, 14.449183464050293, 16.258676528930664, 16.258676528930664, 9.213773727416992, 16.258670806884766, 15.98076343536377, 14.796051025390625, 16.2586612701416, 16.258676528930664, 16.258676528930664, 9.212543487548828, 16.258678436279297, 12.707025527954102, 14.796051025390625, 9.213058471679688, 14.796051025390625, 16.258676528930664, 14.796051025390625, 16.2586612701416, 9.212543487548828, 14.796051979064941, 14.796052932739258, 20.219566345214844, 10.842201232910156, 14.796052932739258, 16.258682250976562, 9.53272533416748, 14.796049118041992, 15.980032920837402, 16.25867462158203, 16.258665084838867, 16.2586612701416, 16.258670806884766, 9.21394157409668, 14.796052932739258, 16.258676528930664, 14.796052932739258, 12.710206031799316, 16.258676528930664, 14.449000358581543, 16.258686065673828, 16.2586612701416, 14.796051025390625, 14.79604721069336, 14.796052932739258, 16.25867462158203, 14.796051025390625, 14.449258804321289, 14.796051025390625, 10.843256950378418, 16.25867462158203, 20.21976661682129, 9.212884902954102, 16.25867462158203, 16.25865936279297, 16.25867462158203, 14.796051025390625, 16.2586669921875, 14.79605484008789, 14.796052932739258, 14.449202537536621, 9.533833503723145, 14.796052932739258, 14.449094772338867, 16.2586612701416, 14.796051025390625, 14.796052932739258, 14.796052932739258, 9.213667869567871, 14.796052932739258, 14.449217796325684, 16.25867462158203, 14.796051025390625, 14.796046257019043, 16.25867462158203, 14.796051025390625, 16.25867462158203, 14.796052932739258, 14.79604721069336, 16.25867462158203, 10.842206954956055, 16.258678436279297, 14.796051025390625, 16.2586727142334, 11.136162757873535, 9.5341157913208, 9.213627815246582, 14.449163436889648, 14.796051025390625, 10.842246055603027, 14.796052932739258, 14.448941230773926, 16.258670806884766, 16.25867462158203, 14.796052932739258, 16.2586612701416, 26.570724487304688, 19.491806030273438, 16.258676528930664, 14.796052932739258, 16.258663177490234, 16.25867462158203, 14.449246406555176, 12.713653564453125, 16.2586612701416, 15.980859756469727, 16.258676528930664, 16.2586669921875, 14.796052932739258, 14.79605484008789, 14.796052932739258, 16.25867462158203, 16.258676528930664, 14.796048164367676, 16.258676528930664, 16.258676528930664, 14.449173927307129, 16.25867462158203, 16.25867462158203, 14.796049118041992, 10.842756271362305, 16.2586612701416, 14.796051025390625, 14.796052932739258, 14.449228286743164, 16.258668899536133, 16.258686065673828, 16.2586612701416, 14.796052932739258, 14.796051025390625, 14.796049118041992, 16.258668899536133, 14.796052932739258, 10.843461036682129, 16.2586612701416, 16.258663177490234, 14.796049118041992, 14.796051025390625, 16.258684158325195, 14.796051025390625, 16.258668899536133, 14.449201583862305, 16.258668899536133, 9.213953018188477, 14.449127197265625, 16.25867462158203, 16.258657455444336, 16.2586669921875, 14.796052932739258, 16.82720184326172, 14.796056747436523, 14.796052932739258, 14.79605484008789, 16.827228546142578, 16.258668899536133, 14.79605484008789, 14.449118614196777, 16.25867462158203, 16.258665084838867, 14.796049118041992, 14.448921203613281, 14.449051856994629, 16.82723045349121, 16.827190399169922, 16.25867462158203, 16.258676528930664, 10.844618797302246, 10.844664573669434, 16.2586612701416, 14.796052932739258, 16.25867462158203, 14.796052932739258, 14.796052932739258, 17.021718978881836, 14.449077606201172, 16.258676528930664, 16.258670806884766, 14.44922161102295, 14.449076652526855, 16.25867462158203, 14.79605484008789, 16.2586612701416, 16.25867462158203, 16.2586612701416, 16.258676528930664, 14.796049118041992, 14.796051025390625, 16.25867462158203, 14.796049118041992, 10.843506813049316, 10.84151840209961, 14.449275970458984, 16.258676528930664, 14.79605484008789, 16.258676528930664, 16.258676528930664, 14.79604721069336, 16.2586612701416, 10.843390464782715, 12.706985473632812, 16.25867462158203, 16.25868034362793, 9.534664154052734, 15.980607986450195, 10.842569351196289, 16.25867462158203, 14.796052932739258, 14.796052932739258, 16.258676528930664, 16.258678436279297, 16.258665084838867, 16.258665084838867, 16.258665084838867, 14.796051025390625, 9.213247299194336, 16.258676528930664, 14.796049118041992, 9.213738441467285, 14.79605484008789, 14.796052932739258, 16.25867462158203, 14.796052932739258, 10.842788696289062, 14.796049118041992, 14.796052932739258, 16.25867462158203, 14.44902229309082, 9.535713195800781, 16.2586612701416, 14.796049118041992, 14.796049118041992, 14.449348449707031, 14.796051025390625, 16.25867462158203, 12.70643138885498, 16.258686065673828, 14.449070930480957, 14.44892692565918, 14.449052810668945, 16.2586612701416, 16.82724952697754, 16.2586612701416, 14.79605484008789, 9.214119911193848, 10.846846580505371, 16.258670806884766, 14.449213027954102, 15.981064796447754, 14.796052932739258, 14.449056625366211, 14.796052932739258, 14.44917106628418, 14.796049118041992, 14.79605484008789, 14.796052932739258, 9.213050842285156, 16.25867462158203, 16.258655548095703, 20.219818115234375, 16.258678436279297, 16.2586612701416, 12.710227966308594, 16.258676528930664, 16.827253341674805, 16.258676528930664, 14.796052932739258, 16.82765769958496, 14.448943138122559, 15.980972290039062, 14.448989868164062, 16.258676528930664, 14.796052932739258, 16.827913284301758, 14.44918155670166, 16.25867462158203, 14.79605484008789, 14.796052932739258, 14.44929313659668, 16.2586612701416, 14.79605484008789, 16.25867462158203, 16.258676528930664, 16.258663177490234, 16.258676528930664, 9.21368408203125, 16.25867462158203, 11.101669311523438, 14.79605484008789, 9.213506698608398, 16.25867462158203, 14.79605484008789, 16.25867462158203, 16.828174591064453, 16.258676528930664, 16.258676528930664, 14.796049118041992, 16.258663177490234, 14.796051025390625, 14.79605484008789, 14.449183464050293, 16.258668899536133, 16.258676528930664, 14.448999404907227, 14.79605484008789, 14.796052932739258, 10.846952438354492, 9.212648391723633, 14.796052932739258, 14.796052932739258, 14.796052932739258, 16.258676528930664, 14.796052932739258, 14.79605484008789, 14.796052932739258, 14.449077606201172, 14.796049118041992, 14.796048164367676, 9.213531494140625, 16.25867462158203, 10.841657638549805, 14.796052932739258, 16.25867462158203, 16.258665084838867, 10.846752166748047, 14.449214935302734, 14.79605484008789, 14.796052932739258, 14.796052932739258, 10.844615936279297, 9.535144805908203, 16.258670806884766, 14.796051025390625, 16.25867462158203, 14.79605484008789, 14.796052932739258, 14.796051025390625, 14.448942184448242, 14.796052932739258, 16.258663177490234, 14.796052932739258, 16.258676528930664, 16.827245712280273, 16.258665084838867, 9.212677955627441, 16.2586669921875, 14.796052932739258, 16.25867462158203, 14.449228286743164, 16.258678436279297, 14.796052932739258, 14.796051025390625, 14.449191093444824, 14.796052932739258, 12.71023178100586, 16.2586612701416, 16.258663177490234, 9.213738441467285, 14.796052932739258, 9.212817192077637, 14.796051025390625, 14.449048042297363, 9.534022331237793, 10.842252731323242, 14.796052932739258, 14.796052932739258, 14.796052932739258, 16.258676528930664, 14.449076652526855, 14.79605484008789, 14.796052932739258, 14.79605484008789, 14.796052932739258, 14.796052932739258, 14.449118614196777, 20.21949005126953, 14.796052932739258, 14.796052932739258, 10.842744827270508, 14.796052932739258, 16.258676528930664, 16.25865936279297, 16.25867462158203, 14.796051025390625, 20.219505310058594, 9.21251106262207, 20.219709396362305, 16.258676528930664, 16.25867462158203, 10.841544151306152, 15.981164932250977, 14.796052932739258, 9.21377944946289, 16.25867462158203, 14.796052932739258, 16.25867462158203, 14.796049118041992, 16.827167510986328, 16.2586612701416, 14.796052932739258, 14.796052932739258, 14.796049118041992, 16.258676528930664, 16.258676528930664, 20.219614028930664, 14.796052932739258, 14.796052932739258, 14.79605484008789, 16.258676528930664, 16.25867462158203, 14.796052932739258, 14.796052932739258, 14.79605484008789, 14.796052932739258, 16.258676528930664, 16.25867462158203, 16.25867462158203, 14.449115753173828, 9.21304702758789, 16.258663177490234, 9.213834762573242, 14.796052932739258, 16.2586669921875, 14.448993682861328, 14.796052932739258, 16.258678436279297, 20.219635009765625, 9.213846206665039, 16.258686065673828, 16.25867462158203, 9.213284492492676, 10.843439102172852, 9.213749885559082, 14.449281692504883, 14.79605484008789, 14.796052932739258, 14.796052932739258, 9.535693168640137, 14.796052932739258, 10.842212677001953, 9.213907241821289, 14.79605484008789, 14.449182510375977, 10.841543197631836, 16.25868034362793, 14.448958396911621, 16.258678436279297, 16.258663177490234, 16.258678436279297, 16.25867462158203, 16.258676528930664, 16.82721710205078, 14.449288368225098, 16.2586612701416, 14.796052932739258, 14.44912338256836, 10.841538429260254, 14.796052932739258, 16.2586669921875, 14.796049118041992, 14.796051025390625, 14.796051025390625, 14.796051025390625, 14.449193954467773, 9.533135414123535, 9.212890625, 14.796052932739258, 12.708658218383789, 16.2586612701416, 14.449068069458008, 14.79604721069336, 10.844579696655273, 14.796052932739258, 15.980710983276367, 9.213661193847656, 14.796052932739258, 16.25868034362793, 16.258678436279297, 14.796049118041992, 14.449078559875488, 14.44920825958252, 16.258686065673828, 14.796052932739258, 14.796052932739258, 14.79605484008789, 16.25867462158203, 14.79605484008789, 16.258678436279297, 10.841999053955078, 14.449024200439453, 16.258676528930664, 16.258676528930664, 16.258676528930664, 16.25867462158203, 16.258676528930664, 16.258676528930664, 14.796052932739258, 10.841971397399902, 16.25867462158203, 10.842795372009277, 10.842742919921875, 14.796052932739258, 14.449202537536621, 10.842243194580078, 16.25867462158203, 9.534586906433105, 20.541122436523438, 9.534113883972168, 9.212584495544434, 10.842742919921875, 14.44925308227539, 14.449101448059082, 14.449030876159668, 16.2586612701416, 14.796052932739258, 16.258676528930664, 16.25867462158203, 10.841705322265625, 16.258676528930664, 14.796051025390625, 14.448883056640625, 16.258670806884766, 16.828250885009766, 14.796052932739258, 14.79605484008789, 14.449297904968262, 16.258678436279297, 16.258676528930664, 14.796052932739258, 14.796052932739258, 16.2586669921875, 14.449047088623047, 16.258676528930664, 14.449247360229492, 16.258678436279297, 16.258676528930664, 12.706954002380371, 16.2586612701416, 10.12579345703125, 16.25867462158203, 14.449281692504883, 16.25865936279297, 20.219594955444336, 10.842795372009277, 14.796052932739258, 14.796052932739258, 16.25867462158203, 16.258676528930664, 16.258676528930664, 16.25867462158203, 14.79605484008789, 16.82767677307129, 16.2586727142334, 14.796052932739258, 16.258682250976562, 14.448997497558594, 20.21976089477539, 9.213691711425781, 16.258676528930664, 9.213895797729492, 14.796052932739258, 14.448877334594727, 16.258676528930664, 14.796052932739258, 14.796052932739258, 16.258678436279297, 16.827659606933594, 14.796052932739258, 16.258686065673828, 16.258676528930664, 14.449185371398926, 16.25867462158203, 9.213762283325195, 10.842266082763672, 14.44922161102295, 9.212918281555176, 16.258665084838867, 16.258665084838867, 12.706443786621094, 14.79605484008789, 14.796052932739258, 14.796052932739258, 16.258676528930664, 16.258676528930664, 10.844560623168945, 10.842733383178711, 10.841550827026367, 16.25867462158203, 16.2586727142334, 16.258663177490234, 14.796052932739258, 16.82716178894043, 16.827241897583008, 16.25867462158203, 12.711700439453125, 14.796051025390625, 14.796049118041992, 14.796052932739258, 15.980515480041504, 10.84272575378418, 14.79605484008789, 16.258665084838867, 16.2586727142334, 16.25867462158203, 16.827211380004883, 10.843426704406738, 10.950514793395996, 14.448890686035156, 16.25867462158203, 14.79605484008789, 14.449131965637207, 14.79605484008789, 16.25867462158203, 14.796052932739258, 10.841958045959473, 14.796052932739258, 14.79605484008789, 14.449146270751953, 16.25867462158203, 14.79605484008789, 16.258663177490234, 33.04882049560547, 16.827211380004883, 14.796052932739258, 10.842236518859863, 14.796052932739258, 14.796052932739258, 16.25867462158203, 16.25867462158203, 14.796049118041992, 14.796052932739258, 10.842266082763672, 16.258676528930664, 16.25867462158203, 16.258676528930664, 16.258665084838867, 9.212478637695312, 14.796052932739258, 14.449183464050293, 14.449357986450195, 14.796052932739258, 14.449254989624023, 16.258676528930664, 9.213785171508789, 16.258676528930664, 16.258676528930664, 14.448919296264648, 16.25867462158203, 9.212518692016602, 16.258676528930664, 14.796052932739258, 14.796052932739258, 16.2586727142334, 16.258668899536133, 14.796049118041992, 9.533050537109375, 16.25868034362793, 16.258676528930664, 10.870433807373047, 9.213069915771484, 15.98060417175293, 14.796052932739258, 14.449178695678711, 14.449106216430664, 12.708883285522461, 16.2586612701416, 14.796052932739258, 14.796051025390625, 16.258676528930664, 16.2586612701416, 16.25867462158203, 9.213981628417969, 14.796051025390625, 14.796052932739258, 16.258663177490234, 12.706953048706055, 16.25867462158203, 14.796052932739258, 16.258676528930664, 14.79605484008789, 14.79604721069336, 14.79605484008789, 16.25867462158203, 16.258676528930664, 14.79605484008789]\n",
            "Mean Squared Error: 103.6929\n",
            "Mean Absolute Error: 6.4931\n",
            "Root Mean Squared Error (RMSE): 10.1830\n",
            "Mean Absolute Percentage Error (MAPE): 0.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.2.1 Evaluation Of Model With Better Edges\n",
        "-----------------------------------------\n",
        "The Mean Squared Error has downgraded from 91.0679 to 103.6929,\n",
        "\n",
        "The mean absolute error improved from 6.5011 to 6.4931,\n",
        "\n",
        "The Root Mean Squared Error improved from 10.1830 to 9.5429,\n",
        "\n",
        "The Mean Absolute Percentage Error improved from 0.45% to 0.38%\n",
        "\n",
        "The model has shown more improvements in three of the metrics where only the mean squared error has downgraded which may be due to a large higher outlier which produced a large error in the test set."
      ],
      "metadata": {
        "id": "PitBs8ev1sN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.3 Testing And Evaluations of Other Models\n"
      ],
      "metadata": {
        "id": "1ydw3kra3AwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.1 Model 2\n",
        "-----------------------\n",
        "In model 2, we added one more graph convolutional layer.\n",
        "The model consists of:\n",
        "\n",
        "3 GCNConv Layer\n",
        "\n",
        "1 Output Linear Layer\n",
        "\n",
        "And we proceed to train and test the model if it is better."
      ],
      "metadata": {
        "id": "lXx-KJQB3V-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the GNN model\n",
        "class GNN2(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNN2, self).__init__()\n",
        "        ###convolutional layer 1 to extract richer features from the data\n",
        "        self.conv1 = GCNConv(in_channels=x.size(1), out_channels=32)\n",
        "        ### convolutional layer 2 further refining these features into a more compact form.\n",
        "        self.conv2 = GCNConv(in_channels=32, out_channels=16)\n",
        "        self.conv3 = GCNConv(in_channels=16, out_channels=8)\n",
        "        self.fc = torch.nn.Linear(8, 1)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        embeddings = x\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "I586Q3nXkfp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model_Iteration2 = GNN2()\n",
        "criterion = torch.nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model_Iteration2.parameters(), lr=0.005)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 15  # Number of epochs to wait for improvement\n",
        "best_val_loss = float('inf')  # Initialize with a large value\n",
        "epochs_no_improve = 0  # Count of epochs since the last improvement\n",
        "\n",
        "\n",
        "# Training loop with additional checks\n",
        "model_Iteration2.train()\n",
        "for epoch in range(500):\n",
        "  train_loss = 0\n",
        "  for batch in train_loader2:\n",
        "      optimizer.zero_grad()\n",
        "      out = model_Iteration2(batch)\n",
        "      loss = criterion(out, batch.y.view(-1, 1).float())\n",
        "      if torch.isnan(loss).sum() > 0:\n",
        "          print(\"NaN loss encountered at epoch:\", epoch)\n",
        "          break\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model_Iteration2.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item() * batch.num_graphs\n",
        "  train_loss /= len(train_loader2.dataset)\n",
        "  print(f'Epoch {epoch+1}, Loss: {loss.item()}')\n",
        "\n",
        "  model_Iteration2.eval()  # Set the model to evaluation mode\n",
        "  val_loss = 0  # Initialize validation loss\n",
        "\n",
        "  with torch.no_grad():  # No gradient computation\n",
        "      for batch in val_loader2:\n",
        "          out = model_Iteration2(batch)  # Forward pass\n",
        "          loss = criterion(out, batch.y.view(-1, 1).float())  # Compute loss\n",
        "          val_loss += loss.item() * batch.num_graphs  # Accumulate validation loss\n",
        "\n",
        "  val_loss /= len(val_loader2.dataset)  # Average validation loss\n",
        "  print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}')\n",
        "\n",
        "  # Early stopping check\n",
        "  if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      epochs_no_improve = 0\n",
        "      # Save the best model\n",
        "      torch.save(model_Iteration2.state_dict(), 'best_model2.pt')\n",
        "  else:\n",
        "      epochs_no_improve += 1\n",
        "\n",
        "    # Check if early stopping criterion is met\n",
        "  if epochs_no_improve >= patience:\n",
        "      print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "      break\n",
        "\n",
        "    # Set the model back to training mode for the next epoch\n",
        "  model_Iteration2.train()"
      ],
      "metadata": {
        "id": "AJ4BrsQGPLAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####4.3.1 Model 2 Evaluation\n",
        "From model 2, it shows that adding a single layer with more neurons has little impact on training the model with the dataset as seen from the results showing little improvements to some of the metrics but is worse than the baseline model which can be seen from the mean absolute error of 6.40 to 6.48."
      ],
      "metadata": {
        "id": "H7goLlW033s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "model_Iteration2.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader2:\n",
        "        out = model_Iteration2(batch)\n",
        "        predictions.extend(out.squeeze().cpu().numpy().tolist())\n",
        "\n",
        "print(f'Predicted valuations: {predictions}')\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "\n",
        "# Convert predictions and targets to numpy arrays for evaluation\n",
        "actuals = test_data.y.numpy()\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(actuals, predictions)\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "rmse = mean_squared_error(actuals, predictions, squared=False)\n",
        "mape = mean_absolute_percentage_error(actuals, predictions)\n",
        "\n",
        "\n",
        "print(f'Mean Squared Error: {mse:.4f}')\n",
        "print(f'Mean Absolute Error: {mae:.4f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {mape:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rz69YDA_RHZz",
        "outputId": "29b03356-55c7-49ff-c882-eb5c56fb5e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted valuations: [16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 9.275904655456543, 16.6998348236084, 14.252598762512207, 11.223886489868164, 15.049765586853027, 9.275906562805176, 14.252598762512207, 9.275910377502441, 9.27590560913086, 16.6998348236084, 13.487652778625488, 13.487659454345703, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 11.460548400878906, 14.252598762512207, 11.223883628845215, 16.6998348236084, 16.6998348236084, 15.48996639251709, 16.6998348236084, 11.223886489868164, 16.6998348236084, 16.6998348236084, 16.6998348236084, 19.641664505004883, 16.6998348236084, 14.252598762512207, 9.275908470153809, 11.223885536193848, 14.252598762512207, 14.252598762512207, 15.049765586853027, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 13.487725257873535, 15.049763679504395, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 16.6998348236084, 11.460489273071289, 15.049766540527344, 15.049763679504395, 15.049765586853027, 11.460489273071289, 9.323226928710938, 17.56412124633789, 15.049766540527344, 11.223894119262695, 16.6998348236084, 19.64164924621582, 16.6998348236084, 14.252598762512207, 11.22388744354248, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 13.487650871276855, 16.6998348236084, 16.6998348236084, 16.6998348236084, 9.275910377502441, 16.6998348236084, 14.252598762512207, 16.6998348236084, 9.27591323852539, 11.223888397216797, 16.6998348236084, 9.323266983032227, 16.6998348236084, 17.56412124633789, 16.6998348236084, 7.163012504577637, 16.6998348236084, 14.252598762512207, 9.323250770568848, 9.27591323852539, 16.6998348236084, 11.223883628845215, 16.6998348236084, 15.049765586853027, 16.6998348236084, 15.049765586853027, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 11.46054744720459, 16.6998348236084, 14.252598762512207, 15.489964485168457, 9.323226928710938, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 15.049765586853027, 14.252598762512207, 14.252598762512207, 16.6998348236084, 9.275908470153809, 16.6998348236084, 17.56412124633789, 16.6998348236084, 16.6998348236084, 17.56410789489746, 7.816183090209961, 16.6998348236084, 14.252598762512207, 15.489951133728027, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 9.275904655456543, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 15.049765586853027, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 17.564115524291992, 15.049766540527344, 15.049765586853027, 16.6998348236084, 11.223894119262695, 17.56412124633789, 15.04976749420166, 16.6998348236084, 14.252598762512207, 16.6998348236084, 15.489968299865723, 16.6998348236084, 17.56410789489746, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 15.049763679504395, 14.252598762512207, 9.323266983032227, 9.3232421875, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 15.04976749420166, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 11.460487365722656, 15.049763679504395, 16.6998348236084, 14.252598762512207, 14.252598762512207, 15.049765586853027, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 9.27591323852539, 14.252598762512207, 9.323226928710938, 16.6998348236084, 17.56412124633789, 19.641647338867188, 15.049766540527344, 16.6998348236084, 15.489974021911621, 11.223894119262695, 14.252598762512207, 14.252598762512207, 14.252598762512207, 11.22389030456543, 15.049763679504395, 14.252598762512207, 14.252598762512207, 14.252598762512207, 14.252598762512207, 15.049766540527344, 16.6998348236084, 16.6998348236084, 14.252598762512207, 17.56412124633789, 14.252598762512207, 11.223894119262695, 19.89824867248535, 14.252598762512207, 9.275904655456543, 14.252598762512207, 9.323240280151367, 14.252598762512207, 15.04976749420166, 15.049765586853027, 11.223883628845215, 14.252598762512207, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 15.049766540527344, 11.460569381713867, 14.252598762512207, 14.252598762512207, 13.487650871276855, 16.6998348236084, 19.766164779663086, 9.323248863220215, 16.6998348236084, 14.252598762512207, 16.6998348236084, 17.56410789489746, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 16.6998348236084, 17.56412124633789, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 15.049765586853027, 14.252598762512207, 11.223886489868164, 13.487723350524902, 14.252598762512207, 17.564115524291992, 16.6998348236084, 16.6998348236084, 11.460528373718262, 16.6998348236084, 14.252598762512207, 13.487725257873535, 14.252598762512207, 19.641653060913086, 11.460526466369629, 16.6998348236084, 9.275904655456543, 14.252598762512207, 17.56412124633789, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 15.049766540527344, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 15.049766540527344, 14.252598762512207, 14.252598762512207, 11.223886489868164, 9.323246955871582, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 15.049765586853027, 9.323266983032227, 14.252598762512207, 15.049765586853027, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 15.049766540527344, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 11.223888397216797, 11.460485458374023, 14.252598762512207, 17.56412124633789, 14.252598762512207, 14.252598762512207, 11.223892211914062, 14.252598762512207, 16.6998348236084, 11.460526466369629, 14.252598762512207, 11.223892211914062, 9.27591323852539, 14.252598762512207, 17.56412124633789, 14.252598762512207, 9.27590560913086, 11.223883628845215, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 13.48762035369873, 17.564115524291992, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 15.049765586853027, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 11.223891258239746, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 13.48762035369873, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 17.564117431640625, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 15.049765586853027, 9.323248863220215, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 10.54818058013916, 15.049765586853027, 15.049766540527344, 16.6998348236084, 14.252598762512207, 14.252598762512207, 11.223892211914062, 14.252598762512207, 15.049765586853027, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 9.323248863220215, 14.252598762512207, 9.27591323852539, 14.252598762512207, 16.6998348236084, 11.223892211914062, 16.6998348236084, 14.252598762512207, 13.487725257873535, 16.6998348236084, 11.223886489868164, 15.489951133728027, 14.252598762512207, 9.3232421875, 9.323247909545898, 14.252598762512207, 14.252598762512207, 15.049763679504395, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 9.171496391296387, 14.252598762512207, 16.6998348236084, 9.323226928710938, 14.252598762512207, 16.6998348236084, 11.223883628845215, 16.6998348236084, 14.252598762512207, 15.049765586853027, 14.252598762512207, 15.049763679504395, 16.6998348236084, 14.252598762512207, 15.049765586853027, 16.6998348236084, 11.223892211914062, 15.049766540527344, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 9.275907516479492, 16.6998348236084, 11.460559844970703, 16.6998348236084, 19.64165496826172, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 9.32326889038086, 17.56410789489746, 16.6998348236084, 20.531679153442383, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 11.460578918457031, 14.252598762512207, 11.223892211914062, 16.6998348236084, 16.6998348236084, 16.6998348236084, 9.275909423828125, 9.275904655456543, 11.223886489868164, 19.64164161682129, 15.489970207214355, 14.252598762512207, 9.3232421875, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 19.641653060913086, 19.641691207885742, 14.252598762512207, 10.536850929260254, 11.460570335388184, 9.275904655456543, 14.252598762512207, 16.6998348236084, 14.252598762512207, 17.564117431640625, 9.323273658752441, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 15.049763679504395, 14.252598762512207, 16.6998348236084, 9.323226928710938, 16.6998348236084, 11.460577964782715, 16.6998348236084, 9.323254585266113, 14.252598762512207, 14.252598762512207, 11.460578918457031, 17.564119338989258, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 15.049766540527344, 14.252598762512207, 15.049763679504395, 19.64164924621582, 14.252598762512207, 15.04976749420166, 15.049763679504395, 14.252598762512207, 16.6998348236084, 14.252598762512207, 15.049763679504395, 14.252598762512207, 11.460548400878906, 14.252598762512207, 16.6998348236084, 16.6998348236084, 11.22388744354248, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 11.223892211914062, 14.252598762512207, 16.6998348236084, 11.460526466369629, 16.6998348236084, 16.6998348236084, 9.275909423828125, 16.6998348236084, 16.6998348236084, 14.252598762512207, 9.323219299316406, 16.6998348236084, 16.6998348236084, 15.049763679504395, 15.489962577819824, 14.252598762512207, 15.049763679504395, 14.252598762512207, 16.6998348236084, 15.049764633178711, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 11.223891258239746, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 9.55530834197998, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 13.487692832946777, 17.56410789489746, 9.3232421875, 14.252598762512207, 16.6998348236084, 14.252598762512207, 16.6998348236084, 11.223889350891113, 14.252598762512207, 14.252598762512207, 14.252598762512207, 14.252598762512207, 15.049763679504395, 11.223892211914062, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 16.6998348236084, 9.323217391967773, 9.275909423828125, 14.252598762512207, 11.223892211914062, 15.049763679504395, 14.252598762512207, 16.6998348236084, 13.487645149230957, 16.6998348236084, 11.223892211914062, 14.252598762512207, 9.275910377502441, 14.252598762512207, 15.049764633178711, 15.049763679504395, 13.487645149230957, 14.252598762512207, 13.487725257873535, 16.6998348236084, 16.6998348236084, 14.252598762512207, 9.27591323852539, 15.049766540527344, 16.6998348236084, 17.564123153686523, 15.049763679504395, 16.6998348236084, 16.6998348236084, 11.223883628845215, 16.6998348236084, 14.252598762512207, 9.275899887084961, 16.6998348236084, 16.6998348236084, 11.223892211914062, 14.252598762512207, 14.252598762512207, 14.252598762512207, 15.04976749420166, 14.252598762512207, 14.252598762512207, 14.252598762512207, 14.252598762512207, 15.049766540527344, 16.6998348236084, 13.487652778625488, 16.6998348236084, 14.252598762512207, 15.489951133728027, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 11.223883628845215, 14.252598762512207, 14.252598762512207, 19.641653060913086, 15.049763679504395, 16.6998348236084, 14.252598762512207, 20.4464054107666, 16.6998348236084, 13.487686157226562, 15.489974021911621, 14.252598762512207, 9.275904655456543, 16.6998348236084, 17.564123153686523, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 15.049763679504395, 16.6998348236084, 14.252598762512207, 13.487625122070312, 14.252598762512207, 11.460567474365234, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 17.564119338989258, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 9.323254585266113, 14.252598762512207, 14.252598762512207, 14.252598762512207, 14.252598762512207, 17.564123153686523, 9.27591323852539, 11.223888397216797, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 15.049763679504395, 16.6998348236084, 16.6998348236084, 9.275904655456543, 16.6998348236084, 15.489962577819824, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 9.27591323852539, 16.6998348236084, 13.487709045410156, 14.252598762512207, 9.275909423828125, 14.252598762512207, 16.6998348236084, 14.252598762512207, 16.6998348236084, 9.27591323852539, 14.252598762512207, 14.252598762512207, 19.64164924621582, 11.223886489868164, 14.252598762512207, 16.6998348236084, 9.323270797729492, 14.252598762512207, 15.489977836608887, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 9.275904655456543, 14.252598762512207, 16.6998348236084, 14.252598762512207, 13.487652778625488, 16.6998348236084, 15.049764633178711, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 15.049763679504395, 14.252598762512207, 11.460551261901855, 16.6998348236084, 19.641647338867188, 9.275909423828125, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 15.049763679504395, 9.323256492614746, 14.252598762512207, 15.049764633178711, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 9.275904655456543, 14.252598762512207, 15.049763679504395, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 11.223886489868164, 16.6998348236084, 14.252598762512207, 16.6998348236084, 10.632922172546387, 9.32325267791748, 9.275904655456543, 15.049763679504395, 14.252598762512207, 11.22388744354248, 14.252598762512207, 15.04976749420166, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 19.956436157226562, 19.969606399536133, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 15.049763679504395, 13.487621307373047, 16.6998348236084, 15.489954948425293, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 15.049763679504395, 16.6998348236084, 16.6998348236084, 14.252598762512207, 11.223883628845215, 16.6998348236084, 14.252598762512207, 14.252598762512207, 15.049763679504395, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 11.460548400878906, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 16.6998348236084, 15.049763679504395, 16.6998348236084, 9.27590274810791, 15.049763679504395, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 17.564123153686523, 14.252598762512207, 14.252598762512207, 14.252598762512207, 17.564123153686523, 16.6998348236084, 14.252598762512207, 15.049763679504395, 16.6998348236084, 16.6998348236084, 14.252598762512207, 15.04976749420166, 15.049764633178711, 17.564119338989258, 17.564123153686523, 16.6998348236084, 16.6998348236084, 11.460528373718262, 11.460528373718262, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.19547462463379, 15.049764633178711, 16.6998348236084, 16.6998348236084, 15.049763679504395, 15.049764633178711, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 11.46054744720459, 11.223892211914062, 15.049763679504395, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 11.460548400878906, 13.487712860107422, 16.6998348236084, 16.6998348236084, 9.3232421875, 15.489962577819824, 11.223883628845215, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 9.275910377502441, 16.6998348236084, 14.252598762512207, 9.275904655456543, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 11.223883628845215, 14.252598762512207, 14.252598762512207, 16.6998348236084, 15.049764633178711, 9.323226928710938, 16.6998348236084, 14.252598762512207, 14.252598762512207, 15.049763679504395, 14.252598762512207, 16.6998348236084, 13.487725257873535, 16.6998348236084, 15.049764633178711, 15.04976749420166, 15.04976749420166, 16.6998348236084, 17.564119338989258, 16.6998348236084, 14.252598762512207, 9.275901794433594, 11.460490226745605, 16.6998348236084, 15.049763679504395, 15.489951133728027, 14.252598762512207, 15.049764633178711, 14.252598762512207, 15.049763679504395, 14.252598762512207, 14.252598762512207, 14.252598762512207, 9.275910377502441, 16.6998348236084, 16.6998348236084, 19.641639709472656, 16.6998348236084, 16.6998348236084, 13.487652778625488, 16.6998348236084, 17.56412124633789, 16.6998348236084, 14.252598762512207, 17.564117431640625, 15.049764633178711, 15.489956855773926, 15.049764633178711, 16.6998348236084, 14.252598762512207, 17.564111709594727, 15.049763679504395, 16.6998348236084, 14.252598762512207, 14.252598762512207, 15.049763679504395, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 9.275904655456543, 16.6998348236084, 11.255741119384766, 14.252598762512207, 9.275904655456543, 16.6998348236084, 14.252598762512207, 16.6998348236084, 17.56410789489746, 16.6998348236084, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 15.049763679504395, 16.6998348236084, 16.6998348236084, 15.04976749420166, 14.252598762512207, 14.252598762512207, 11.46048641204834, 9.27591323852539, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 15.049764633178711, 14.252598762512207, 14.252598762512207, 9.275904655456543, 16.6998348236084, 11.223889350891113, 14.252598762512207, 16.6998348236084, 16.6998348236084, 11.460491180419922, 15.049763679504395, 14.252598762512207, 14.252598762512207, 14.252598762512207, 11.460528373718262, 9.323234558105469, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 15.04976749420166, 14.252598762512207, 16.6998348236084, 14.252598762512207, 16.6998348236084, 17.564119338989258, 16.6998348236084, 9.27591323852539, 16.6998348236084, 14.252598762512207, 16.6998348236084, 15.049763679504395, 16.6998348236084, 14.252598762512207, 14.252598762512207, 15.049763679504395, 14.252598762512207, 13.487652778625488, 16.6998348236084, 16.6998348236084, 9.275904655456543, 14.252598762512207, 9.275909423828125, 14.252598762512207, 15.049764633178711, 9.323251724243164, 11.223884582519531, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 15.049764633178711, 14.252598762512207, 14.252598762512207, 14.252598762512207, 14.252598762512207, 14.252598762512207, 15.049763679504395, 19.641653060913086, 14.252598762512207, 14.252598762512207, 11.223883628845215, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 19.64165687561035, 9.27591323852539, 19.641653060913086, 16.6998348236084, 16.6998348236084, 11.223892211914062, 15.489954948425293, 14.252598762512207, 9.275904655456543, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 17.56412124633789, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 19.641653060913086, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 15.049763679504395, 9.275910377502441, 16.6998348236084, 9.275904655456543, 14.252598762512207, 16.6998348236084, 15.049764633178711, 14.252598762512207, 16.6998348236084, 19.641651153564453, 9.275904655456543, 16.6998348236084, 16.6998348236084, 9.275910377502441, 11.460548400878906, 9.275904655456543, 15.049763679504395, 14.252598762512207, 14.252598762512207, 14.252598762512207, 9.323225975036621, 14.252598762512207, 11.223886489868164, 9.275904655456543, 14.252598762512207, 15.049763679504395, 11.223892211914062, 16.6998348236084, 15.049764633178711, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 17.56412124633789, 15.049763679504395, 16.6998348236084, 14.252598762512207, 15.049763679504395, 11.223892211914062, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 14.252598762512207, 15.049763679504395, 9.323266983032227, 9.275911331176758, 14.252598762512207, 13.487686157226562, 16.6998348236084, 15.049763679504395, 14.252598762512207, 11.460528373718262, 14.252598762512207, 15.489960670471191, 9.275904655456543, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 15.049763679504395, 15.049763679504395, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 14.252598762512207, 16.6998348236084, 11.223889350891113, 15.049763679504395, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 11.223888397216797, 16.6998348236084, 11.223883628845215, 11.223883628845215, 14.252598762512207, 15.049763679504395, 11.223884582519531, 16.6998348236084, 9.3232421875, 19.77511215209961, 9.323248863220215, 9.27591323852539, 11.223883628845215, 15.049763679504395, 15.049763679504395, 15.049764633178711, 16.6998348236084, 14.252598762512207, 16.6998348236084, 16.6998348236084, 11.460579872131348, 16.6998348236084, 14.252598762512207, 15.04976749420166, 16.6998348236084, 17.56410789489746, 14.252598762512207, 14.252598762512207, 15.049763679504395, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 15.049763679504395, 16.6998348236084, 15.049763679504395, 16.6998348236084, 16.6998348236084, 13.487712860107422, 16.6998348236084, 11.627540588378906, 16.6998348236084, 15.049763679504395, 16.6998348236084, 19.641647338867188, 11.223883628845215, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 17.56411361694336, 16.6998348236084, 14.252598762512207, 16.6998348236084, 15.049764633178711, 19.64164161682129, 9.275904655456543, 16.6998348236084, 9.275904655456543, 14.252598762512207, 15.049766540527344, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 17.564117431640625, 14.252598762512207, 16.6998348236084, 16.6998348236084, 15.049763679504395, 16.6998348236084, 9.275904655456543, 11.22388744354248, 15.049763679504395, 9.275910377502441, 16.6998348236084, 16.6998348236084, 13.487725257873535, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 11.460528373718262, 11.223883628845215, 11.223892211914062, 16.6998348236084, 16.6998348236084, 16.6998348236084, 14.252598762512207, 17.564123153686523, 17.564119338989258, 16.6998348236084, 13.487625122070312, 14.252598762512207, 14.252598762512207, 14.252598762512207, 15.489962577819824, 11.223883628845215, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 17.564123153686523, 11.460550308227539, 10.536931037902832, 15.049766540527344, 16.6998348236084, 14.252598762512207, 15.049763679504395, 14.252598762512207, 16.6998348236084, 14.252598762512207, 11.223891258239746, 14.252598762512207, 14.252598762512207, 15.049763679504395, 16.6998348236084, 14.252598762512207, 16.6998348236084, 27.80360221862793, 17.564123153686523, 14.252598762512207, 11.223886489868164, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 14.252598762512207, 11.22388744354248, 16.6998348236084, 16.6998348236084, 16.6998348236084, 16.6998348236084, 9.27591323852539, 14.252598762512207, 15.049763679504395, 15.049763679504395, 14.252598762512207, 15.049763679504395, 16.6998348236084, 9.275904655456543, 16.6998348236084, 16.6998348236084, 15.049766540527344, 16.6998348236084, 9.27591323852539, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207, 9.323266983032227, 16.6998348236084, 16.6998348236084, 10.547945976257324, 9.275909423828125, 15.489962577819824, 14.252598762512207, 15.049763679504395, 15.049765586853027, 13.487675666809082, 16.6998348236084, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 16.6998348236084, 9.275904655456543, 14.252598762512207, 14.252598762512207, 16.6998348236084, 13.487713813781738, 16.6998348236084, 14.252598762512207, 16.6998348236084, 14.252598762512207, 14.252598762512207, 14.252598762512207, 16.6998348236084, 16.6998348236084, 14.252598762512207]\n",
            "Mean Squared Error: 103.0622\n",
            "Mean Absolute Error: 6.4803\n",
            "Root Mean Squared Error (RMSE): 10.1520\n",
            "Mean Absolute Percentage Error (MAPE): 0.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.2 Model 3\n",
        "From the model above, the improvements made to model 2 were the use of normalizing the outputs from the hidden layers using the batchnorm1d of pytorch library and using the dropout to set a 50% chance for some of the neurons to be set to 0 to prevent the network from being too reliant on several neurons for the output in each forward pass."
      ],
      "metadata": {
        "id": "1unYes3_5Udf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the GNN model\n",
        "class GNN3(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GNN3, self).__init__()\n",
        "        ###convolutional layer 1 to extract richer features from the data\n",
        "        self.conv1 = GCNConv(in_channels=x.size(1), out_channels=32)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(32)\n",
        "        ### convolutional layer 2 further refining these features into a more compact form.\n",
        "        self.conv2 = GCNConv(in_channels=32, out_channels=16)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(16)\n",
        "        self.conv3 = GCNConv(in_channels=16, out_channels=8)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(8)\n",
        "        self.fc = torch.nn.Linear(8, 1)\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        embeddings = x\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "iM_6okQSYM7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model_Iteration3 = GNN3()\n",
        "criterion = torch.nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model_Iteration3.parameters(), lr=0.005)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 15  # Number of epochs to wait for improvement\n",
        "best_val_loss = float('inf')  # Initialize with a large value\n",
        "epochs_no_improve = 0  # Count of epochs since the last improvement\n",
        "\n",
        "\n",
        "for epoch in range(500):\n",
        "  model_Iteration3.train()\n",
        "  train_loss = 0\n",
        "  for batch in train_loader2:\n",
        "      optimizer.zero_grad()\n",
        "      out = model_Iteration3(batch)\n",
        "      loss = criterion(out, batch.y.view(-1, 1).float())\n",
        "      if torch.isnan(loss).sum() > 0:\n",
        "          print(\"NaN loss encountered at epoch:\", epoch)\n",
        "          break\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model_Iteration3.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item() * batch.num_graphs\n",
        "  train_loss /= len(train_loader2.dataset)\n",
        "\n",
        "\n",
        "  model_Iteration3.eval()  # Set the model to evaluation mode\n",
        "  val_loss = 0  # Initialize validation loss\n",
        "\n",
        "  with torch.no_grad():  # No gradient computation\n",
        "      for batch in val_loader2:\n",
        "          out = model_Iteration3(batch)  # Forward pass\n",
        "          loss = criterion(out, batch.y.view(-1, 1).float())  # Compute loss\n",
        "          val_loss += loss.item() * batch.num_graphs  # Accumulate validation loss\n",
        "\n",
        "  val_loss /= len(val_loader2.dataset)  # Average validation loss\n",
        "  print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f},Loss: {train_loss:.4f},best_val_loss:{best_val_loss:.4f}')\n",
        "\n",
        "  # Early stopping check\n",
        "  if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      epochs_no_improve = 0\n",
        "      # Save the best model\n",
        "      torch.save(model_Iteration3.state_dict(), 'best_model2.pt')\n",
        "  else:\n",
        "      epochs_no_improve += 1\n",
        "\n",
        "    # Check if early stopping criterion is met\n",
        "  if epochs_no_improve >= patience:\n",
        "      print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "      break\n",
        "\n",
        "    # Set the model back to training mode for the next epoch\n",
        "  model_Iteration3.train()"
      ],
      "metadata": {
        "id": "luNN1wjGZI8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3.2 Model 3 Evaluation\n",
        "Model 3 has shown little improvements or has downgraded from the original better metrics in model 2. It performed worse than the baseline model which can be seen from the mean absolute error of 6.40 to the model's accuracy of 6.48.\n",
        "\n"
      ],
      "metadata": {
        "id": "QOtRQsEr6Apn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "model_Iteration3.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader2:\n",
        "        out = model_Iteration3(batch)\n",
        "        predictions.extend(out.squeeze().cpu().numpy().tolist())\n",
        "\n",
        "print(f'Predicted valuations: {predictions}')\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "\n",
        "# Convert predictions and targets to numpy arrays for evaluation\n",
        "actuals = test_data.y.numpy()\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(actuals, predictions)\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "rmse = mean_squared_error(actuals, predictions, squared=False)\n",
        "mape = mean_absolute_percentage_error(actuals, predictions)\n",
        "\n",
        "\n",
        "print(f'Mean Squared Error: {mse:.4f}')\n",
        "print(f'Mean Absolute Error: {mae:.4f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {mape:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LRmoUYdZWrQ",
        "outputId": "2eff0cbc-68aa-4dc5-b266-c3cbb028f4d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted valuations: [15.818586349487305, 15.818586349487305, 17.578813552856445, 15.818586349487305, 14.092850685119629, 15.818586349487305, 17.578813552856445, 14.468147277832031, 14.015161514282227, 14.092851638793945, 17.578813552856445, 14.092850685119629, 14.092851638793945, 15.818586349487305, 13.16054916381836, 13.160540580749512, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 14.366205215454102, 17.578813552856445, 14.468147277832031, 15.818586349487305, 15.818586349487305, 15.511839866638184, 15.818586349487305, 14.468147277832031, 15.818586349487305, 15.818586349487305, 15.818586349487305, 20.05028533935547, 15.818586349487305, 17.578811645507812, 14.092850685119629, 14.468146324157715, 17.578811645507812, 17.578813552856445, 14.015161514282227, 15.818586349487305, 15.818586349487305, 17.578811645507812, 17.578813552856445, 13.160477638244629, 14.015159606933594, 17.578813552856445, 17.578813552856445, 15.818586349487305, 17.578811645507812, 15.818586349487305, 14.366202354431152, 14.015159606933594, 14.015161514282227, 14.015159606933594, 14.366204261779785, 13.966045379638672, 16.043010711669922, 14.015161514282227, 14.468147277832031, 15.818586349487305, 20.050352096557617, 15.818586349487305, 17.578813552856445, 14.468147277832031, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578813552856445, 15.818586349487305, 13.16054916381836, 15.818586349487305, 15.818586349487305, 15.818586349487305, 14.092850685119629, 15.818586349487305, 17.578813552856445, 15.818586349487305, 14.092851638793945, 14.468147277832031, 15.818586349487305, 13.966049194335938, 15.818586349487305, 16.04300880432129, 15.818586349487305, 15.15367317199707, 15.818586349487305, 17.578813552856445, 13.966046333312988, 14.092851638793945, 15.818586349487305, 14.468147277832031, 15.818586349487305, 14.01516056060791, 15.818586349487305, 14.015161514282227, 17.578813552856445, 17.578813552856445, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578813552856445, 17.578813552856445, 15.818586349487305, 17.578811645507812, 14.366209030151367, 15.818586349487305, 17.578811645507812, 15.511778831481934, 13.966044425964355, 17.578811645507812, 15.818586349487305, 17.578811645507812, 17.578813552856445, 15.818586349487305, 14.015161514282227, 17.578811645507812, 17.578813552856445, 15.818586349487305, 14.092850685119629, 15.818586349487305, 16.04300880432129, 15.818586349487305, 15.818586349487305, 16.042997360229492, 16.422224044799805, 15.818586349487305, 17.578813552856445, 15.511916160583496, 15.818586349487305, 17.578813552856445, 17.578811645507812, 15.818586349487305, 15.818586349487305, 17.578813552856445, 14.092850685119629, 17.578811645507812, 17.578813552856445, 15.818586349487305, 17.578813552856445, 14.015159606933594, 15.818586349487305, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578813552856445, 15.818586349487305, 16.04300308227539, 14.015161514282227, 14.015161514282227, 15.818586349487305, 14.468147277832031, 16.04300880432129, 14.015161514282227, 15.818586349487305, 17.578813552856445, 15.818586349487305, 15.51177978515625, 15.818586349487305, 16.042997360229492, 17.578813552856445, 15.818586349487305, 15.818586349487305, 15.818586349487305, 14.015161514282227, 17.578813552856445, 13.966049194335938, 13.966046333312988, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578811645507812, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 14.015161514282227, 15.818586349487305, 17.578813552856445, 17.578813552856445, 15.818586349487305, 14.366202354431152, 14.015161514282227, 15.818586349487305, 17.578813552856445, 17.578813552856445, 14.015159606933594, 15.818586349487305, 15.818586349487305, 17.578811645507812, 15.818586349487305, 17.578813552856445, 14.092851638793945, 17.578813552856445, 13.966044425964355, 15.818586349487305, 16.043004989624023, 20.050350189208984, 14.015161514282227, 15.818586349487305, 15.511884689331055, 14.468147277832031, 17.578811645507812, 17.578811645507812, 17.578813552856445, 14.468147277832031, 14.015161514282227, 17.578813552856445, 17.578813552856445, 17.578813552856445, 17.578813552856445, 14.01516056060791, 15.818586349487305, 15.818586349487305, 17.578811645507812, 16.043010711669922, 17.578813552856445, 14.468147277832031, 9.55377197265625, 17.578813552856445, 14.092850685119629, 17.578811645507812, 13.966046333312988, 17.578811645507812, 14.015161514282227, 14.01516056060791, 14.468147277832031, 17.578813552856445, 15.818586349487305, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578811645507812, 14.015161514282227, 14.366207122802734, 17.578813552856445, 17.578811645507812, 13.16055679321289, 15.818586349487305, 13.474069595336914, 13.966047286987305, 15.818586349487305, 17.578813552856445, 15.818586349487305, 16.042993545532227, 17.578813552856445, 17.578813552856445, 17.578811645507812, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578813552856445, 15.818586349487305, 17.578813552856445, 15.818586349487305, 16.043006896972656, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578813552856445, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 14.015161514282227, 17.578813552856445, 14.468147277832031, 13.160479545593262, 17.578813552856445, 16.042999267578125, 15.818586349487305, 15.818586349487305, 14.366207122802734, 15.818586349487305, 17.578813552856445, 13.160475730895996, 17.578813552856445, 20.050338745117188, 14.366203308105469, 15.818586349487305, 14.092850685119629, 17.578813552856445, 16.043006896972656, 15.818586349487305, 17.578811645507812, 17.578813552856445, 15.818586349487305, 14.01516056060791, 15.818586349487305, 17.578811645507812, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578811645507812, 15.818586349487305, 14.01516056060791, 17.578813552856445, 17.578811645507812, 14.468147277832031, 13.966047286987305, 17.578811645507812, 17.578811645507812, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578811645507812, 17.578813552856445, 14.015161514282227, 13.966049194335938, 17.578813552856445, 14.015161514282227, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578811645507812, 17.578813552856445, 14.015161514282227, 17.578813552856445, 17.578811645507812, 15.818586349487305, 17.578813552856445, 17.578813552856445, 15.818586349487305, 15.818586349487305, 14.468146324157715, 14.366201400756836, 17.578813552856445, 16.04300880432129, 17.578813552856445, 17.578813552856445, 14.468147277832031, 17.578813552856445, 15.818586349487305, 14.366205215454102, 17.578811645507812, 14.468147277832031, 14.092851638793945, 17.578811645507812, 16.043006896972656, 17.578811645507812, 14.092851638793945, 14.468147277832031, 17.578813552856445, 15.818586349487305, 17.578811645507812, 17.578811645507812, 13.160582542419434, 16.04300308227539, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578811645507812, 15.818586349487305, 15.818586349487305, 17.578813552856445, 14.01516056060791, 15.818586349487305, 17.578813552856445, 17.578811645507812, 17.578813552856445, 14.468147277832031, 17.578813552856445, 17.578811645507812, 17.578811645507812, 15.818586349487305, 17.578811645507812, 15.818586349487305, 17.578813552856445, 15.818586349487305, 17.578811645507812, 17.578811645507812, 13.1605806350708, 17.578813552856445, 17.578813552856445, 15.818586349487305, 15.818586349487305, 16.04300880432129, 15.818586349487305, 17.578813552856445, 15.818586349487305, 17.578813552856445, 17.578813552856445, 14.015161514282227, 13.966047286987305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578811645507812, 11.309505462646484, 14.015161514282227, 14.015161514282227, 15.818586349487305, 17.578813552856445, 17.578813552856445, 14.468147277832031, 17.578813552856445, 14.015161514282227, 17.578811645507812, 17.578811645507812, 15.818586349487305, 17.578813552856445, 13.966047286987305, 17.578813552856445, 14.092851638793945, 17.578813552856445, 15.818586349487305, 14.468147277832031, 15.818586349487305, 17.578811645507812, 13.16047477722168, 15.818586349487305, 14.468147277832031, 15.511914253234863, 17.578811645507812, 13.966046333312988, 13.966047286987305, 17.578811645507812, 17.578811645507812, 14.015159606933594, 15.818586349487305, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578811645507812, 17.578811645507812, 13.6541748046875, 17.578813552856445, 15.818586349487305, 13.966044425964355, 17.578811645507812, 15.818586349487305, 14.468147277832031, 15.818586349487305, 17.578811645507812, 14.01516056060791, 17.578813552856445, 14.015159606933594, 15.818586349487305, 17.578811645507812, 14.01516056060791, 15.818586349487305, 14.468147277832031, 14.01516056060791, 17.578811645507812, 17.578811645507812, 15.818586349487305, 17.578811645507812, 14.092850685119629, 15.818586349487305, 14.366205215454102, 15.818586349487305, 20.050296783447266, 15.818586349487305, 15.818586349487305, 17.578813552856445, 15.818586349487305, 17.578811645507812, 17.578811645507812, 15.818586349487305, 15.818586349487305, 13.966049194335938, 16.042993545532227, 15.818586349487305, 9.588071823120117, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 14.366207122802734, 17.578813552856445, 14.468147277832031, 15.818586349487305, 15.818586349487305, 15.818586349487305, 14.092851638793945, 14.092852592468262, 14.468147277832031, 20.050336837768555, 15.511903762817383, 17.578813552856445, 13.966046333312988, 15.818586349487305, 15.818586349487305, 17.578811645507812, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578811645507812, 20.050315856933594, 20.050369262695312, 17.578811645507812, 11.334554672241211, 14.366207122802734, 14.092850685119629, 17.578813552856445, 15.818586349487305, 17.578811645507812, 16.04300308227539, 13.966050148010254, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578813552856445, 15.818586349487305, 17.578811645507812, 14.015159606933594, 17.578813552856445, 15.818586349487305, 13.966044425964355, 15.818586349487305, 14.366207122802734, 15.818586349487305, 13.966048240661621, 17.578813552856445, 17.578813552856445, 14.366207122802734, 16.043010711669922, 17.578813552856445, 17.578813552856445, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578813552856445, 14.015161514282227, 17.578813552856445, 14.015159606933594, 20.05031967163086, 17.578811645507812, 14.015161514282227, 14.01516056060791, 17.578811645507812, 15.818586349487305, 17.578813552856445, 14.015161514282227, 17.578811645507812, 14.366205215454102, 17.578813552856445, 15.818586349487305, 15.818586349487305, 14.468147277832031, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 14.468147277832031, 17.578811645507812, 15.818586349487305, 14.366203308105469, 15.818586349487305, 15.818586349487305, 14.092850685119629, 15.818586349487305, 15.818586349487305, 17.578813552856445, 13.96605396270752, 15.818586349487305, 15.818586349487305, 14.015161514282227, 15.511898040771484, 17.578811645507812, 14.015161514282227, 17.578813552856445, 15.818586349487305, 14.01516056060791, 15.818586349487305, 15.818586349487305, 17.578811645507812, 15.818586349487305, 17.578811645507812, 15.818586349487305, 15.818586349487305, 17.578811645507812, 17.578813552856445, 17.578811645507812, 14.468147277832031, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 13.75802993774414, 15.818586349487305, 15.818586349487305, 17.578813552856445, 15.818586349487305, 17.578811645507812, 17.578811645507812, 17.578813552856445, 13.160516738891602, 16.04299545288086, 13.966047286987305, 17.578811645507812, 15.818586349487305, 17.578813552856445, 15.818586349487305, 14.468147277832031, 17.578811645507812, 17.578813552856445, 17.578813552856445, 17.578813552856445, 14.015161514282227, 14.468147277832031, 17.578813552856445, 17.578813552856445, 15.818586349487305, 17.578813552856445, 15.818586349487305, 13.966043472290039, 14.092851638793945, 17.578811645507812, 14.468147277832031, 14.015161514282227, 17.578813552856445, 15.818586349487305, 13.16055679321289, 15.818586349487305, 14.468146324157715, 17.578813552856445, 14.092850685119629, 17.578813552856445, 14.015161514282227, 14.015161514282227, 13.160558700561523, 17.578811645507812, 13.160472869873047, 15.818586349487305, 15.818586349487305, 17.578811645507812, 14.092851638793945, 14.015161514282227, 15.818586349487305, 16.043004989624023, 14.015161514282227, 15.818586349487305, 15.818586349487305, 14.468147277832031, 15.818586349487305, 17.578811645507812, 14.092850685119629, 15.818586349487305, 15.818586349487305, 14.468147277832031, 17.578813552856445, 17.578813552856445, 17.578813552856445, 14.015161514282227, 17.578813552856445, 17.578813552856445, 17.578813552856445, 17.578813552856445, 14.015161514282227, 15.818586349487305, 13.160551071166992, 15.818586349487305, 17.578813552856445, 15.511914253234863, 17.578813552856445, 17.578813552856445, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 15.818586349487305, 17.578813552856445, 17.578813552856445, 15.818586349487305, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578813552856445, 15.818586349487305, 17.578811645507812, 17.578811645507812, 17.578813552856445, 14.468147277832031, 17.578813552856445, 17.578813552856445, 20.05034065246582, 14.015161514282227, 15.818586349487305, 17.578813552856445, 25.206138610839844, 15.818586349487305, 13.160512924194336, 15.511893272399902, 17.578813552856445, 14.092850685119629, 15.818586349487305, 16.04300880432129, 17.578813552856445, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578811645507812, 14.015161514282227, 15.818586349487305, 17.578811645507812, 13.160593032836914, 17.578813552856445, 14.366205215454102, 17.578813552856445, 17.578811645507812, 17.578811645507812, 15.818586349487305, 16.04300880432129, 15.818586349487305, 17.578813552856445, 17.578811645507812, 17.578813552856445, 13.966048240661621, 17.578813552856445, 17.578813552856445, 17.578813552856445, 17.578811645507812, 16.043006896972656, 14.092851638793945, 14.468147277832031, 17.578813552856445, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578811645507812, 14.015159606933594, 15.818586349487305, 15.818586349487305, 14.092850685119629, 15.818586349487305, 15.511894226074219, 17.578811645507812, 15.818586349487305, 15.818586349487305, 15.818586349487305, 14.092851638793945, 15.818586349487305, 13.16048526763916, 17.578811645507812, 14.092851638793945, 17.578813552856445, 15.818586349487305, 17.578813552856445, 15.818586349487305, 14.092851638793945, 17.578813552856445, 17.578813552856445, 20.050355911254883, 14.468147277832031, 17.578813552856445, 15.818586349487305, 13.966049194335938, 17.578811645507812, 15.51185417175293, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 14.092851638793945, 17.578813552856445, 15.818586349487305, 17.578813552856445, 13.160550117492676, 15.818586349487305, 14.01516056060791, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578811645507812, 17.578813552856445, 15.818586349487305, 17.578813552856445, 14.01516056060791, 17.578813552856445, 14.366205215454102, 15.818586349487305, 20.050315856933594, 14.092851638793945, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 15.818586349487305, 17.578811645507812, 17.578813552856445, 14.015161514282227, 13.966048240661621, 17.578813552856445, 14.01516056060791, 15.818586349487305, 17.578813552856445, 17.578811645507812, 17.578813552856445, 14.092850685119629, 17.578811645507812, 14.01516056060791, 15.818586349487305, 17.578813552856445, 17.578811645507812, 15.818586349487305, 17.578813552856445, 15.818586349487305, 17.578813552856445, 17.578811645507812, 15.818586349487305, 14.468147277832031, 15.818586349487305, 17.578811645507812, 15.818586349487305, 13.205412864685059, 13.966048240661621, 14.092850685119629, 14.015161514282227, 17.578811645507812, 14.468146324157715, 17.578813552856445, 14.015161514282227, 15.818586349487305, 15.818586349487305, 17.578813552856445, 15.818586349487305, 28.586557388305664, 13.642695426940918, 15.818586349487305, 17.578813552856445, 15.818586349487305, 15.818586349487305, 14.015159606933594, 13.16066837310791, 15.818586349487305, 15.511914253234863, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578813552856445, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578811645507812, 15.818586349487305, 15.818586349487305, 14.015161514282227, 15.818586349487305, 15.818586349487305, 17.578811645507812, 14.468146324157715, 15.818586349487305, 17.578811645507812, 17.578813552856445, 14.015159606933594, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578811645507812, 17.578811645507812, 15.818586349487305, 17.578813552856445, 14.366205215454102, 15.818586349487305, 15.818586349487305, 17.578811645507812, 17.578811645507812, 15.818586349487305, 17.578811645507812, 15.818586349487305, 14.01516056060791, 15.818586349487305, 14.092851638793945, 14.015161514282227, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 16.043010711669922, 17.578811645507812, 17.578813552856445, 17.578811645507812, 16.04300880432129, 15.818586349487305, 17.578811645507812, 14.015161514282227, 15.818586349487305, 15.818586349487305, 17.578811645507812, 14.015161514282227, 14.01516056060791, 16.043010711669922, 16.043010711669922, 15.818586349487305, 15.818586349487305, 14.366203308105469, 14.366203308105469, 15.818586349487305, 17.578813552856445, 15.818586349487305, 17.578811645507812, 17.578813552856445, 13.920713424682617, 14.015161514282227, 15.818586349487305, 15.818586349487305, 14.01516056060791, 14.01516056060791, 15.818586349487305, 17.578813552856445, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578811645507812, 17.578811645507812, 15.818586349487305, 17.578811645507812, 14.366209030151367, 14.468147277832031, 14.015159606933594, 15.818586349487305, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578811645507812, 15.818586349487305, 14.36620807647705, 13.160484313964844, 15.818586349487305, 15.818586349487305, 13.966047286987305, 15.51191520690918, 14.468146324157715, 15.818586349487305, 17.578813552856445, 17.578813552856445, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 14.092851638793945, 15.818586349487305, 17.578811645507812, 14.092850685119629, 17.578811645507812, 17.578813552856445, 15.818586349487305, 17.578813552856445, 14.468147277832031, 17.578811645507812, 17.578813552856445, 15.818586349487305, 14.015161514282227, 13.966045379638672, 15.818586349487305, 17.578811645507812, 17.578811645507812, 14.015159606933594, 17.578811645507812, 15.818586349487305, 13.160478591918945, 15.818586349487305, 14.01516056060791, 14.015161514282227, 14.015161514282227, 15.818586349487305, 16.043010711669922, 15.818586349487305, 17.578813552856445, 14.092850685119629, 14.366201400756836, 15.818586349487305, 14.01516056060791, 15.511868476867676, 17.578813552856445, 14.01516056060791, 17.578813552856445, 14.015161514282227, 17.578811645507812, 17.578811645507812, 17.578813552856445, 14.092850685119629, 15.818586349487305, 15.818586349487305, 20.05035400390625, 15.818586349487305, 15.818586349487305, 13.160548210144043, 15.818586349487305, 16.043006896972656, 15.818586349487305, 17.578813552856445, 16.04300308227539, 14.015161514282227, 15.511900901794434, 14.015161514282227, 15.818586349487305, 17.578811645507812, 16.042997360229492, 14.01516056060791, 15.818586349487305, 17.578811645507812, 17.578813552856445, 14.015159606933594, 15.818586349487305, 17.578811645507812, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 14.092850685119629, 15.818586349487305, 14.32820987701416, 17.578811645507812, 14.092851638793945, 15.818586349487305, 17.578813552856445, 15.818586349487305, 16.04298973083496, 15.818586349487305, 15.818586349487305, 17.578811645507812, 15.818586349487305, 17.578811645507812, 17.578811645507812, 14.01516056060791, 15.818586349487305, 15.818586349487305, 14.015161514282227, 17.578813552856445, 17.578813552856445, 14.366203308105469, 14.092851638793945, 17.578813552856445, 17.578813552856445, 17.578813552856445, 15.818586349487305, 17.578813552856445, 17.578813552856445, 17.578813552856445, 14.01516056060791, 17.578813552856445, 17.578813552856445, 14.092851638793945, 15.818586349487305, 14.468147277832031, 17.578813552856445, 15.818586349487305, 15.818586349487305, 14.366201400756836, 14.01516056060791, 17.578813552856445, 17.578813552856445, 17.578813552856445, 14.366203308105469, 13.966045379638672, 15.818586349487305, 17.578813552856445, 15.818586349487305, 17.578813552856445, 17.578813552856445, 17.578813552856445, 14.015161514282227, 17.578813552856445, 15.818586349487305, 17.578813552856445, 15.818586349487305, 16.04300880432129, 15.818586349487305, 14.092851638793945, 15.818586349487305, 17.578813552856445, 15.818586349487305, 14.015159606933594, 15.818586349487305, 17.578813552856445, 17.578813552856445, 14.01516056060791, 17.578813552856445, 13.160551071166992, 15.818586349487305, 15.818586349487305, 14.092850685119629, 17.578813552856445, 14.092851638793945, 17.578813552856445, 14.01516056060791, 13.966047286987305, 14.468147277832031, 17.578813552856445, 17.578813552856445, 17.578813552856445, 15.818586349487305, 14.01516056060791, 17.578813552856445, 17.578813552856445, 17.578811645507812, 17.578813552856445, 17.578813552856445, 14.015161514282227, 20.050329208374023, 17.578813552856445, 17.578813552856445, 14.468147277832031, 17.578813552856445, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 20.050310134887695, 14.092851638793945, 20.05031394958496, 15.818586349487305, 15.818586349487305, 14.468147277832031, 15.511892318725586, 17.578813552856445, 14.092850685119629, 15.818586349487305, 17.578813552856445, 15.818586349487305, 17.578811645507812, 16.04300880432129, 15.818586349487305, 17.578813552856445, 17.578813552856445, 17.578811645507812, 15.818586349487305, 15.818586349487305, 20.05032730102539, 17.578813552856445, 17.578813552856445, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578813552856445, 17.578811645507812, 17.578813552856445, 15.818586349487305, 15.818586349487305, 15.818586349487305, 14.01516056060791, 14.092850685119629, 15.818586349487305, 14.092851638793945, 17.578813552856445, 15.818586349487305, 14.015161514282227, 17.578813552856445, 15.818586349487305, 20.050315856933594, 14.092851638793945, 15.818586349487305, 15.818586349487305, 14.092851638793945, 14.366205215454102, 14.092850685119629, 14.01516056060791, 17.578811645507812, 17.578813552856445, 17.578813552856445, 13.966045379638672, 17.578813552856445, 14.468147277832031, 14.092850685119629, 17.578811645507812, 14.01516056060791, 14.468147277832031, 15.818586349487305, 14.015161514282227, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 16.043010711669922, 14.015159606933594, 15.818586349487305, 17.578813552856445, 14.015161514282227, 14.468147277832031, 17.578813552856445, 15.818586349487305, 17.578813552856445, 17.578813552856445, 17.578813552856445, 17.578813552856445, 14.01516056060791, 13.966050148010254, 14.092851638793945, 17.578813552856445, 13.160521507263184, 15.818586349487305, 14.01516056060791, 17.578813552856445, 14.366205215454102, 17.578813552856445, 15.511848449707031, 14.092850685119629, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578813552856445, 14.01516056060791, 14.01516056060791, 15.818586349487305, 17.578813552856445, 17.578813552856445, 17.578813552856445, 15.818586349487305, 17.578813552856445, 15.818586349487305, 14.468147277832031, 14.015161514282227, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 14.468147277832031, 15.818586349487305, 14.468147277832031, 14.468147277832031, 17.578813552856445, 14.01516056060791, 14.468147277832031, 15.818586349487305, 13.966045379638672, 13.480603218078613, 13.966047286987305, 14.092851638793945, 14.468147277832031, 14.015159606933594, 14.01516056060791, 14.015161514282227, 15.818586349487305, 17.578813552856445, 15.818586349487305, 15.818586349487305, 14.366205215454102, 15.818586349487305, 17.578813552856445, 14.015161514282227, 15.818586349487305, 16.042991638183594, 17.578813552856445, 17.578811645507812, 14.015159606933594, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578813552856445, 15.818586349487305, 14.01516056060791, 15.818586349487305, 14.01516056060791, 15.818586349487305, 15.818586349487305, 13.160482406616211, 15.818586349487305, 14.344783782958984, 15.818586349487305, 14.01516056060791, 15.818586349487305, 20.050344467163086, 14.468146324157715, 17.578813552856445, 17.578813552856445, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 16.04300308227539, 15.818586349487305, 17.578813552856445, 15.818586349487305, 14.015161514282227, 20.050350189208984, 14.092849731445312, 15.818586349487305, 14.092851638793945, 17.578813552856445, 14.015161514282227, 15.818586349487305, 17.578813552856445, 17.578813552856445, 15.818586349487305, 16.043001174926758, 17.578813552856445, 15.818586349487305, 15.818586349487305, 14.01516056060791, 15.818586349487305, 14.092850685119629, 14.468147277832031, 14.01516056060791, 14.092850685119629, 15.818586349487305, 15.818586349487305, 13.160476684570312, 17.578811645507812, 17.578813552856445, 17.578813552856445, 15.818586349487305, 15.818586349487305, 14.366207122802734, 14.468147277832031, 14.468146324157715, 15.818586349487305, 15.818586349487305, 15.818586349487305, 17.578813552856445, 16.043006896972656, 16.04300880432129, 15.818586349487305, 13.1605806350708, 17.578811645507812, 17.578811645507812, 17.578813552856445, 15.511911392211914, 14.468147277832031, 17.578813552856445, 15.818586349487305, 15.818586349487305, 15.818586349487305, 16.04300880432129, 14.366205215454102, 11.336155891418457, 14.015161514282227, 15.818586349487305, 17.578813552856445, 14.015161514282227, 17.578813552856445, 15.818586349487305, 17.578813552856445, 14.468147277832031, 17.578813552856445, 17.578811645507812, 14.015161514282227, 15.818586349487305, 17.578811645507812, 15.818586349487305, 13.429972648620605, 16.043010711669922, 17.578813552856445, 14.468146324157715, 17.578813552856445, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578813552856445, 17.578813552856445, 14.468147277832031, 15.818586349487305, 15.818586349487305, 15.818586349487305, 15.818586349487305, 14.092852592468262, 17.578813552856445, 14.015161514282227, 14.01516056060791, 17.578813552856445, 14.01516056060791, 15.818586349487305, 14.092851638793945, 15.818586349487305, 15.818586349487305, 14.015161514282227, 15.818586349487305, 14.092851638793945, 15.818586349487305, 17.578813552856445, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578813552856445, 13.966049194335938, 15.818586349487305, 15.818586349487305, 11.308253288269043, 14.092850685119629, 15.511890411376953, 17.578813552856445, 14.01516056060791, 14.015161514282227, 13.160524368286133, 15.818586349487305, 17.578813552856445, 17.578813552856445, 15.818586349487305, 15.818586349487305, 15.818586349487305, 14.092851638793945, 17.578813552856445, 17.578813552856445, 15.818586349487305, 13.160482406616211, 15.818586349487305, 17.578813552856445, 15.818586349487305, 17.578813552856445, 17.578813552856445, 17.578813552856445, 15.818586349487305, 15.818586349487305, 17.578813552856445]\n",
            "Mean Squared Error: 99.0370\n",
            "Mean Absolute Error: 6.4811\n",
            "Root Mean Squared Error (RMSE): 9.9517\n",
            "Mean Absolute Percentage Error (MAPE): 0.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.3 Model 4\n",
        "In this model, we explore if more layers with double the neuron in the first layer will improve the model."
      ],
      "metadata": {
        "id": "246_hGag6xM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the GNN model\n",
        "class GNN4(torch.nn.Module):\n",
        "    def __init__(self,input_dim):\n",
        "        super(GNN4, self).__init__()\n",
        "        ###convolutional layer 1 to extract richer features from the data\n",
        "        self.conv1 = GCNConv(in_channels=input_dim, out_channels=64)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(64)\n",
        "\n",
        "       ### convolutional layer 2 further refining these features into a more compact form.\n",
        "        self.conv2 = GCNConv(in_channels=64, out_channels=32)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(32)\n",
        "\n",
        "        self.conv3 = GCNConv(in_channels=32, out_channels=16)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(16)\n",
        "\n",
        "        self.conv4 = GCNConv(in_channels=16,out_channels=8)\n",
        "        self.bn4 = torch.nn.BatchNorm1d(8)\n",
        "\n",
        "        self.fc = torch.nn.Linear(8, 1)\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv4(x, edge_index)\n",
        "        x = self.bn4(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        embeddings = x\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "26qaqIr1N7-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model_Iteration4 = GNN4(5)\n",
        "criterion = torch.nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model_Iteration4.parameters(), lr=0.005)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 30  # Number of epochs to wait for improvement\n",
        "best_val_loss = float('inf')  # Initialize with a large value\n",
        "epochs_no_improve = 0  # Count of epochs since the last improvement\n",
        "\n",
        "\n",
        "for epoch in range(500):\n",
        "  model_Iteration4.train()\n",
        "  train_loss = 0\n",
        "  for batch in train_loader2:\n",
        "      optimizer.zero_grad()\n",
        "      out = model_Iteration4(batch)\n",
        "      loss = criterion(out, batch.y.view(-1, 1).float())\n",
        "      if torch.isnan(loss).sum() > 0:\n",
        "          print(\"NaN loss encountered at epoch:\", epoch)\n",
        "          break\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model_Iteration4.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item() * batch.num_graphs\n",
        "  train_loss /= len(train_loader2.dataset)\n",
        "\n",
        "\n",
        "  model_Iteration4.eval()  # Set the model to evaluation mode\n",
        "  val_loss = 0  # Initialize validation loss\n",
        "\n",
        "  with torch.no_grad():  # No gradient computation\n",
        "      for batch in val_loader2:\n",
        "          out = model_Iteration4(batch)  # Forward pass\n",
        "          loss = criterion(out, batch.y.view(-1, 1).float())  # Compute loss\n",
        "          val_loss += loss.item() * batch.num_graphs  # Accumulate validation loss\n",
        "\n",
        "  val_loss /= len(val_loader2.dataset)  # Average validation loss\n",
        "  print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f},Loss: {train_loss:.4f},best_val_loss:{best_val_loss:.4f}')\n",
        "\n",
        "  # Early stopping check\n",
        "  if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      epochs_no_improve = 0\n",
        "      # Save the best model\n",
        "      torch.save(model_Iteration4.state_dict(), 'best_model2.pt')\n",
        "  else:\n",
        "      epochs_no_improve += 1\n",
        "\n",
        "    # Check if early stopping criterion is met\n",
        "  if epochs_no_improve >= patience:\n",
        "      print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "      break\n",
        "\n",
        "    # Set the model back to training mode for the next epoch\n",
        "  model_Iteration4.train()"
      ],
      "metadata": {
        "id": "5zUhmFGSN8nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3.3 Model 4 Evaluation\n",
        "----------------------------------\n",
        "The results from Model 4 is about the same as Model 3 as from the results, the metrics do not improve drastically with more neurons or having more layers. It however performed better than the baseline model in the Mean Absolute Error which is the main metrics we want to be better than the baselne model."
      ],
      "metadata": {
        "id": "958mJ_5R7EwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "model_Iteration4.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader2:\n",
        "        out = model_Iteration4(batch)\n",
        "        predictions.extend(out.squeeze().cpu().numpy().tolist())\n",
        "\n",
        "print(f'Predicted valuations: {predictions}')\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "\n",
        "# Convert predictions and targets to numpy arrays for evaluation\n",
        "actuals = test_data.y.numpy()\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(actuals, predictions)\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "rmse = mean_squared_error(actuals, predictions, squared=False)\n",
        "mape = mean_absolute_percentage_error(actuals, predictions)\n",
        "\n",
        "\n",
        "print(f'Mean Squared Error: {mse:.4f}')\n",
        "print(f'Mean Absolute Error: {mae:.4f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {mape:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqK_nIMVN88v",
        "outputId": "1628ee9b-8800-48b0-d8aa-cfe03d24a28e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted valuations: [16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 13.869486808776855, 16.878067016601562, 14.79757022857666, 11.777496337890625, 14.107336044311523, 13.869486808776855, 14.79757022857666, 13.869486808776855, 13.869486808776855, 16.878067016601562, 14.316536903381348, 14.316536903381348, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 13.517804145812988, 14.79757022857666, 11.777495384216309, 16.878067016601562, 16.878067016601562, 15.13044548034668, 16.878067016601562, 11.777496337890625, 16.878067016601562, 16.878067016601562, 16.878067016601562, 19.627227783203125, 16.878067016601562, 14.79757022857666, 13.869486808776855, 11.777496337890625, 14.79757022857666, 14.79757022857666, 14.107336044311523, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.316536903381348, 14.107336044311523, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 16.878067016601562, 13.517804145812988, 14.107336044311523, 14.107336044311523, 14.107336044311523, 13.517804145812988, 15.167634010314941, 17.22077178955078, 14.107336044311523, 11.777497291564941, 16.878067016601562, 19.627227783203125, 16.878067016601562, 14.79757022857666, 11.777496337890625, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.316536903381348, 16.878067016601562, 16.878067016601562, 16.878067016601562, 13.869486808776855, 16.878067016601562, 14.79757022857666, 16.878067016601562, 13.869486808776855, 11.777496337890625, 16.878067016601562, 15.167634963989258, 16.878067016601562, 17.22077178955078, 16.878067016601562, 16.630882263183594, 16.878067016601562, 14.79757022857666, 15.167634963989258, 13.869486808776855, 16.878067016601562, 11.777495384216309, 16.878067016601562, 14.107336044311523, 16.878067016601562, 14.107336044311523, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 13.517804145812988, 16.878067016601562, 14.79757022857666, 15.130446434020996, 15.167634010314941, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.107336044311523, 14.79757022857666, 14.79757022857666, 16.878067016601562, 13.869486808776855, 16.878067016601562, 17.22077178955078, 16.878067016601562, 16.878067016601562, 17.22077178955078, 16.84198760986328, 16.878067016601562, 14.79757022857666, 15.130446434020996, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 13.869486808776855, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.107336044311523, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 17.22077178955078, 14.107336044311523, 14.107336044311523, 16.878067016601562, 11.777497291564941, 17.22077178955078, 14.107336044311523, 16.878067016601562, 14.79757022857666, 16.878067016601562, 15.130446434020996, 16.878067016601562, 17.22077178955078, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.107336044311523, 14.79757022857666, 15.167634963989258, 15.167634963989258, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.107336044311523, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 13.517804145812988, 14.107336044311523, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.107336044311523, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 13.869486808776855, 14.79757022857666, 15.167634010314941, 16.878067016601562, 17.22077178955078, 19.627227783203125, 14.107336044311523, 16.878067016601562, 15.13044548034668, 11.777497291564941, 14.79757022857666, 14.79757022857666, 14.79757022857666, 11.777496337890625, 14.107336044311523, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.107336044311523, 16.878067016601562, 16.878067016601562, 14.79757022857666, 17.22077178955078, 14.79757022857666, 11.777497291564941, 14.751287460327148, 14.79757022857666, 13.869486808776855, 14.79757022857666, 15.167634963989258, 14.79757022857666, 14.107336044311523, 14.107336044311523, 11.777495384216309, 14.79757022857666, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.107336044311523, 13.517805099487305, 14.79757022857666, 14.79757022857666, 14.316536903381348, 16.878067016601562, 17.49908447265625, 15.167634963989258, 16.878067016601562, 14.79757022857666, 16.878067016601562, 17.22077178955078, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 16.878067016601562, 17.22077178955078, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.107336044311523, 14.79757022857666, 11.777496337890625, 14.316536903381348, 14.79757022857666, 17.22077178955078, 16.878067016601562, 16.878067016601562, 13.517804145812988, 16.878067016601562, 14.79757022857666, 14.316536903381348, 14.79757022857666, 19.627227783203125, 13.517804145812988, 16.878067016601562, 13.869486808776855, 14.79757022857666, 17.22077178955078, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.107336044311523, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.107336044311523, 14.79757022857666, 14.79757022857666, 11.777496337890625, 15.167634963989258, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.107336044311523, 15.167635917663574, 14.79757022857666, 14.107336044311523, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.107336044311523, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 11.777496337890625, 13.517804145812988, 14.79757022857666, 17.22077178955078, 14.79757022857666, 14.79757022857666, 11.777496337890625, 14.79757022857666, 16.878067016601562, 13.517804145812988, 14.79757022857666, 11.777496337890625, 13.869486808776855, 14.79757022857666, 17.22077178955078, 14.79757022857666, 13.869486808776855, 11.777496337890625, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.316535949707031, 17.22077178955078, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.107336044311523, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 11.777496337890625, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.316535949707031, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 17.22077178955078, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.107336044311523, 15.167634963989258, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 10.177614212036133, 14.107336044311523, 14.107336044311523, 16.878067016601562, 14.79757022857666, 14.79757022857666, 11.777497291564941, 14.79757022857666, 14.107336044311523, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 15.167634963989258, 14.79757022857666, 13.869486808776855, 14.79757022857666, 16.878067016601562, 11.777497291564941, 16.878067016601562, 14.79757022857666, 14.316536903381348, 16.878067016601562, 11.777496337890625, 15.130446434020996, 14.79757022857666, 15.167634963989258, 15.167634963989258, 14.79757022857666, 14.79757022857666, 14.107336044311523, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.709693908691406, 14.79757022857666, 16.878067016601562, 15.167634010314941, 14.79757022857666, 16.878067016601562, 11.777495384216309, 16.878067016601562, 14.79757022857666, 14.107336044311523, 14.79757022857666, 14.107336044311523, 16.878067016601562, 14.79757022857666, 14.107336044311523, 16.878067016601562, 11.777496337890625, 14.107336044311523, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 13.869486808776855, 16.878067016601562, 13.517805099487305, 16.878067016601562, 19.627227783203125, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 15.167634963989258, 17.22077178955078, 16.878067016601562, 14.547138214111328, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 13.517804145812988, 14.79757022857666, 11.777496337890625, 16.878067016601562, 16.878067016601562, 16.878067016601562, 13.869486808776855, 13.869486808776855, 11.777496337890625, 19.62723159790039, 15.13044548034668, 14.79757022857666, 15.167634963989258, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 19.62723159790039, 19.627227783203125, 14.79757022857666, 10.17460823059082, 13.517805099487305, 13.869486808776855, 14.79757022857666, 16.878067016601562, 14.79757022857666, 17.22077178955078, 15.167635917663574, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.107336044311523, 14.79757022857666, 16.878067016601562, 15.167634010314941, 16.878067016601562, 13.517804145812988, 16.878067016601562, 15.167634963989258, 14.79757022857666, 14.79757022857666, 13.517804145812988, 17.22077178955078, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.107336044311523, 14.79757022857666, 14.107336044311523, 19.62723159790039, 14.79757022857666, 14.107336044311523, 14.107336044311523, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.107336044311523, 14.79757022857666, 13.517805099487305, 14.79757022857666, 16.878067016601562, 16.878067016601562, 11.777496337890625, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 11.777496337890625, 14.79757022857666, 16.878067016601562, 13.517804145812988, 16.878067016601562, 16.878067016601562, 13.869486808776855, 16.878067016601562, 16.878067016601562, 14.79757022857666, 15.167635917663574, 16.878067016601562, 16.878067016601562, 14.107336044311523, 15.13044548034668, 14.79757022857666, 14.107336044311523, 14.79757022857666, 16.878067016601562, 14.107336044311523, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 11.777496337890625, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 15.906294822692871, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.316536903381348, 17.22077178955078, 15.167634963989258, 14.79757022857666, 16.878067016601562, 14.79757022857666, 16.878067016601562, 11.777496337890625, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.107336044311523, 11.777496337890625, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 16.878067016601562, 15.167634010314941, 13.869486808776855, 14.79757022857666, 11.777496337890625, 14.107336044311523, 14.79757022857666, 16.878067016601562, 14.316535949707031, 16.878067016601562, 11.777496337890625, 14.79757022857666, 13.869486808776855, 14.79757022857666, 14.107336044311523, 14.107336044311523, 14.316535949707031, 14.79757022857666, 14.316536903381348, 16.878067016601562, 16.878067016601562, 14.79757022857666, 13.869486808776855, 14.107336044311523, 16.878067016601562, 17.22077178955078, 14.107336044311523, 16.878067016601562, 16.878067016601562, 11.777497291564941, 16.878067016601562, 14.79757022857666, 13.869486808776855, 16.878067016601562, 16.878067016601562, 11.777496337890625, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.107336044311523, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.107336044311523, 16.878067016601562, 14.316536903381348, 16.878067016601562, 14.79757022857666, 15.130446434020996, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 11.777496337890625, 14.79757022857666, 14.79757022857666, 19.62723159790039, 14.107336044311523, 16.878067016601562, 14.79757022857666, 16.888286590576172, 16.878067016601562, 14.316536903381348, 15.130444526672363, 14.79757022857666, 13.869486808776855, 16.878067016601562, 17.22077178955078, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.107336044311523, 16.878067016601562, 14.79757022857666, 14.316535949707031, 14.79757022857666, 13.517805099487305, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 17.22077178955078, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 15.167634963989258, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.79757022857666, 17.22077178955078, 13.869486808776855, 11.777496337890625, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.107336044311523, 16.878067016601562, 16.878067016601562, 13.869486808776855, 16.878067016601562, 15.130446434020996, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 13.869486808776855, 16.878067016601562, 14.316536903381348, 14.79757022857666, 13.869486808776855, 14.79757022857666, 16.878067016601562, 14.79757022857666, 16.878067016601562, 13.869486808776855, 14.79757022857666, 14.79757022857666, 19.627227783203125, 11.777496337890625, 14.79757022857666, 16.878067016601562, 15.167635917663574, 14.79757022857666, 15.13044548034668, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 13.869486808776855, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.316535949707031, 16.878067016601562, 14.107336044311523, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.107336044311523, 14.79757022857666, 13.517805099487305, 16.878067016601562, 19.62723159790039, 13.869486808776855, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.107336044311523, 15.167634963989258, 14.79757022857666, 14.107336044311523, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 13.869486808776855, 14.79757022857666, 14.107336044311523, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 11.777496337890625, 16.878067016601562, 14.79757022857666, 16.878067016601562, 12.530335426330566, 15.167634963989258, 13.869486808776855, 14.107336044311523, 14.79757022857666, 11.777496337890625, 14.79757022857666, 14.107336044311523, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 17.85649871826172, 17.464820861816406, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.107336044311523, 14.316536903381348, 16.878067016601562, 15.130446434020996, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.107336044311523, 16.878067016601562, 16.878067016601562, 14.79757022857666, 11.777496337890625, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.107336044311523, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 13.517805099487305, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.107336044311523, 16.878067016601562, 13.869486808776855, 14.107336044311523, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 17.22077178955078, 14.79757022857666, 14.79757022857666, 14.79757022857666, 17.22077178955078, 16.878067016601562, 14.79757022857666, 14.107336044311523, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.107336044311523, 14.107336044311523, 17.22077178955078, 17.22077178955078, 16.878067016601562, 16.878067016601562, 13.517804145812988, 13.517804145812988, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.659042358398438, 14.107336044311523, 16.878067016601562, 16.878067016601562, 14.107336044311523, 14.107336044311523, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 13.517805099487305, 11.777496337890625, 14.107336044311523, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 13.517805099487305, 14.316536903381348, 16.878067016601562, 16.878067016601562, 15.167634963989258, 15.13044548034668, 11.777496337890625, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 13.869486808776855, 16.878067016601562, 14.79757022857666, 13.869486808776855, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 11.777496337890625, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.107336044311523, 15.167634010314941, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.107336044311523, 14.79757022857666, 16.878067016601562, 14.316536903381348, 16.878067016601562, 14.107336044311523, 14.107336044311523, 14.107336044311523, 16.878067016601562, 17.22077178955078, 16.878067016601562, 14.79757022857666, 13.869486808776855, 13.517804145812988, 16.878067016601562, 14.107336044311523, 15.130446434020996, 14.79757022857666, 14.107336044311523, 14.79757022857666, 14.107336044311523, 14.79757022857666, 14.79757022857666, 14.79757022857666, 13.869486808776855, 16.878067016601562, 16.878067016601562, 19.627227783203125, 16.878067016601562, 16.878067016601562, 14.316535949707031, 16.878067016601562, 17.22077178955078, 16.878067016601562, 14.79757022857666, 17.22077178955078, 14.107336044311523, 15.130446434020996, 14.107336044311523, 16.878067016601562, 14.79757022857666, 17.22077178955078, 14.107336044311523, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.107336044311523, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 13.869486808776855, 16.878067016601562, 14.927062034606934, 14.79757022857666, 13.869486808776855, 16.878067016601562, 14.79757022857666, 16.878067016601562, 17.22077178955078, 16.878067016601562, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.107336044311523, 16.878067016601562, 16.878067016601562, 14.107336044311523, 14.79757022857666, 14.79757022857666, 13.517804145812988, 13.869486808776855, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.107336044311523, 14.79757022857666, 14.79757022857666, 13.869486808776855, 16.878067016601562, 11.777496337890625, 14.79757022857666, 16.878067016601562, 16.878067016601562, 13.517804145812988, 14.107336044311523, 14.79757022857666, 14.79757022857666, 14.79757022857666, 13.517804145812988, 15.167634963989258, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.107336044311523, 14.79757022857666, 16.878067016601562, 14.79757022857666, 16.878067016601562, 17.22077178955078, 16.878067016601562, 13.869486808776855, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.107336044311523, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.107336044311523, 14.79757022857666, 14.316535949707031, 16.878067016601562, 16.878067016601562, 13.869486808776855, 14.79757022857666, 13.869486808776855, 14.79757022857666, 14.107336044311523, 15.167634963989258, 11.777496337890625, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.107336044311523, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.107336044311523, 19.62723159790039, 14.79757022857666, 14.79757022857666, 11.777497291564941, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 19.62723159790039, 13.869486808776855, 19.62723159790039, 16.878067016601562, 16.878067016601562, 11.777496337890625, 15.130446434020996, 14.79757022857666, 13.869486808776855, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 17.22077178955078, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 19.62723159790039, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.107336044311523, 13.869486808776855, 16.878067016601562, 13.869486808776855, 14.79757022857666, 16.878067016601562, 14.107336044311523, 14.79757022857666, 16.878067016601562, 19.62723159790039, 13.869486808776855, 16.878067016601562, 16.878067016601562, 13.869486808776855, 13.517804145812988, 13.869486808776855, 14.107336044311523, 14.79757022857666, 14.79757022857666, 14.79757022857666, 15.167634010314941, 14.79757022857666, 11.777496337890625, 13.869486808776855, 14.79757022857666, 14.107336044311523, 11.777496337890625, 16.878067016601562, 14.107336044311523, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 17.22077178955078, 14.107336044311523, 16.878067016601562, 14.79757022857666, 14.107336044311523, 11.777496337890625, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.79757022857666, 14.107336044311523, 15.167635917663574, 13.869486808776855, 14.79757022857666, 14.316536903381348, 16.878067016601562, 14.107336044311523, 14.79757022857666, 13.517804145812988, 14.79757022857666, 15.130446434020996, 13.869486808776855, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.107336044311523, 14.107336044311523, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.79757022857666, 16.878067016601562, 11.777496337890625, 14.107336044311523, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 11.777496337890625, 16.878067016601562, 11.777496337890625, 11.777496337890625, 14.79757022857666, 14.107336044311523, 11.777496337890625, 16.878067016601562, 15.167634963989258, 17.499126434326172, 15.167634963989258, 13.869486808776855, 11.777496337890625, 14.107336044311523, 14.107336044311523, 14.107336044311523, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.878067016601562, 13.517804145812988, 16.878067016601562, 14.79757022857666, 14.107336044311523, 16.878067016601562, 17.22077178955078, 14.79757022857666, 14.79757022857666, 14.107336044311523, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.107336044311523, 16.878067016601562, 14.10733699798584, 16.878067016601562, 16.878067016601562, 14.316536903381348, 16.878067016601562, 15.12688159942627, 16.878067016601562, 14.107336044311523, 16.878067016601562, 19.62723159790039, 11.777497291564941, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 17.22077178955078, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.107336044311523, 19.627227783203125, 13.869486808776855, 16.878067016601562, 13.869486808776855, 14.79757022857666, 14.107336044311523, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 17.22077178955078, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.107336044311523, 16.878067016601562, 13.869486808776855, 11.777496337890625, 14.107336044311523, 13.869486808776855, 16.878067016601562, 16.878067016601562, 14.316536903381348, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 13.517804145812988, 11.777496337890625, 11.777496337890625, 16.878067016601562, 16.878067016601562, 16.878067016601562, 14.79757022857666, 17.22077178955078, 17.22077178955078, 16.878067016601562, 14.316535949707031, 14.79757022857666, 14.79757022857666, 14.79757022857666, 15.130444526672363, 11.777496337890625, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 17.22077178955078, 13.517805099487305, 10.174059867858887, 14.107336044311523, 16.878067016601562, 14.79757022857666, 14.107336044311523, 14.79757022857666, 16.878067016601562, 14.79757022857666, 11.777496337890625, 14.79757022857666, 14.79757022857666, 14.107336044311523, 16.878067016601562, 14.79757022857666, 16.878067016601562, 16.214900970458984, 17.22077178955078, 14.79757022857666, 11.777496337890625, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 14.79757022857666, 11.777496337890625, 16.878067016601562, 16.878067016601562, 16.878067016601562, 16.878067016601562, 13.869486808776855, 14.79757022857666, 14.107336044311523, 14.107336044311523, 14.79757022857666, 14.107336044311523, 16.878067016601562, 13.869486808776855, 16.878067016601562, 16.878067016601562, 14.107336044311523, 16.878067016601562, 13.869486808776855, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666, 15.167635917663574, 16.878067016601562, 16.878067016601562, 10.177226066589355, 13.869486808776855, 15.130444526672363, 14.79757022857666, 14.107336044311523, 14.107336044311523, 14.316536903381348, 16.878067016601562, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 16.878067016601562, 13.869486808776855, 14.79757022857666, 14.79757022857666, 16.878067016601562, 14.316536903381348, 16.878067016601562, 14.79757022857666, 16.878067016601562, 14.79757022857666, 14.79757022857666, 14.79757022857666, 16.878067016601562, 16.878067016601562, 14.79757022857666]\n",
            "Mean Squared Error: 99.0291\n",
            "Mean Absolute Error: 6.3832\n",
            "Root Mean Squared Error (RMSE): 9.9513\n",
            "Mean Absolute Percentage Error (MAPE): 0.39%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the GNN model\n",
        "class GNN6(torch.nn.Module):\n",
        "    def __init__(self,input_dim):\n",
        "        super(GNN6, self).__init__()\n",
        "        ###convolutional layer 1 to extract richer features from the data\n",
        "        self.conv1 = GCNConv(in_channels=input_dim, out_channels=32)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(32)\n",
        "\n",
        "        ##adding a hidden layer\n",
        "        self.fc_hidden = torch.nn.Linear(32, 16)\n",
        "        self.bn_hidden = torch.nn.BatchNorm1d(16)\n",
        "\n",
        "       ### convolutional layer 2 further refining these features into a more compact form.\n",
        "        self.conv2 = GCNConv(in_channels=16, out_channels=8)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(8)\n",
        "\n",
        "        self.conv3 = GCNConv(in_channels=8, out_channels=4)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(4)\n",
        "\n",
        "        self.fc = torch.nn.Linear(4, 1)\n",
        "        self.dropout = torch.nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc_hidden(x)\n",
        "        x = self.bn_hidden(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv3(x,edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        embeddings = x\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "qzbGhr1HDR5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3.4 Final Model Test\n",
        "-----------------------------------\n",
        "This model consists of :    \n",
        "- 3 convolutional layer (minus one from the previous)\n",
        "- two linear layers ( one for output and one inbetween)\n",
        "- 4 normalization processes\n",
        "- 4 Relu activation function\n",
        "\n",
        "We shall see if this model works better than the baseline model with the full training without early stopping to check if it is better.\n"
      ],
      "metadata": {
        "id": "vpRG_BRD8ae9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model_Iteration6 = GNN6(5)\n",
        "criterion = torch.nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model_Iteration6.parameters(), lr=0.005)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 500  # Number of epochs to wait for improvement\n",
        "best_val_loss = float('inf')  # Initialize with a large value\n",
        "epochs_no_improve = 0  # Count of epochs since the last improvement\n",
        "\n",
        "\n",
        "for epoch in range(500):\n",
        "  model_Iteration6.train()\n",
        "  train_loss = 0\n",
        "  for batch in train_loader2:\n",
        "      optimizer.zero_grad()\n",
        "      out = model_Iteration6(batch)\n",
        "      loss = criterion(out, batch.y.view(-1, 1).float())\n",
        "      if torch.isnan(loss).sum() > 0:\n",
        "          print(\"NaN loss encountered at epoch:\", epoch)\n",
        "          break\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model_Iteration6.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item() * batch.num_graphs\n",
        "  train_loss /= len(train_loader2.dataset)\n",
        "\n",
        "\n",
        "  model_Iteration6.eval()  # Set the model to evaluation mode\n",
        "  val_loss = 0  # Initialize validation loss\n",
        "\n",
        "  with torch.no_grad():  # No gradient computation\n",
        "      for batch in val_loader2:\n",
        "          out = model_Iteration6(batch)  # Forward pass\n",
        "          loss = criterion(out, batch.y.view(-1, 1).float())  # Compute loss\n",
        "          val_loss += loss.item() * batch.num_graphs  # Accumulate validation loss\n",
        "\n",
        "  val_loss /= len(val_loader2.dataset)  # Average validation loss\n",
        "  print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f},Loss: {train_loss:.4f},best_val_loss:{best_val_loss:.4f}')\n",
        "\n",
        "  # Early stopping check\n",
        "  if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      epochs_no_improve = 0\n",
        "      # Save the best model\n",
        "      torch.save(model_Iteration6.state_dict(), 'best_model2.pt')\n",
        "  else:\n",
        "      epochs_no_improve += 1\n",
        "\n",
        "    # Check if early stopping criterion is met\n",
        "  if epochs_no_improve >= patience:\n",
        "      print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "      break\n",
        "\n",
        "    # Set the model back to training mode for the next epoch\n",
        "  model_Iteration6.train()"
      ],
      "metadata": {
        "id": "X_5VuPiRGQKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3.4 Final Model Evaluation\n",
        "---------------------------------------\n",
        "The Mean Squared Error is worst than the baseline model which is 84.45 as compared to the model's result of 95.58\n",
        "\n",
        "The Mean Absolute error is slightly better than the Mean Absolute Error of the baseline model as seen from 6.40 to 6.38 of the final model's Mean Absolute Error\n",
        "\n",
        "The Root Mean Squared Error of the model's 9.7765 is worse than the Baseline model's 9.19.\n",
        "\n",
        "The Mean Absolute Percentage Error is better than the Baseline's model as seen from the model's Mean Absolute Percentage error of 0.40 to the Baseline Model's Mean Absolute Percentage Error of 0.45\n",
        "\n",
        "Therefore, the model might still be making a large error due to outliers in the dataset but is still more accurate in predicting the IPO_Prices. With this result, we begin to test for the best model's parameters."
      ],
      "metadata": {
        "id": "oEfo3qm-9JF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "model_Iteration6.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader2:\n",
        "        out = model_Iteration6(batch)\n",
        "        predictions.extend(out.squeeze().cpu().numpy().tolist())\n",
        "\n",
        "print(f'Predicted valuations: {predictions}')\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "\n",
        "# Convert predictions and targets to numpy arrays for evaluation\n",
        "actuals = test_data.y.numpy()\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(actuals, predictions)\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "rmse = mean_squared_error(actuals, predictions, squared=False)\n",
        "mape = mean_absolute_percentage_error(actuals, predictions)\n",
        "\n",
        "\n",
        "print(f'Mean Squared Error: {mse:.4f}')\n",
        "print(f'Mean Absolute Error: {mae:.4f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {mape:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iocgQBeHQCk",
        "outputId": "53517e46-6687-4112-921b-d0f8ce0b281d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted valuations: [18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 13.359572410583496, 18.463668823242188, 16.019189834594727, 13.097433090209961, 13.497237205505371, 13.35957145690918, 16.019189834594727, 13.359572410583496, 13.35957145690918, 18.463668823242188, 13.444790840148926, 13.444786071777344, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 13.173151969909668, 16.019189834594727, 13.097432136535645, 18.463668823242188, 18.463668823242188, 16.47368812561035, 18.463668823242188, 13.097433090209961, 18.463668823242188, 18.463668823242188, 18.463668823242188, 17.71795654296875, 18.463668823242188, 16.019189834594727, 13.359572410583496, 13.097433090209961, 16.019189834594727, 16.019189834594727, 13.497237205505371, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 13.444742202758789, 13.497236251831055, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 18.463668823242188, 13.173155784606934, 13.497236251831055, 13.497237205505371, 13.497236251831055, 13.173155784606934, 13.121260643005371, 17.22940444946289, 13.497236251831055, 13.097431182861328, 18.463668823242188, 17.717954635620117, 18.463668823242188, 16.019189834594727, 13.097432136535645, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 13.444791793823242, 18.463668823242188, 18.463668823242188, 18.463668823242188, 13.359572410583496, 18.463668823242188, 16.019189834594727, 18.463668823242188, 13.359573364257812, 13.097431182861328, 18.463668823242188, 13.121259689331055, 18.463668823242188, 17.22940444946289, 18.463668823242188, 10.999009132385254, 18.463668823242188, 16.019189834594727, 13.121259689331055, 13.359573364257812, 18.463668823242188, 13.097433090209961, 18.463668823242188, 13.497236251831055, 18.463668823242188, 13.497237205505371, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 13.173154830932617, 18.463668823242188, 16.019189834594727, 16.47368812561035, 13.121259689331055, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 13.497237205505371, 16.019189834594727, 16.019189834594727, 18.463668823242188, 13.359572410583496, 18.463668823242188, 17.22940444946289, 18.463668823242188, 18.463668823242188, 17.229400634765625, 12.666779518127441, 18.463668823242188, 16.019189834594727, 16.47368049621582, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.35957145690918, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 13.497236251831055, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 17.229402542114258, 13.497236251831055, 13.497237205505371, 18.463668823242188, 13.097432136535645, 17.229402542114258, 13.497237205505371, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.473684310913086, 18.463668823242188, 17.229398727416992, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 13.497238159179688, 16.019189834594727, 13.121259689331055, 13.121260643005371, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.497237205505371, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 13.173155784606934, 13.497237205505371, 18.463668823242188, 16.019189834594727, 16.019189834594727, 13.497236251831055, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 13.359573364257812, 16.019189834594727, 13.121259689331055, 18.463668823242188, 17.229402542114258, 17.717954635620117, 13.497236251831055, 18.463668823242188, 16.47368621826172, 13.097432136535645, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.097431182861328, 13.497237205505371, 16.019189834594727, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.497236251831055, 18.463668823242188, 18.463668823242188, 16.019189834594727, 17.22940444946289, 16.019189834594727, 13.097431182861328, 10.744893074035645, 16.019189834594727, 13.35957145690918, 16.019189834594727, 13.121259689331055, 16.019189834594727, 13.497236251831055, 13.497236251831055, 13.097433090209961, 16.019189834594727, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.497236251831055, 13.173151016235352, 16.019189834594727, 16.019189834594727, 13.444791793823242, 18.463668823242188, 17.051111221313477, 13.121259689331055, 18.463668823242188, 16.019189834594727, 18.463668823242188, 17.22939682006836, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 18.463668823242188, 17.22940444946289, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 13.497236251831055, 16.019189834594727, 13.097433090209961, 13.444742202758789, 16.019189834594727, 17.229402542114258, 18.463668823242188, 18.463668823242188, 13.173154830932617, 18.463668823242188, 16.019189834594727, 13.444744110107422, 16.019189834594727, 17.71795654296875, 13.1731538772583, 18.463668823242188, 13.35957145690918, 16.019189834594727, 17.22940444946289, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 13.497236251831055, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 13.497236251831055, 16.019189834594727, 16.019189834594727, 13.097433090209961, 13.121259689331055, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 13.497237205505371, 13.121258735656738, 16.019189834594727, 13.497237205505371, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.497236251831055, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 13.097431182861328, 13.173155784606934, 16.019189834594727, 17.22940444946289, 16.019189834594727, 16.019189834594727, 13.097432136535645, 16.019189834594727, 18.463668823242188, 13.1731538772583, 16.019189834594727, 13.097432136535645, 13.359573364257812, 16.019189834594727, 17.229402542114258, 16.019189834594727, 13.35957145690918, 13.097433090209961, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 13.44481086730957, 17.229402542114258, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.497236251831055, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.097431182861328, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 13.444808006286621, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 17.229402542114258, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 13.497236251831055, 13.121259689331055, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 12.237833976745605, 13.497237205505371, 13.497236251831055, 18.463668823242188, 16.019189834594727, 16.019189834594727, 13.097431182861328, 16.019189834594727, 13.497237205505371, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 13.121259689331055, 16.019189834594727, 13.359573364257812, 16.019189834594727, 18.463668823242188, 13.097431182861328, 18.463668823242188, 16.019189834594727, 13.444742202758789, 18.463668823242188, 13.097433090209961, 16.47368049621582, 16.019189834594727, 13.121259689331055, 13.121259689331055, 16.019189834594727, 16.019189834594727, 13.497236251831055, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 17.333942413330078, 16.019189834594727, 18.463668823242188, 13.121259689331055, 16.019189834594727, 18.463668823242188, 13.097433090209961, 18.463668823242188, 16.019189834594727, 13.497236251831055, 16.019189834594727, 13.497236251831055, 18.463668823242188, 16.019189834594727, 13.497236251831055, 18.463668823242188, 13.097432136535645, 13.497236251831055, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 13.359572410583496, 18.463668823242188, 13.173151016235352, 18.463668823242188, 17.71795654296875, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 13.121259689331055, 17.229398727416992, 18.463668823242188, 12.299551010131836, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.173151016235352, 16.019189834594727, 13.097431182861328, 18.463668823242188, 18.463668823242188, 18.463668823242188, 13.359572410583496, 13.35957145690918, 13.097433090209961, 17.717952728271484, 16.47368621826172, 16.019189834594727, 13.121260643005371, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 17.71795654296875, 17.71796417236328, 16.019189834594727, 12.243856430053711, 13.173151016235352, 13.35957145690918, 16.019189834594727, 18.463668823242188, 16.019189834594727, 17.229402542114258, 13.121259689331055, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 13.497236251831055, 16.019189834594727, 18.463668823242188, 13.121259689331055, 18.463668823242188, 13.173151016235352, 18.463668823242188, 13.121259689331055, 16.019189834594727, 16.019189834594727, 13.173151016235352, 17.22940444946289, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.497236251831055, 16.019189834594727, 13.497236251831055, 17.71795654296875, 16.019189834594727, 13.497237205505371, 13.497236251831055, 16.019189834594727, 18.463668823242188, 16.019189834594727, 13.497237205505371, 16.019189834594727, 13.173152923583984, 16.019189834594727, 18.463668823242188, 18.463668823242188, 13.097433090209961, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 13.097432136535645, 16.019189834594727, 18.463668823242188, 13.173152923583984, 18.463668823242188, 18.463668823242188, 13.359572410583496, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.121261596679688, 18.463668823242188, 18.463668823242188, 13.497237205505371, 16.473684310913086, 16.019189834594727, 13.497236251831055, 16.019189834594727, 18.463668823242188, 13.497236251831055, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.097433090209961, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.066267013549805, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.444766998291016, 17.229398727416992, 13.121259689331055, 16.019189834594727, 18.463668823242188, 16.019189834594727, 18.463668823242188, 13.097432136535645, 16.019189834594727, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.497236251831055, 13.097433090209961, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 18.463668823242188, 13.121259689331055, 13.359572410583496, 16.019189834594727, 13.097433090209961, 13.497237205505371, 16.019189834594727, 18.463668823242188, 13.444793701171875, 18.463668823242188, 13.097432136535645, 16.019189834594727, 13.359572410583496, 16.019189834594727, 13.497236251831055, 13.497237205505371, 13.444793701171875, 16.019189834594727, 13.444741249084473, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.359573364257812, 13.497236251831055, 18.463668823242188, 17.22940444946289, 13.497236251831055, 18.463668823242188, 18.463668823242188, 13.097433090209961, 18.463668823242188, 16.019189834594727, 13.35957145690918, 18.463668823242188, 18.463668823242188, 13.097432136535645, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.497237205505371, 16.019189834594727, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.497236251831055, 18.463668823242188, 13.44478988647461, 18.463668823242188, 16.019189834594727, 16.47368049621582, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.097433090209961, 16.019189834594727, 16.019189834594727, 17.71795654296875, 13.497236251831055, 18.463668823242188, 16.019189834594727, 17.354772567749023, 18.463668823242188, 13.4447660446167, 16.47368621826172, 16.019189834594727, 13.35957145690918, 18.463668823242188, 17.22940444946289, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.497237205505371, 18.463668823242188, 16.019189834594727, 13.44481086730957, 16.019189834594727, 13.173151016235352, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 17.22940444946289, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.121259689331055, 16.019189834594727, 16.019189834594727, 16.019189834594727, 16.019189834594727, 17.22940444946289, 13.359572410583496, 13.097431182861328, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.497236251831055, 18.463668823242188, 18.463668823242188, 13.35957145690918, 18.463668823242188, 16.473684310913086, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 13.359573364257812, 18.463668823242188, 13.444746971130371, 16.019189834594727, 13.359572410583496, 16.019189834594727, 18.463668823242188, 16.019189834594727, 18.463668823242188, 13.359573364257812, 16.019189834594727, 16.019189834594727, 17.717954635620117, 13.097433090209961, 16.019189834594727, 18.463668823242188, 13.121259689331055, 16.019189834594727, 16.47368621826172, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 13.35957145690918, 16.019189834594727, 18.463668823242188, 16.019189834594727, 13.444790840148926, 18.463668823242188, 13.497236251831055, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 13.497236251831055, 16.019189834594727, 13.173151969909668, 18.463668823242188, 17.717954635620117, 13.359572410583496, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 13.497236251831055, 13.121259689331055, 16.019189834594727, 13.497236251831055, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.35957145690918, 16.019189834594727, 13.497236251831055, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 13.097433090209961, 18.463668823242188, 16.019189834594727, 18.463668823242188, 13.207681655883789, 13.121259689331055, 13.35957145690918, 13.497237205505371, 16.019189834594727, 13.097433090209961, 16.019189834594727, 13.497236251831055, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 14.548019409179688, 17.3856201171875, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 13.497236251831055, 13.444818496704102, 18.463668823242188, 16.473682403564453, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 13.497237205505371, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.097433090209961, 18.463668823242188, 16.019189834594727, 16.019189834594727, 13.497236251831055, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 13.173152923583984, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 18.463668823242188, 13.497236251831055, 18.463668823242188, 13.35957145690918, 13.497237205505371, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 17.22940444946289, 16.019189834594727, 16.019189834594727, 16.019189834594727, 17.22940444946289, 18.463668823242188, 16.019189834594727, 13.497237205505371, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.497237205505371, 13.497236251831055, 17.22940444946289, 17.22940444946289, 18.463668823242188, 18.463668823242188, 13.1731538772583, 13.1731538772583, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.276119232177734, 13.497236251831055, 18.463668823242188, 18.463668823242188, 13.497237205505371, 13.497236251831055, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 13.173152923583984, 13.097432136535645, 13.497236251831055, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 13.173152923583984, 13.444747924804688, 18.463668823242188, 18.463668823242188, 13.121261596679688, 16.473684310913086, 13.097433090209961, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.359572410583496, 18.463668823242188, 16.019189834594727, 13.35957145690918, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 13.097433090209961, 16.019189834594727, 16.019189834594727, 18.463668823242188, 13.497236251831055, 13.121259689331055, 18.463668823242188, 16.019189834594727, 16.019189834594727, 13.497236251831055, 16.019189834594727, 18.463668823242188, 13.444744110107422, 18.463668823242188, 13.497236251831055, 13.497236251831055, 13.497237205505371, 18.463668823242188, 17.22940444946289, 18.463668823242188, 16.019189834594727, 13.35957145690918, 13.173155784606934, 18.463668823242188, 13.497237205505371, 16.473682403564453, 16.019189834594727, 13.497236251831055, 16.019189834594727, 13.497237205505371, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.359572410583496, 18.463668823242188, 18.463668823242188, 17.717952728271484, 18.463668823242188, 18.463668823242188, 13.444791793823242, 18.463668823242188, 17.22940444946289, 18.463668823242188, 16.019189834594727, 17.22940444946289, 13.497236251831055, 16.473682403564453, 13.497236251831055, 18.463668823242188, 16.019189834594727, 17.229402542114258, 13.497236251831055, 18.463668823242188, 16.019189834594727, 16.019189834594727, 13.497237205505371, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 13.35957145690918, 18.463668823242188, 13.57361125946045, 16.019189834594727, 13.35957145690918, 18.463668823242188, 16.019189834594727, 18.463668823242188, 17.229398727416992, 18.463668823242188, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 13.497236251831055, 18.463668823242188, 18.463668823242188, 13.497236251831055, 16.019189834594727, 16.019189834594727, 13.17315673828125, 13.359573364257812, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.497236251831055, 16.019189834594727, 16.019189834594727, 13.35957145690918, 18.463668823242188, 13.097432136535645, 16.019189834594727, 18.463668823242188, 18.463668823242188, 13.17315673828125, 13.497237205505371, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.1731538772583, 13.121258735656738, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.497236251831055, 16.019189834594727, 18.463668823242188, 16.019189834594727, 18.463668823242188, 17.22940444946289, 18.463668823242188, 13.359573364257812, 18.463668823242188, 16.019189834594727, 18.463668823242188, 13.497236251831055, 18.463668823242188, 16.019189834594727, 16.019189834594727, 13.497236251831055, 16.019189834594727, 13.444791793823242, 18.463668823242188, 18.463668823242188, 13.35957145690918, 16.019189834594727, 13.359573364257812, 16.019189834594727, 13.497236251831055, 13.121259689331055, 13.097433090209961, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 13.497236251831055, 16.019189834594727, 16.019189834594727, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.497236251831055, 17.717954635620117, 16.019189834594727, 16.019189834594727, 13.097433090209961, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 17.71795654296875, 13.359573364257812, 17.71795654296875, 18.463668823242188, 18.463668823242188, 13.097432136535645, 16.47368049621582, 16.019189834594727, 13.35957145690918, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 17.22940444946289, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 17.71795654296875, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 13.497236251831055, 13.359572410583496, 18.463668823242188, 13.35957145690918, 16.019189834594727, 18.463668823242188, 13.497236251831055, 16.019189834594727, 18.463668823242188, 17.717954635620117, 13.35957145690918, 18.463668823242188, 18.463668823242188, 13.359572410583496, 13.173151969909668, 13.35957145690918, 13.497237205505371, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.121259689331055, 16.019189834594727, 13.097433090209961, 13.35957145690918, 16.019189834594727, 13.497236251831055, 13.097433090209961, 18.463668823242188, 13.497236251831055, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 17.22940444946289, 13.497236251831055, 18.463668823242188, 16.019189834594727, 13.497237205505371, 13.097432136535645, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 16.019189834594727, 13.497236251831055, 13.121259689331055, 13.359573364257812, 16.019189834594727, 13.444768905639648, 18.463668823242188, 13.497236251831055, 16.019189834594727, 13.173154830932617, 16.019189834594727, 16.473682403564453, 13.359572410583496, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.497236251831055, 13.497236251831055, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 16.019189834594727, 18.463668823242188, 13.097431182861328, 13.497236251831055, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.097431182861328, 18.463668823242188, 13.097433090209961, 13.097433090209961, 16.019189834594727, 13.497236251831055, 13.097433090209961, 18.463668823242188, 13.121260643005371, 17.050811767578125, 13.121259689331055, 13.359573364257812, 13.097433090209961, 13.497236251831055, 13.497236251831055, 13.497236251831055, 18.463668823242188, 16.019189834594727, 18.463668823242188, 18.463668823242188, 13.173151016235352, 18.463668823242188, 16.019189834594727, 13.497236251831055, 18.463668823242188, 17.229398727416992, 16.019189834594727, 16.019189834594727, 13.497236251831055, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 13.497236251831055, 18.463668823242188, 13.497236251831055, 18.463668823242188, 18.463668823242188, 13.444746971130371, 18.463668823242188, 13.646698951721191, 18.463668823242188, 13.497236251831055, 18.463668823242188, 17.717954635620117, 13.097433090209961, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 17.229402542114258, 18.463668823242188, 16.019189834594727, 18.463668823242188, 13.497236251831055, 17.717952728271484, 13.359572410583496, 18.463668823242188, 13.359572410583496, 16.019189834594727, 13.497236251831055, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 17.229402542114258, 16.019189834594727, 18.463668823242188, 18.463668823242188, 13.497236251831055, 18.463668823242188, 13.359572410583496, 13.097433090209961, 13.497236251831055, 13.359573364257812, 18.463668823242188, 18.463668823242188, 13.444742202758789, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 13.173154830932617, 13.097433090209961, 13.097433090209961, 18.463668823242188, 18.463668823242188, 18.463668823242188, 16.019189834594727, 17.22940444946289, 17.22940444946289, 18.463668823242188, 13.444808959960938, 16.019189834594727, 16.019189834594727, 16.019189834594727, 16.473684310913086, 13.097433090209961, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 17.22940444946289, 13.173152923583984, 12.244561195373535, 13.497236251831055, 18.463668823242188, 16.019189834594727, 13.497236251831055, 16.019189834594727, 18.463668823242188, 16.019189834594727, 13.097433090209961, 16.019189834594727, 16.019189834594727, 13.497236251831055, 18.463668823242188, 16.019189834594727, 18.463668823242188, 15.372492790222168, 17.22940444946289, 16.019189834594727, 13.097433090209961, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 16.019189834594727, 13.097433090209961, 18.463668823242188, 18.463668823242188, 18.463668823242188, 18.463668823242188, 13.359573364257812, 16.019189834594727, 13.497236251831055, 13.497236251831055, 16.019189834594727, 13.497236251831055, 18.463668823242188, 13.35957145690918, 18.463668823242188, 18.463668823242188, 13.497236251831055, 18.463668823242188, 13.359573364257812, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727, 13.121259689331055, 18.463668823242188, 18.463668823242188, 12.237808227539062, 13.359573364257812, 16.473684310913086, 16.019189834594727, 13.497236251831055, 13.497236251831055, 13.444774627685547, 18.463668823242188, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 18.463668823242188, 13.359572410583496, 16.019189834594727, 16.019189834594727, 18.463668823242188, 13.444746971130371, 18.463668823242188, 16.019189834594727, 18.463668823242188, 16.019189834594727, 16.019189834594727, 16.019189834594727, 18.463668823242188, 18.463668823242188, 16.019189834594727]\n",
            "Mean Squared Error: 95.5797\n",
            "Mean Absolute Error: 6.3768\n",
            "Root Mean Squared Error (RMSE): 9.7765\n",
            "Mean Absolute Percentage Error (MAPE): 0.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5 Hypertuning the Parameters using Grid Search\n",
        "------------------------------------------------------\n",
        "For the best parameters of the model, we first define the set of parameters that we need to test which are:\n",
        "\n",
        "-output_size:32,64,128\n",
        "\n",
        "-learning_rate:0.001,0.005,0.01\n",
        "\n",
        "-optimizer:sgd,adam\n",
        "\n",
        "-dropout_rate: 0.3,0.5,0.7\n",
        "we then do a grid search to see which yields the best validation loss for the final model and see if it performs better results than the final model."
      ],
      "metadata": {
        "id": "PZq8JXHl-YRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter = {\n",
        "    'output_size': [32],  # Different sizes for the hidden layer\n",
        "    'learning_rate': [0.001, 0.005, 0.01],  # Different learning rates\n",
        "    'optimizer': ['sgd','adam'],  # Different optimizers\n",
        "    'dropout_rate': [0.3, 0.5, 0.7]  # Different dropout rates\n",
        "}\n",
        "\n",
        "#variable to keep the results of hypertuning parameters\n",
        "best_params = None\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Generate all combinations of hyperparameters as a list\n",
        "hyperparameter_combinations = list(itertools.product(\n",
        "    hyperparameter['output_size'],\n",
        "    hyperparameter['learning_rate'],\n",
        "    hyperparameter['optimizer'],\n",
        "    hyperparameter['dropout_rate']\n",
        "))"
      ],
      "metadata": {
        "id": "FagpoNISQ0Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the GNN model\n",
        "class GNN6(torch.nn.Module):\n",
        "    def __init__(self,input_dim,output_size,dropout_rate):\n",
        "        super(GNN6, self).__init__()\n",
        "\n",
        "        assert output_size % 8 == 0, \"Output size should be divisible by 8.\"\n",
        "\n",
        "        ###convolutional layer 1 to extract richer features from the data\n",
        "        self.conv1 = GCNConv(in_channels=input_dim, out_channels=output_size)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(output_size)\n",
        "\n",
        "        ##adding a hidden layer\n",
        "        self.fc_hidden = torch.nn.Linear(output_size, output_size//2)\n",
        "        self.bn_hidden = torch.nn.BatchNorm1d(output_size//2)\n",
        "\n",
        "       ### convolutional layer 2 further refining these features into a more compact form.\n",
        "        self.conv2 = GCNConv(in_channels=output_size//2, out_channels=output_size//4)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(output_size//4)\n",
        "\n",
        "        self.conv3 = GCNConv(in_channels=output_size//4, out_channels=output_size//8)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(output_size//8)\n",
        "\n",
        "        self.fc = torch.nn.Linear(output_size//8, 1)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc_hidden(x)\n",
        "        x = self.bn_hidden(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv3(x,edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        embeddings = x\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "t2X3eNDWa41G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(output_size, learning_rate, optimizer_name, dropout_rate):\n",
        "    # Initialize the model with given hyperparameters\n",
        "    model = GNN6(input_dim=5, output_size=output_size, dropout_rate=dropout_rate)\n",
        "    print(f'Currently Testing Model:{output_size},{learning_rate},{optimizer_name},{dropout_rate}')\n",
        "\n",
        "    # Define the optimizer\n",
        "    if optimizer_name == 'adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    elif optimizer_name == 'sgd':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = torch.nn.L1Loss()\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 500\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(500):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch in train_loader2:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(batch)\n",
        "            loss = criterion(out, batch.y.view(-1, 1).float())\n",
        "            if torch.isnan(loss).sum() > 0:\n",
        "                print(\"NaN loss encountered at epoch:\", epoch)\n",
        "                return float('inf')  # Return infinity to indicate a failed run\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item() * batch.num_graphs\n",
        "        train_loss /= len(train_loader2.dataset)\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader2:\n",
        "                out = model(batch)\n",
        "                loss = criterion(out, batch.y.view(-1, 1).float())\n",
        "                val_loss += loss.item() * batch.num_graphs\n",
        "        val_loss /= len(val_loader2.dataset)\n",
        "\n",
        "        # Early stopping check\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "\n",
        "        if epochs_no_improve >= patience:\n",
        "            break\n",
        "\n",
        "    return best_val_loss"
      ],
      "metadata": {
        "id": "vBCfxSAaq-8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for output_size, learning_rate, optimizer_name, dropout_rate in hyperparameter_combinations:\n",
        "    val_loss = train_and_evaluate(output_size, learning_rate, optimizer_name, dropout_rate)\n",
        "\n",
        "    print(f\"Tested params: hidden_layer_size={output_size}, learning_rate={learning_rate}, optimizer={optimizer_name}, dropout_rate={dropout_rate}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_params = {\n",
        "            'hidden_layer_size': output_size,\n",
        "            'learning_rate': learning_rate,\n",
        "            'optimizer': optimizer_name,\n",
        "            'dropout_rate': dropout_rate\n",
        "        }\n",
        "\n",
        "print(f\"Best hyperparameters: {best_params}, Best Validation Loss: {best_loss:.4f}\")"
      ],
      "metadata": {
        "id": "NY2bFq6qSBC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter = {\n",
        "    'output_size': [64],  # Different sizes for the hidden layer\n",
        "    'learning_rate': [0.001, 0.005, 0.01],  # Different learning rates\n",
        "    'optimizer': ['sgd','adam'],  # Different optimizers\n",
        "    'dropout_rate': [0.3, 0.5, 0.7]  # Different dropout rates\n",
        "}\n",
        "\n",
        "#variable to keep the results of hypertuning parameters\n",
        "best_params = None\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Generate all combinations of hyperparameters as a list\n",
        "hyperparameter_combinations = list(itertools.product(\n",
        "    hyperparameter['output_size'],\n",
        "    hyperparameter['learning_rate'],\n",
        "    hyperparameter['optimizer'],\n",
        "    hyperparameter['dropout_rate']\n",
        "))"
      ],
      "metadata": {
        "id": "qEw-vb6Euv42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for output_size, learning_rate, optimizer_name, dropout_rate in hyperparameter_combinations:\n",
        "    val_loss = train_and_evaluate(output_size, learning_rate, optimizer_name, dropout_rate)\n",
        "\n",
        "    print(f\"Tested params: hidden_layer_size={output_size}, learning_rate={learning_rate}, optimizer={optimizer_name}, dropout_rate={dropout_rate}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_params = {\n",
        "            'hidden_layer_size': output_size,\n",
        "            'learning_rate': learning_rate,\n",
        "            'optimizer': optimizer_name,\n",
        "            'dropout_rate': dropout_rate\n",
        "        }\n",
        "\n",
        "print(f\"Best hyperparameters: {best_params}, Best Validation Loss: {best_loss:.4f}\")"
      ],
      "metadata": {
        "id": "vLL8RHR3cRrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter = {\n",
        "    'output_size': [128],  # Different sizes for the hidden layer\n",
        "    'learning_rate': [ 0.001,0.005,0.01],  # Different learning rates\n",
        "    'optimizer': ['sgd','adam'],  # Different optimizers\n",
        "    'dropout_rate': [0.3, 0.5, 0.7]  # Different dropout rates\n",
        "}\n",
        "\n",
        "#variable to keep the results of hypertuning parameters\n",
        "best_params = None\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Generate all combinations of hyperparameters as a list\n",
        "hyperparameter_combinations = list(itertools.product(\n",
        "    hyperparameter['output_size'],\n",
        "    hyperparameter['learning_rate'],\n",
        "    hyperparameter['optimizer'],\n",
        "    hyperparameter['dropout_rate']\n",
        "))"
      ],
      "metadata": {
        "id": "O-97IEeBOKmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for output_size, learning_rate, optimizer_name, dropout_rate in hyperparameter_combinations:\n",
        "    val_loss = train_and_evaluate(output_size, learning_rate, optimizer_name, dropout_rate)\n",
        "\n",
        "    print(f\"Tested params: hidden_layer_size={output_size}, learning_rate={learning_rate}, optimizer={optimizer_name}, dropout_rate={dropout_rate}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_params = {\n",
        "            'hidden_layer_size': output_size,\n",
        "            'learning_rate': learning_rate,\n",
        "            'optimizer': optimizer_name,\n",
        "            'dropout_rate': dropout_rate\n",
        "        }\n",
        "\n",
        "print(f\"Best hyperparameters: {best_params}, Best Validation Loss: {best_loss:.4f}\")"
      ],
      "metadata": {
        "id": "bqysCXl47A5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter = {\n",
        "    'output_size': [128],  # Different sizes for the hidden layer\n",
        "    'learning_rate': [ 0.01],  # Different learning rates\n",
        "    'optimizer': ['adam'],  # Different optimizers\n",
        "    'dropout_rate': [0.3, 0.5, 0.7]  # Different dropout rates\n",
        "}\n",
        "\n",
        "#variable to keep the results of hypertuning parameters\n",
        "best_params = None\n",
        "best_loss = float('inf')\n",
        "\n",
        "# Generate all combinations of hyperparameters as a list\n",
        "hyperparameter_combinations = list(itertools.product(\n",
        "    hyperparameter['output_size'],\n",
        "    hyperparameter['learning_rate'],\n",
        "    hyperparameter['optimizer'],\n",
        "    hyperparameter['dropout_rate']\n",
        "))"
      ],
      "metadata": {
        "id": "U0NYP4Md1Xfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for output_size, learning_rate, optimizer_name, dropout_rate in hyperparameter_combinations:\n",
        "    val_loss = train_and_evaluate(output_size, learning_rate, optimizer_name, dropout_rate)\n",
        "\n",
        "    print(f\"Tested params: hidden_layer_size={output_size}, learning_rate={learning_rate}, optimizer={optimizer_name}, dropout_rate={dropout_rate}, Validation Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        best_params = {\n",
        "            'hidden_layer_size': output_size,\n",
        "            'learning_rate': learning_rate,\n",
        "            'optimizer': optimizer_name,\n",
        "            'dropout_rate': dropout_rate\n",
        "        }\n",
        "\n",
        "print(f\"Best hyperparameters: {best_params}, Best Validation Loss: {best_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88sl82lbPuSj",
        "outputId": "32e15bff-67e4-42bb-ca5f-3fe83be22017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently Testing Model:128,0.01,adam,0.3\n",
            "Tested params: hidden_layer_size=128, learning_rate=0.01, optimizer=adam, dropout_rate=0.3, Validation Loss: 6.0825\n",
            "Currently Testing Model:128,0.01,adam,0.5\n",
            "Tested params: hidden_layer_size=128, learning_rate=0.01, optimizer=adam, dropout_rate=0.5, Validation Loss: 6.0972\n",
            "Currently Testing Model:128,0.01,adam,0.7\n",
            "Tested params: hidden_layer_size=128, learning_rate=0.01, optimizer=adam, dropout_rate=0.7, Validation Loss: 6.1021\n",
            "Best hyperparameters: {'hidden_layer_size': 128, 'learning_rate': 0.01, 'optimizer': 'adam', 'dropout_rate': 0.3}, Best Validation Loss: 6.0825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of a new model architecture\n",
        "class GNN_New(torch.nn.Module):\n",
        "    def __init__(self, num_features):\n",
        "        super(GNN_New, self).__init__()\n",
        "        self.conv1 = GATConv(num_features, 32, heads=4, dropout=0.6)\n",
        "        self.conv2 = GATConv(32 * 4, 16, heads=4, dropout=0.6, concat=False)\n",
        "        self.fc = torch.nn.Linear(16, 1)  # Output layer\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        x = F.dropout(x, p=0.6, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7vv6oR0HorVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_new = GNN_New(5)\n",
        "criterion = torch.nn.L1Loss()\n",
        "optimizer_new = torch.optim.Adam(model_new.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "SASFFc2PpKP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Early stopping parameters for the new model\n",
        "patience = 500\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "\n",
        "for epoch in range(500):\n",
        "    model_new.train()\n",
        "    train_loss = 0\n",
        "    for batch in train_loader2:\n",
        "        optimizer_new.zero_grad()\n",
        "        out = model_new(batch)\n",
        "        loss = criterion(out, batch.y.view(-1, 1).float())\n",
        "        if torch.isnan(loss).sum() > 0:\n",
        "            print(\"NaN loss encountered at epoch:\", epoch)\n",
        "            break\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model_new.parameters(), max_norm=1.0)\n",
        "        optimizer_new.step()\n",
        "        train_loss += loss.item() * batch.num_graphs\n",
        "    train_loss /= len(train_loader2.dataset)\n",
        "\n",
        "    model_new.eval()\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader2:\n",
        "            out = model_new(batch)\n",
        "            loss = criterion(out, batch.y.view(-1, 1).float())\n",
        "            val_loss += loss.item() * batch.num_graphs\n",
        "\n",
        "    val_loss /= len(val_loader2.dataset)\n",
        "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Loss: {train_loss:.4f}, Best Val Loss: {best_val_loss:.4f}')\n",
        "\n",
        "    # Early stopping check\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "        # Save the best model\n",
        "        torch.save(model_new.state_dict(), 'best_model_new.pt')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "\n",
        "    if epochs_no_improve >= patience:\n",
        "        print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "        break\n",
        "\n",
        "    model_new.train()"
      ],
      "metadata": {
        "id": "NKNV1b2xpqMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "model_new.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader2:\n",
        "        out = model_new(batch)\n",
        "        predictions.extend(out.squeeze().cpu().numpy().tolist())\n",
        "\n",
        "print(f'Predicted valuations: {predictions}')\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "\n",
        "# Convert predictions and targets to numpy arrays for evaluation\n",
        "actuals = test_data.y.numpy()\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(actuals, predictions)\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "rmse = mean_squared_error(actuals, predictions, squared=False)\n",
        "mape = mean_absolute_percentage_error(actuals, predictions)\n",
        "\n",
        "\n",
        "print(f'Mean Squared Error: {mse:.4f}')\n",
        "print(f'Mean Absolute Error: {mae:.4f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {mape:.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7jtkD_DOCjf",
        "outputId": "41f4ff44-9225-4b81-e2e5-dedd76cd1bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted valuations: [21.804887771606445, 21.804889678955078, 34.936466217041016, 21.804889678955078, 19.9669189453125, 21.804887771606445, 27.818143844604492, 18.25661849975586, 21.746299743652344, 19.966873168945312, 27.631914138793945, 19.889678955078125, 19.89610481262207, 21.804887771606445, 17.08754539489746, 17.098899841308594, 21.804889678955078, 23.366872787475586, 21.804887771606445, 21.804885864257812, 16.95244598388672, 27.818143844604492, 18.246238708496094, 23.366870880126953, 21.83098602294922, 35.235755920410156, 21.804889678955078, 18.25661849975586, 21.804887771606445, 21.804889678955078, 21.791351318359375, 38.872833251953125, 21.804887771606445, 27.860628128051758, 19.889671325683594, 18.289207458496094, 27.85369873046875, 27.67658805847168, 21.77044677734375, 21.79109001159668, 21.804889678955078, 27.86063003540039, 34.936466217041016, 17.170284271240234, 22.82923126220703, 27.676589965820312, 27.676589965820312, 21.804889678955078, 27.854129791259766, 21.804889678955078, 17.170917510986328, 21.803585052490234, 21.7896671295166, 22.82920265197754, 17.170902252197266, 19.26822853088379, 28.209095001220703, 21.784135818481445, 18.367111206054688, 21.804885864257812, 38.59703826904297, 21.804889678955078, 27.63190460205078, 18.295455932617188, 21.804887771606445, 21.804889678955078, 34.936466217041016, 27.676589965820312, 21.804887771606445, 17.093883514404297, 21.804889678955078, 21.804887771606445, 21.78781509399414, 19.89610481262207, 21.804885864257812, 27.81825828552246, 21.804889678955078, 19.846420288085938, 18.28976821899414, 21.804889678955078, 19.386493682861328, 21.804889678955078, 28.209362030029297, 21.804885864257812, 8.385025024414062, 21.791667938232422, 27.676589965820312, 19.365497589111328, 19.846420288085938, 21.804887771606445, 18.246234893798828, 21.804887771606445, 21.813636779785156, 21.804885864257812, 21.77043914794922, 27.74496078491211, 34.936466217041016, 21.804887771606445, 21.804889678955078, 21.804887771606445, 21.790904998779297, 27.744962692260742, 27.818143844604492, 27.88665008544922, 21.790199279785156, 27.676589965820312, 16.952497482299805, 21.804885864257812, 27.85413360595703, 35.2410888671875, 19.268230438232422, 27.853647232055664, 23.366870880126953, 27.860631942749023, 27.818143844604492, 21.804889678955078, 21.746295928955078, 27.676586151123047, 27.676586151123047, 21.804887771606445, 19.896102905273438, 21.804889678955078, 28.20350456237793, 21.804889678955078, 21.804889678955078, 31.374954223632812, 17.18892478942871, 21.804885864257812, 27.67658805847168, 34.704872131347656, 21.804889678955078, 27.63190269470215, 27.860631942749023, 21.804889678955078, 21.78781509399414, 27.631916046142578, 19.966888427734375, 27.972082138061523, 27.81826400756836, 21.804889678955078, 27.818260192871094, 22.829214096069336, 21.804889678955078, 27.676589965820312, 21.791664123535156, 21.804889678955078, 27.676586151123047, 21.804889678955078, 27.725357055664062, 21.80359649658203, 22.360767364501953, 21.804885864257812, 18.368724822998047, 28.221237182617188, 21.77472686767578, 21.804887771606445, 27.744958877563477, 23.366870880126953, 35.23575973510742, 21.80489158630371, 31.374961853027344, 34.936466217041016, 21.791091918945312, 21.804889678955078, 21.804885864257812, 21.813636779785156, 27.879478454589844, 19.365497589111328, 19.277164459228516, 27.67658805847168, 21.804887771606445, 21.78781509399414, 27.744956970214844, 27.91819190979004, 21.830974578857422, 21.804887771606445, 21.804889678955078, 21.804889678955078, 27.81825828552246, 21.774728775024414, 21.79020118713379, 27.676591873168945, 34.936466217041016, 21.80489158630371, 17.170913696289062, 21.789657592773438, 21.804889678955078, 27.676589965820312, 27.744956970214844, 21.793737411499023, 21.804885864257812, 21.804889678955078, 34.936466217041016, 23.366870880126953, 27.744964599609375, 19.846420288085938, 27.67658805847168, 19.268234252929688, 21.804887771606445, 28.20361328125, 38.597042083740234, 21.78413200378418, 21.804889678955078, 35.23575973510742, 18.368738174438477, 27.853904724121094, 27.919979095458984, 27.63190460205078, 18.296152114868164, 21.789621353149414, 27.744956970214844, 27.63190269470215, 27.676589965820312, 27.631914138793945, 21.79376983642578, 21.804889678955078, 21.804889678955078, 27.744964599609375, 28.20915985107422, 27.676589965820312, 18.33173179626465, 27.887962341308594, 27.631916046142578, 19.940814971923828, 27.854074478149414, 19.326772689819336, 27.933515548706055, 21.813636779785156, 21.813636779785156, 18.24623680114746, 27.818140029907227, 21.804885864257812, 27.919979095458984, 21.80489158630371, 21.78781509399414, 27.972082138061523, 34.87864685058594, 17.071529388427734, 27.676589965820312, 27.818143844604492, 17.09385871887207, 21.787813186645508, 18.053752899169922, 19.290130615234375, 23.963558197021484, 27.631914138793945, 21.787813186645508, 36.815345764160156, 27.631914138793945, 27.74496841430664, 27.86063003540039, 21.80489158630371, 21.804889678955078, 27.631912231445312, 27.676586151123047, 21.804889678955078, 27.919981002807617, 21.804889678955078, 28.203632354736328, 27.631914138793945, 21.804889678955078, 21.804889678955078, 34.936466217041016, 21.804887771606445, 21.787813186645508, 23.366870880126953, 21.804885864257812, 21.746294021606445, 27.81814193725586, 18.25661849975586, 17.170856475830078, 34.936466217041016, 27.725379943847656, 21.791667938232422, 21.804887771606445, 17.15058135986328, 21.804887771606445, 27.744964599609375, 17.170351028442383, 34.936466217041016, 38.872833251953125, 17.15058708190918, 21.804887771606445, 19.966842651367188, 27.818143844604492, 28.20948028564453, 21.804889678955078, 27.919979095458984, 27.91619110107422, 21.790557861328125, 21.73805046081543, 21.804887771606445, 23.107938766479492, 21.804887771606445, 21.804887771606445, 27.818260192871094, 27.86063003540039, 21.790559768676758, 21.813636779785156, 27.916194915771484, 27.919986724853516, 18.25661849975586, 19.365495681762695, 27.676589965820312, 27.91997718811035, 21.804889678955078, 21.83098793029785, 21.804889678955078, 27.37821388244629, 27.676586151123047, 21.789627075195312, 19.365497589111328, 27.676584243774414, 21.813636779785156, 21.804889678955078, 21.787813186645508, 23.366872787475586, 27.879478454589844, 21.804889678955078, 21.804889678955078, 27.63190269470215, 27.85416030883789, 27.676589965820312, 21.756397247314453, 27.818260192871094, 27.378215789794922, 21.804885864257812, 27.744956970214844, 27.63190460205078, 21.804885864257812, 21.804887771606445, 18.28977394104004, 17.17091941833496, 27.631916046142578, 28.203533172607422, 27.493732452392578, 27.631906509399414, 18.3670711517334, 27.63190460205078, 21.804887771606445, 17.10271453857422, 27.67658805847168, 18.368751525878906, 19.846424102783203, 27.927268981933594, 28.203584671020508, 27.63190269470215, 20.03985023498535, 18.296154022216797, 27.63190269470215, 21.804889678955078, 27.919979095458984, 27.631914138793945, 17.00546646118164, 27.725353240966797, 21.787811279296875, 21.804889678955078, 21.804885864257812, 27.378211975097656, 21.804887771606445, 21.790416717529297, 27.744958877563477, 21.79373550415039, 21.791481018066406, 27.676586151123047, 27.853527069091797, 27.67658805847168, 18.29615020751953, 27.886653900146484, 27.912410736083984, 27.86063003540039, 21.790559768676758, 27.631912231445312, 21.804885864257812, 27.631916046142578, 21.804885864257812, 27.972061157226562, 27.854129791259766, 17.0054874420166, 27.744958877563477, 27.818260192871094, 21.804885864257812, 21.804889678955078, 28.203462600708008, 21.804885864257812, 27.676589965820312, 23.366870880126953, 27.744956970214844, 27.676586151123047, 21.746299743652344, 19.290132522583008, 21.804885864257812, 21.804889678955078, 21.804889678955078, 27.919981002807617, 27.886653900146484, 26.35390853881836, 21.802017211914062, 21.784133911132812, 21.804889678955078, 27.63190269470215, 34.936466217041016, 18.33180809020996, 27.87948226928711, 21.802061080932617, 27.918203353881836, 27.631908416748047, 21.804887771606445, 27.818260192871094, 19.290130615234375, 27.676584243774414, 19.846426010131836, 27.818140029907227, 21.804885864257812, 18.367063522338867, 21.804885864257812, 27.63190460205078, 17.170391082763672, 21.791664123535156, 18.25661849975586, 34.704959869384766, 27.919981002807617, 19.277162551879883, 19.365497589111328, 27.853763580322266, 27.818143844604492, 22.829219818115234, 21.804889678955078, 27.744964599609375, 23.366872787475586, 21.804889678955078, 27.854198455810547, 27.912410736083984, 21.87343978881836, 27.87948226928711, 21.804887771606445, 19.268230438232422, 27.378215789794922, 21.804887771606445, 18.246234893798828, 21.804887771606445, 27.97207260131836, 21.813636779785156, 27.631916046142578, 21.813636779785156, 21.804889678955078, 27.818138122558594, 21.813634872436523, 21.804889678955078, 18.367515563964844, 21.813636779785156, 27.818143844604492, 27.93358612060547, 21.804889678955078, 27.67658805847168, 19.873096466064453, 21.804889678955078, 17.10271644592285, 21.830978393554688, 38.59767532348633, 21.804889678955078, 21.804889678955078, 27.74496078491211, 21.804889678955078, 27.972061157226562, 27.919979095458984, 21.804889678955078, 21.804885864257812, 19.365497589111328, 36.81535339355469, 21.804887771606445, 27.059600830078125, 21.804889678955078, 21.804885864257812, 21.804889678955078, 23.963558197021484, 27.493730545043945, 17.43324089050293, 27.88665008544922, 18.315364837646484, 21.79020118713379, 21.78781509399414, 23.366870880126953, 19.88966941833496, 19.96685791015625, 18.276748657226562, 38.872833251953125, 35.23720932006836, 27.74496078491211, 19.277164459228516, 21.79056167602539, 21.790557861328125, 27.91998291015625, 21.79041290283203, 21.804889678955078, 21.804889678955078, 27.631912231445312, 27.631916046142578, 38.59737014770508, 38.273101806640625, 27.919979095458984, 26.874847412109375, 17.071197509765625, 19.96683692932129, 27.631912231445312, 21.804885864257812, 27.825122833251953, 28.22123908996582, 19.365495681762695, 21.790416717529297, 21.804887771606445, 21.790889739990234, 27.879484176635742, 27.81825828552246, 21.804889678955078, 21.804885864257812, 27.74496078491211, 27.67658805847168, 21.804887771606445, 27.63190269470215, 22.829191207885742, 27.744962692260742, 21.804889678955078, 19.268230438232422, 21.804887771606445, 17.433805465698242, 21.804885864257812, 19.324932098388672, 27.879478454589844, 27.676586151123047, 17.433212280273438, 28.2033634185791, 27.631906509399414, 27.886648178100586, 27.81814193725586, 21.804889678955078, 21.790199279785156, 27.818138122558594, 21.813636779785156, 27.631912231445312, 21.793733596801758, 38.872833251953125, 27.676591873168945, 21.813636779785156, 21.813636779785156, 27.919981002807617, 21.804885864257812, 27.818260192871094, 21.770450592041016, 27.67658805847168, 16.95246124267578, 27.818140029907227, 21.804885864257812, 21.804889678955078, 18.296152114868164, 21.804889678955078, 21.804889678955078, 21.790559768676758, 21.79149627685547, 18.45085334777832, 27.854124069213867, 21.804885864257812, 17.15058135986328, 21.804889678955078, 21.804889678955078, 19.873096466064453, 21.804889678955078, 21.804889678955078, 27.63190460205078, 19.27526092529297, 21.804889678955078, 21.804885864257812, 21.770444869995117, 35.23802185058594, 27.854217529296875, 21.74629783630371, 27.631912231445312, 21.804885864257812, 21.738046646118164, 21.804885864257812, 21.804889678955078, 27.853958129882812, 21.804889678955078, 27.97207260131836, 21.804885864257812, 21.804889678955078, 27.37821388244629, 27.67658805847168, 27.493730545043945, 18.27420997619629, 23.366872787475586, 21.804885864257812, 21.787813186645508, 27.818138122558594, 23.00951385498047, 21.804889678955078, 21.804885864257812, 27.818256378173828, 21.787813186645508, 27.919979095458984, 27.807849884033203, 27.631914138793945, 17.089004516601562, 36.81535339355469, 19.277156829833984, 27.919979095458984, 21.830978393554688, 27.744962692260742, 21.804887771606445, 18.296154022216797, 27.854045867919922, 27.74496078491211, 27.67658805847168, 27.631908416748047, 21.813636779785156, 18.36719512939453, 27.631912231445312, 27.631912231445312, 21.79056167602539, 27.879478454589844, 21.804889678955078, 19.365495681762695, 19.896102905273438, 27.92727279663086, 18.368488311767578, 21.80191993713379, 27.676586151123047, 21.804889678955078, 17.098899841308594, 21.790416717529297, 18.367040634155273, 27.676589965820312, 19.896102905273438, 27.818256378173828, 21.756399154663086, 21.789628982543945, 17.098899841308594, 27.676586151123047, 17.098899841308594, 23.366870880126953, 21.804885864257812, 27.972068786621094, 19.846426010131836, 21.78414535522461, 21.804885864257812, 28.20362663269043, 21.74629783630371, 23.96356201171875, 23.963558197021484, 18.246231079101562, 21.791336059570312, 27.676589965820312, 19.966960906982422, 21.804889678955078, 21.804885864257812, 18.314838409423828, 27.493736267089844, 27.63190460205078, 27.63191795349121, 21.813636779785156, 27.886655807495117, 27.63191032409668, 27.631914138793945, 27.631916046142578, 21.78414535522461, 21.804885864257812, 17.087539672851562, 23.366870880126953, 27.744956970214844, 34.70487976074219, 27.81826400756836, 27.63190460205078, 23.963558197021484, 21.804887771606445, 21.804885864257812, 21.804887771606445, 27.676586151123047, 21.804887771606445, 27.91619110107422, 27.631908416748047, 21.804889678955078, 27.63190460205078, 21.804885864257812, 21.804889678955078, 27.818138122558594, 21.790559768676758, 27.818140029907227, 27.854167938232422, 27.81826400756836, 18.24623680114746, 27.63190269470215, 27.74496078491211, 38.59735107421875, 21.746295928955078, 21.804887771606445, 27.744956970214844, 17.88019561767578, 21.79166603088379, 17.088802337646484, 35.23777770996094, 27.676586151123047, 19.96689224243164, 21.804889678955078, 28.20346450805664, 27.88665008544922, 21.804885864257812, 21.804889678955078, 21.787811279296875, 27.91241455078125, 21.78964614868164, 21.804885864257812, 27.85400390625, 17.093971252441406, 27.676586151123047, 17.072660446166992, 34.936466217041016, 27.19054412841797, 27.631912231445312, 21.804889678955078, 28.21814727783203, 21.804885864257812, 27.67658805847168, 27.853357315063477, 27.676586151123047, 19.324981689453125, 27.631914138793945, 27.631908416748047, 27.81825828552246, 27.52860450744629, 28.203617095947266, 19.846424102783203, 18.289762496948242, 27.818262100219727, 21.804887771606445, 21.804885864257812, 21.804887771606445, 21.804887771606445, 27.972064971923828, 21.997478485107422, 21.804889678955078, 21.804889678955078, 19.94083023071289, 21.79090118408203, 35.238128662109375, 27.919981002807617, 21.787813186645508, 21.804887771606445, 21.804889678955078, 19.84642219543457, 21.804893493652344, 17.078327178955078, 27.818138122558594, 19.883808135986328, 27.631912231445312, 21.804887771606445, 27.74496078491211, 21.787817001342773, 19.846426010131836, 27.91619110107422, 27.631914138793945, 38.59706115722656, 18.25661849975586, 27.63190460205078, 21.804885864257812, 19.365495681762695, 27.933528900146484, 35.235755920410156, 21.804887771606445, 23.366870880126953, 21.78781509399414, 21.830978393554688, 19.96685218811035, 27.744962692260742, 21.804885864257812, 27.631912231445312, 17.08754539489746, 21.80489158630371, 21.79376983642578, 21.804885864257812, 21.78781509399414, 27.744962692260742, 27.927276611328125, 34.936466217041016, 21.804889678955078, 27.631908416748047, 21.789703369140625, 27.631912231445312, 17.12331199645996, 21.804889678955078, 38.597171783447266, 19.88381576538086, 21.804889678955078, 21.790203094482422, 21.804887771606445, 27.676591873168945, 23.366870880126953, 27.63191032409668, 27.879478454589844, 21.789676666259766, 19.324935913085938, 27.631912231445312, 21.813636779785156, 23.963558197021484, 27.74496078491211, 27.631912231445312, 27.676589965820312, 19.92703628540039, 27.63190460205078, 21.789714813232422, 21.804889678955078, 27.63191032409668, 27.919973373413086, 21.804885864257812, 27.63191032409668, 21.804887771606445, 27.676586151123047, 27.97207260131836, 21.804885864257812, 18.25661849975586, 21.804889678955078, 27.86063003540039, 21.83098602294922, 28.14520263671875, 19.290191650390625, 19.940807342529297, 22.36077117919922, 27.86063003540039, 18.296152114868164, 27.63191032409668, 21.774728775024414, 21.7908992767334, 21.804887771606445, 27.63190460205078, 21.78781509399414, 20.684932708740234, 17.974586486816406, 21.804887771606445, 27.919979095458984, 21.78781509399414, 21.804885864257812, 22.82921600341797, 17.098899841308594, 21.78781509399414, 35.23575973510742, 21.804887771606445, 23.366872787475586, 27.744958877563477, 27.879484176635742, 27.631914138793945, 21.804885864257812, 21.804889678955078, 27.919979095458984, 21.80489158630371, 21.804887771606445, 21.813634872436523, 21.804889678955078, 21.804887771606445, 27.853897094726562, 18.246238708496094, 21.79056167602539, 27.37821388244629, 27.818138122558594, 22.82919692993164, 21.791667938232422, 21.804889678955078, 21.78781509399414, 27.676589965820312, 27.86063003540039, 27.972068786621094, 23.366870880126953, 27.676586151123047, 16.952470779418945, 21.787813186645508, 21.790559768676758, 27.972084045410156, 27.86063003540039, 21.804887771606445, 27.972049713134766, 21.79166603088379, 21.996902465820312, 23.366870880126953, 19.927169799804688, 21.77044677734375, 21.804885864257812, 21.790203094482422, 23.366870880126953, 27.676586151123047, 28.20917510986328, 27.919981002807617, 27.63190460205078, 27.81814193725586, 28.203495025634766, 21.79041862487793, 27.67658805847168, 21.770450592041016, 21.790407180786133, 21.790557861328125, 27.919979095458984, 21.774730682373047, 21.803586959838867, 28.218002319335938, 28.203407287597656, 21.804885864257812, 21.804889678955078, 17.150588989257812, 17.150585174560547, 21.78781509399414, 27.74496078491211, 21.804885864257812, 27.919981002807617, 27.74496078491211, 23.026992797851562, 21.756397247314453, 21.804889678955078, 21.79041290283203, 21.813636779785156, 21.738048553466797, 21.804885864257812, 27.818260192871094, 21.790557861328125, 21.804885864257812, 21.78781509399414, 21.804887771606445, 27.972061157226562, 27.767526626586914, 21.804887771606445, 27.933570861816406, 16.952510833740234, 18.331846237182617, 21.813636779785156, 21.804889678955078, 27.631900787353516, 21.804889678955078, 21.804889678955078, 27.97208023071289, 21.790557861328125, 16.952556610107422, 17.07827377319336, 21.804885864257812, 21.804887771606445, 19.277170181274414, 35.240028381347656, 18.289207458496094, 21.804885864257812, 34.936466217041016, 27.63190269470215, 21.804887771606445, 21.804889678955078, 23.366870880126953, 23.366870880126953, 21.791431427001953, 27.91998291015625, 19.873096466064453, 21.804885864257812, 27.93354606628418, 19.96685028076172, 27.631908416748047, 27.631914138793945, 21.804889678955078, 27.631914138793945, 18.282840728759766, 27.85388946533203, 27.81826400756836, 21.804885864257812, 21.813636779785156, 19.268226623535156, 21.790199279785156, 27.82512092590332, 27.854232788085938, 22.829246520996094, 27.860628128051758, 21.804887771606445, 17.170562744140625, 21.804885864257812, 21.813636779785156, 21.784147262573242, 20.61562156677246, 21.790559768676758, 28.221237182617188, 21.78781509399414, 27.886653900146484, 19.966949462890625, 17.17091178894043, 21.791349411010742, 21.789661407470703, 35.235755920410156, 27.818260192871094, 21.803592681884766, 27.744958877563477, 21.801963806152344, 27.82512664794922, 27.63190269470215, 27.744962692260742, 19.889678955078125, 21.804885864257812, 21.79166603088379, 38.872833251953125, 21.804885864257812, 21.78781509399414, 17.087549209594727, 21.804885864257812, 28.20360565185547, 21.804885864257812, 27.744964599609375, 27.72534942626953, 21.813636779785156, 35.23575973510742, 21.79376983642578, 21.804887771606445, 27.81814193725586, 31.374961853027344, 21.997358322143555, 21.804889678955078, 27.67658805847168, 27.74496078491211, 21.766902923583984, 21.790199279785156, 27.676591873168945, 21.804889678955078, 21.804889678955078, 21.78781509399414, 21.804887771606445, 19.966838836669922, 21.804885864257812, 18.162630081176758, 27.63190269470215, 20.039779663085938, 21.804889678955078, 27.74496078491211, 21.804885864257812, 36.81535339355469, 21.804887771606445, 21.804885864257812, 27.972068786621094, 21.78781509399414, 26.781909942626953, 27.63190460205078, 21.789634704589844, 21.790416717529297, 21.804887771606445, 21.8035888671875, 27.818134307861328, 27.67658805847168, 17.170913696289062, 19.846420288085938, 27.744956970214844, 27.676586151123047, 27.744958877563477, 21.804885864257812, 27.63191032409668, 27.676589965820312, 27.631900787353516, 21.738048553466797, 27.919986724853516, 27.91998291015625, 20.039932250976562, 21.804885864257812, 18.527236938476562, 27.676589965820312, 21.804887771606445, 21.791664123535156, 17.10271644592285, 21.789661407470703, 27.88665199279785, 27.860631942749023, 27.81825828552246, 17.150588989257812, 19.275360107421875, 23.366870880126953, 27.81814193725586, 21.804885864257812, 27.744956970214844, 27.744958877563477, 27.85366439819336, 21.774728775024414, 27.74496078491211, 21.78781509399414, 27.631916046142578, 21.804889678955078, 28.221237182617188, 23.366870880126953, 19.846418380737305, 23.366870880126953, 27.744958877563477, 21.804889678955078, 22.829212188720703, 21.804889678955078, 27.81826400756836, 27.860626220703125, 21.789642333984375, 27.63190460205078, 17.087543487548828, 21.787811279296875, 21.78781509399414, 19.96684455871582, 27.67658805847168, 34.41093826293945, 27.86063003540039, 21.73805046081543, 19.365497589111328, 18.276748657226562, 27.81825828552246, 27.81826400756836, 27.676586151123047, 21.804887771606445, 21.73805046081543, 27.676589965820312, 27.67658805847168, 27.631916046142578, 27.67658805847168, 27.818262100219727, 21.746295928955078, 38.872833251953125, 27.676589965820312, 27.631912231445312, 18.246238708496094, 27.91619873046875, 21.804887771606445, 21.790557861328125, 21.804885864257812, 27.631914138793945, 38.59769058227539, 19.846424102783203, 38.872833251953125, 21.804885864257812, 21.804885864257812, 18.3687801361084, 34.70502471923828, 27.744958877563477, 19.966859817504883, 21.804885864257812, 27.63190460205078, 21.804887771606445, 27.972064971923828, 28.209272384643555, 21.790557861328125, 27.63190460205078, 27.744966506958008, 27.972082138061523, 21.804889678955078, 21.804887771606445, 38.59725570678711, 27.81825828552246, 27.744958877563477, 34.936466217041016, 21.804885864257812, 21.804885864257812, 27.919979095458984, 27.63190460205078, 27.67658233642578, 27.67658233642578, 21.804889678955078, 21.804889678955078, 21.804885864257812, 21.803590774536133, 19.873096466064453, 21.78781509399414, 19.966842651367188, 27.744958877563477, 23.366870880126953, 21.756397247314453, 27.853097915649414, 21.804889678955078, 38.59722900390625, 19.966833114624023, 21.804885864257812, 21.804885864257812, 19.87309455871582, 16.952468872070312, 19.940845489501953, 21.78980255126953, 27.81814193725586, 27.88665199279785, 27.744964599609375, 19.26822280883789, 27.744956970214844, 18.25661849975586, 19.926959991455078, 27.631916046142578, 19.618221282958984, 18.368812561035156, 21.804885864257812, 21.756397247314453, 21.804885864257812, 21.78781509399414, 21.804889678955078, 21.804889678955078, 21.804889678955078, 28.20922088623047, 22.829282760620117, 21.78781509399414, 34.936466217041016, 21.813636779785156, 18.367027282714844, 27.676586151123047, 23.366870880126953, 27.652345657348633, 27.860626220703125, 27.860633850097656, 27.631914138793945, 21.789663314819336, 19.381725311279297, 19.896102905273438, 27.919981002807617, 17.088830947875977, 21.787813186645508, 21.813636779785156, 27.972076416015625, 17.15058708190918, 27.879478454589844, 35.235755920410156, 19.966838836669922, 27.631912231445312, 21.804887771606445, 21.80489158630371, 27.854145050048828, 21.813636779785156, 21.7896728515625, 21.804885864257812, 27.676589965820312, 34.936466217041016, 27.87948226928711, 21.804887771606445, 34.936466217041016, 21.804885864257812, 18.296152114868164, 21.813636779785156, 21.804887771606445, 21.804889678955078, 21.804889678955078, 21.804887771606445, 21.804887771606445, 21.804889678955078, 27.818256378173828, 18.296152114868164, 21.804889678955078, 18.282840728759766, 18.24623680114746, 27.676589965820312, 21.813634872436523, 18.276752471923828, 21.804889678955078, 19.277172088623047, 18.052650451660156, 19.290117263793945, 19.846424102783203, 18.246238708496094, 22.829204559326172, 21.738048553466797, 21.756393432617188, 21.790559768676758, 27.879478454589844, 21.804889678955078, 21.804887771606445, 17.434280395507812, 21.804885864257812, 27.91619110107422, 21.813636779785156, 21.791095733642578, 36.81534194946289, 27.631916046142578, 27.631916046142578, 22.829238891601562, 21.804889678955078, 21.804887771606445, 27.818262100219727, 27.676586151123047, 21.791431427001953, 21.738052368164062, 21.804885864257812, 22.8292236328125, 21.804889678955078, 21.804889678955078, 17.07821273803711, 21.78781509399414, 18.162628173828125, 21.804889678955078, 22.829313278198242, 21.790557861328125, 38.59708786010742, 18.28284454345703, 27.631914138793945, 27.744958877563477, 21.80489158630371, 21.804885864257812, 21.804887771606445, 21.804889678955078, 27.818262100219727, 27.725330352783203, 21.79090118408203, 27.818138122558594, 21.804885864257812, 21.756397247314453, 38.872833251953125, 20.039470672607422, 21.804887771606445, 19.966943740844727, 27.77135467529297, 21.813636779785156, 21.804889678955078, 27.631914138793945, 27.744962692260742, 21.804889678955078, 27.72538948059082, 27.631914138793945, 21.804885864257812, 21.804887771606445, 21.789661407470703, 21.804885864257812, 19.966842651367188, 18.288991928100586, 22.829195022583008, 19.89610481262207, 23.366870880126953, 21.78781509399414, 17.170394897460938, 27.81813621520996, 27.67658805847168, 27.676586151123047, 21.79166603088379, 21.804889678955078, 17.150585174560547, 18.24623680114746, 18.368799209594727, 21.804889678955078, 21.804885864257812, 23.366870880126953, 27.744964599609375, 28.209423065185547, 28.21808433532715, 21.804885864257812, 17.094022750854492, 27.919979095458984, 27.854013442993164, 27.63191032409668, 35.24018859863281, 18.246234893798828, 27.63190460205078, 23.366870880126953, 21.804885864257812, 21.804887771606445, 28.21817398071289, 16.952491760253906, 26.876964569091797, 21.784137725830078, 21.804885864257812, 27.493736267089844, 21.770450592041016, 27.676586151123047, 21.804885864257812, 34.936466217041016, 18.28978157043457, 27.818138122558594, 26.78191375732422, 21.80206298828125, 21.804889678955078, 27.67658805847168, 21.78781509399414, 24.016937255859375, 28.20338249206543, 27.744966506958008, 18.276748657226562, 27.818262100219727, 27.631914138793945, 21.804889678955078, 21.804887771606445, 27.972078323364258, 27.81814193725586, 18.288991928100586, 21.804889678955078, 21.804885864257812, 21.804889678955078, 23.366870880126953, 19.846424102783203, 27.74496078491211, 21.813636779785156, 22.829330444335938, 27.631912231445312, 22.82924461364746, 21.804889678955078, 19.966835021972656, 21.804889678955078, 21.804885864257812, 21.784143447875977, 21.804887771606445, 19.846426010131836, 21.804885864257812, 27.67658805847168, 27.67658805847168, 21.804885864257812, 21.791095733642578, 27.652347564697266, 19.365495681762695, 21.804885864257812, 21.804887771606445, 27.453975677490234, 19.883811950683594, 35.240291595458984, 27.63190460205078, 21.78965950012207, 21.738048553466797, 17.098899841308594, 21.78781509399414, 27.676589965820312, 27.676586151123047, 21.804885864257812, 21.787813186645508, 21.804885864257812, 20.039011001586914, 27.818256378173828, 27.493736267089844, 21.790557861328125, 17.078237533569336, 21.804885864257812, 34.936466217041016, 21.804889678955078, 27.74496078491211, 27.972087860107422, 27.818143844604492, 21.804887771606445, 21.804885864257812, 27.676584243774414]\n",
            "Mean Squared Error: 149.6622\n",
            "Mean Absolute Error: 9.8392\n",
            "Root Mean Squared Error (RMSE): 12.2337\n",
            "Mean Absolute Percentage Error (MAPE): 0.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.1 Hypertuning Results\n",
        "The results of running several tests show that as the model behaves best with adam and the best results come from the learning rate of 0.01 and hidden_layer_size of 128 and dropout = 0.3. The validation loss of 6.0825 was the best performing out of all the models. Therefore, we shall implement to see if it surpasses the final model's metrics."
      ],
      "metadata": {
        "id": "_3GA6K6TjKzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the GNN model\n",
        "class GNN6(torch.nn.Module):\n",
        "    def __init__(self,input_dim,output_size,dropout_rate):\n",
        "        super(GNN6, self).__init__()\n",
        "\n",
        "        assert output_size % 8 == 0, \"Output size should be divisible by 8.\"\n",
        "\n",
        "        ###convolutional layer 1 to extract richer features from the data\n",
        "        self.conv1 = GCNConv(in_channels=input_dim, out_channels=output_size)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(output_size)\n",
        "\n",
        "        ##adding a hidden layer\n",
        "        self.fc_hidden = torch.nn.Linear(output_size, output_size//2)\n",
        "        self.bn_hidden = torch.nn.BatchNorm1d(output_size//2)\n",
        "\n",
        "       ### convolutional layer 2 further refining these features into a more compact form.\n",
        "        self.conv2 = GCNConv(in_channels=output_size//2, out_channels=output_size//4)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(output_size//4)\n",
        "\n",
        "        self.conv3 = GCNConv(in_channels=output_size//4, out_channels=output_size//8)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(output_size//8)\n",
        "\n",
        "        self.fc = torch.nn.Linear(output_size//8, 1)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.fc_hidden(x)\n",
        "        x = self.bn_hidden(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = self.conv3(x,edge_index)\n",
        "        x = self.bn3(x)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        embeddings = x\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "OWctFge5OHeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model_Iteration6 = GNN6(input_dim=5,output_size=128,dropout_rate=0.3)\n",
        "criterion = torch.nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model_Iteration6.parameters(), lr=0.01)\n",
        "\n",
        "# Early stopping parameters\n",
        "patience = 500  # Number of epochs to wait for improvement\n",
        "best_val_loss = float('inf')  # Initialize with a large value\n",
        "epochs_no_improve = 0  # Count of epochs since the last improvement\n",
        "\n",
        "\n",
        "for epoch in range(500):\n",
        "  model_Iteration6.train()\n",
        "  train_loss = 0\n",
        "  for batch in train_loader2:\n",
        "      optimizer.zero_grad()\n",
        "      out = model_Iteration6(batch)\n",
        "      loss = criterion(out, batch.y.view(-1, 1).float())\n",
        "      if torch.isnan(loss).sum() > 0:\n",
        "          print(\"NaN loss encountered at epoch:\", epoch)\n",
        "          break\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(model_Iteration6.parameters(), max_norm=1.0)\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item() * batch.num_graphs\n",
        "  train_loss /= len(train_loader2.dataset)\n",
        "\n",
        "\n",
        "  model_Iteration6.eval()  # Set the model to evaluation mode\n",
        "  val_loss = 0  # Initialize validation loss\n",
        "\n",
        "  with torch.no_grad():  # No gradient computation\n",
        "      for batch in val_loader2:\n",
        "          out = model_Iteration6(batch)  # Forward pass\n",
        "          loss = criterion(out, batch.y.view(-1, 1).float())  # Compute loss\n",
        "          val_loss += loss.item() * batch.num_graphs  # Accumulate validation loss\n",
        "\n",
        "  val_loss /= len(val_loader2.dataset)  # Average validation loss\n",
        "  print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f},Loss: {train_loss:.4f},best_val_loss:{best_val_loss:.4f}')\n",
        "\n",
        "  # Early stopping check\n",
        "  if val_loss < best_val_loss:\n",
        "      best_val_loss = val_loss\n",
        "      epochs_no_improve = 0\n",
        "      # Save the best model\n",
        "      torch.save(model_Iteration6.state_dict(), 'best_model2.pt')\n",
        "  else:\n",
        "      epochs_no_improve += 1\n",
        "\n",
        "    # Check if early stopping criterion is met\n",
        "  if epochs_no_improve >= patience:\n",
        "      print(f'Early stopping triggered after {epoch+1} epochs')\n",
        "      break\n",
        "\n",
        "    # Set the model back to training mode for the next epoch\n",
        "  model_Iteration6.train()"
      ],
      "metadata": {
        "id": "7qatmBDdw4AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "model_Iteration6.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader2:\n",
        "        out = model_Iteration6(batch)\n",
        "        predictions.extend(out.squeeze().cpu().numpy().tolist())\n",
        "\n",
        "print(f'Predicted valuations: {predictions}')\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "\n",
        "# Convert predictions and targets to numpy arrays for evaluation\n",
        "actuals = test_data.y.numpy()\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "mse = mean_squared_error(actuals, predictions)\n",
        "mae = mean_absolute_error(actuals, predictions)\n",
        "rmse = mean_squared_error(actuals, predictions, squared=False)\n",
        "mape = mean_absolute_percentage_error(actuals, predictions)\n",
        "\n",
        "\n",
        "print(f'Mean Squared Error: {mse:.4f}')\n",
        "print(f'Mean Absolute Error: {mae:.4f}')\n",
        "print(f'Root Mean Squared Error (RMSE): {rmse:.4f}')\n",
        "print(f'Mean Absolute Percentage Error (MAPE): {mape:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2EcgA7Z1ZOC",
        "outputId": "2e5969f2-ce14-4996-eaad-dc9f7d7411c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted valuations: [17.00674057006836, 17.00674057006836, 15.634843826293945, 17.00674057006836, 9.443140983581543, 17.00674057006836, 15.76291561126709, 13.243549346923828, 11.65355396270752, 9.443140029907227, 15.92860221862793, 9.48320484161377, 9.498150825500488, 17.00674057006836, 13.279548645019531, 13.177889823913574, 17.00674057006836, 15.633118629455566, 17.00674057006836, 17.00674057006836, 15.416971206665039, 15.76291561126709, 13.249523162841797, 15.633118629455566, 15.484336853027344, 15.388965606689453, 17.00674057006836, 13.243524551391602, 17.00674057006836, 17.00674057006836, 15.600032806396484, 17.897661209106445, 17.00674057006836, 15.747207641601562, 9.483203887939453, 13.176414489746094, 15.705995559692383, 15.874368667602539, 11.660080909729004, 15.602745056152344, 17.00674057006836, 15.747207641601562, 15.634843826293945, 13.023650169372559, 11.601829528808594, 15.874368667602539, 15.874368667602539, 17.00674057006836, 15.705995559692383, 17.00674057006836, 15.008645057678223, 11.692177772521973, 11.558618545532227, 11.601829528808594, 15.00869369506836, 9.468652725219727, 16.835603713989258, 11.677864074707031, 13.236067771911621, 17.00674057006836, 19.964004516601562, 17.00674057006836, 15.92860221862793, 13.108743667602539, 17.00674057006836, 17.00674057006836, 15.634843826293945, 15.874368667602539, 17.00674057006836, 13.201260566711426, 17.00674057006836, 17.00674057006836, 15.701452255249023, 9.498150825500488, 17.00674057006836, 15.763065338134766, 17.00674057006836, 9.309823989868164, 13.138242721557617, 17.00674057006836, 9.550056457519531, 17.00674057006836, 16.835601806640625, 17.00674057006836, 16.967458724975586, 15.594595909118652, 15.874368667602539, 9.580439567565918, 9.309822082519531, 17.00674057006836, 13.24952507019043, 17.00674057006836, 11.700328826904297, 17.00674057006836, 11.660079002380371, 15.811363220214844, 15.634843826293945, 17.00674057006836, 17.00674057006836, 17.00674057006836, 15.605432510375977, 15.811363220214844, 15.76291561126709, 15.741310119628906, 15.613446235656738, 15.874368667602539, 15.417081832885742, 17.00674057006836, 15.705995559692383, 15.880102157592773, 9.4686279296875, 15.705995559692383, 15.633118629455566, 15.747207641601562, 15.76291561126709, 17.00674057006836, 11.653554916381836, 15.874368667602539, 15.874368667602539, 17.00674057006836, 9.498150825500488, 17.00674057006836, 17.2056827545166, 17.00674057006836, 17.00674057006836, 15.77456283569336, 17.205747604370117, 17.00674057006836, 15.874368667602539, 15.475539207458496, 17.00674057006836, 15.92860221862793, 15.747207641601562, 17.00674057006836, 15.701452255249023, 15.92860221862793, 9.443140029907227, 15.749494552612305, 15.763065338134766, 17.00674057006836, 15.763065338134766, 11.601829528808594, 17.00674057006836, 15.874368667602539, 15.594595909118652, 17.00674057006836, 15.874368667602539, 17.00674057006836, 16.63477897644043, 11.692197799682617, 11.695642471313477, 17.00674057006836, 13.236115455627441, 16.25920295715332, 11.671611785888672, 17.00674057006836, 15.811363220214844, 15.633118629455566, 15.388965606689453, 17.00674057006836, 15.774561882019043, 15.634843826293945, 15.602745056152344, 17.00674057006836, 17.00674057006836, 11.700328826904297, 15.742973327636719, 9.580439567565918, 9.468716621398926, 15.874368667602539, 17.00674057006836, 15.701452255249023, 15.811363220214844, 15.372138023376465, 15.484336853027344, 17.00674057006836, 17.00674057006836, 17.00674057006836, 15.763065338134766, 11.671611785888672, 15.613446235656738, 15.874368667602539, 15.634843826293945, 17.00674057006836, 15.008668899536133, 11.558618545532227, 17.00674057006836, 15.874368667602539, 15.811363220214844, 11.684556007385254, 17.00674057006836, 17.00674057006836, 15.634843826293945, 15.633118629455566, 15.811363220214844, 9.309823036193848, 15.874368667602539, 9.468616485595703, 17.00674057006836, 17.205678939819336, 19.96400260925293, 11.677862167358398, 17.00674057006836, 15.388965606689453, 13.236115455627441, 15.705995559692383, 15.735006332397461, 15.92860221862793, 13.167160034179688, 11.558618545532227, 15.811363220214844, 15.92860221862793, 15.874368667602539, 15.92860221862793, 11.68459701538086, 17.00674057006836, 17.00674057006836, 15.811363220214844, 16.835603713989258, 15.874368667602539, 13.186788558959961, 10.278148651123047, 15.92860221862793, 9.458077430725098, 15.705995559692383, 9.636053085327148, 15.737586975097656, 11.700328826904297, 11.700328826904297, 13.24952507019043, 15.76291561126709, 17.00674057006836, 15.735006332397461, 17.00674057006836, 15.701452255249023, 15.749494552612305, 11.782110214233398, 15.507665634155273, 15.874368667602539, 15.76291561126709, 13.201273918151855, 15.701452255249023, 14.486263275146484, 9.481830596923828, 15.51814079284668, 15.92860221862793, 15.701452255249023, 18.576183319091797, 15.92860221862793, 15.811363220214844, 15.74720573425293, 17.00674057006836, 17.00674057006836, 15.92860221862793, 15.874368667602539, 17.00674057006836, 15.735006332397461, 17.00674057006836, 17.2056827545166, 15.92860221862793, 17.00674057006836, 17.00674057006836, 15.634843826293945, 17.00674057006836, 15.701452255249023, 15.633118629455566, 17.00674057006836, 11.653555870056152, 15.76291561126709, 13.243528366088867, 13.02365493774414, 15.634843826293945, 16.634775161743164, 15.594595909118652, 17.00674057006836, 15.024956703186035, 17.00674057006836, 15.811363220214844, 13.023642539978027, 15.634843826293945, 17.897661209106445, 15.024887084960938, 17.00674057006836, 9.443140029907227, 15.76291561126709, 16.835601806640625, 17.00674057006836, 15.735006332397461, 15.735618591308594, 15.588261604309082, 11.65095329284668, 17.00674057006836, 14.171365737915039, 17.00674057006836, 17.00674057006836, 15.763065338134766, 15.74720573425293, 15.588261604309082, 11.700328826904297, 15.735618591308594, 15.735006332397461, 13.2435302734375, 9.580439567565918, 15.874368667602539, 15.735006332397461, 17.00674057006836, 15.484336853027344, 17.00674057006836, 14.751652717590332, 15.874368667602539, 11.558618545532227, 9.580439567565918, 15.874368667602539, 11.700328826904297, 17.00674057006836, 15.701452255249023, 15.633118629455566, 15.742974281311035, 17.00674057006836, 17.00674057006836, 15.92860221862793, 15.705995559692383, 15.874368667602539, 11.661354064941406, 15.763065338134766, 14.751652717590332, 17.00674057006836, 15.811363220214844, 15.92860221862793, 17.00674057006836, 17.00674057006836, 13.138252258300781, 15.00864028930664, 15.92860221862793, 17.2056827545166, 14.979554176330566, 15.92860221862793, 13.236069679260254, 15.92860221862793, 17.00674057006836, 15.603612899780273, 15.874368667602539, 13.236114501953125, 9.309820175170898, 15.736257553100586, 17.20568084716797, 15.92860221862793, 9.392146110534668, 13.167160034179688, 15.92860221862793, 17.00674057006836, 15.735006332397461, 15.92860221862793, 12.926200866699219, 16.63477897644043, 15.701452255249023, 17.00674057006836, 17.00674057006836, 14.751652717590332, 17.00674057006836, 15.613434791564941, 15.811363220214844, 11.68455696105957, 15.597330093383789, 15.874368667602539, 15.705995559692383, 15.874368667602539, 13.167160034179688, 15.74130916595459, 15.736257553100586, 15.74720573425293, 15.588261604309082, 15.92860221862793, 17.00674057006836, 15.92860221862793, 17.00674057006836, 15.749494552612305, 15.705995559692383, 12.926193237304688, 15.811363220214844, 15.763065338134766, 17.00674057006836, 17.00674057006836, 17.2056827545166, 17.00674057006836, 15.874368667602539, 15.633118629455566, 15.811363220214844, 15.874368667602539, 11.653555870056152, 9.48183822631836, 17.00674057006836, 17.00674057006836, 17.00674057006836, 15.735006332397461, 15.74130916595459, 15.73819351196289, 11.671689987182617, 11.677862167358398, 17.00674057006836, 15.92860221862793, 15.634843826293945, 13.186790466308594, 15.742974281311035, 11.671690940856934, 15.372138023376465, 15.92860221862793, 17.00674057006836, 15.763065338134766, 9.48185920715332, 15.874368667602539, 9.309820175170898, 15.76291561126709, 17.00674057006836, 13.236067771911621, 17.00674057006836, 15.92860221862793, 13.023640632629395, 15.594595909118652, 13.243528366088867, 15.475554466247559, 15.735006332397461, 9.468728065490723, 9.580440521240234, 15.705995559692383, 15.76291561126709, 11.601829528808594, 17.00674057006836, 15.811363220214844, 15.633118629455566, 17.00674057006836, 15.705995559692383, 15.736258506774902, 17.12236785888672, 15.742975234985352, 17.00674057006836, 9.468646049499512, 14.751652717590332, 17.00674057006836, 13.249523162841797, 17.00674057006836, 15.749494552612305, 11.700328826904297, 15.92860221862793, 11.700328826904297, 17.00674057006836, 15.76291561126709, 11.700328826904297, 17.00674057006836, 13.236080169677734, 11.700328826904297, 15.76291561126709, 15.737586975097656, 17.00674057006836, 15.874368667602539, 9.443828582763672, 17.00674057006836, 15.603612899780273, 15.484336853027344, 19.964012145996094, 17.00674057006836, 17.00674057006836, 15.811363220214844, 17.00674057006836, 15.749494552612305, 15.735006332397461, 17.00674057006836, 17.00674057006836, 9.580440521240234, 18.57618522644043, 17.00674057006836, 8.717011451721191, 17.00674057006836, 17.00674057006836, 17.00674057006836, 15.51814079284668, 14.979554176330566, 15.483299255371094, 15.74130916595459, 13.176443099975586, 15.613446235656738, 15.701452255249023, 15.633118629455566, 9.483213424682617, 9.44314193725586, 13.197525024414062, 17.897663116455078, 15.562685012817383, 15.811363220214844, 9.468713760375977, 15.588261604309082, 15.588261604309082, 15.735006332397461, 15.613434791564941, 17.00674057006836, 17.00674057006836, 15.92860221862793, 15.92860221862793, 19.964008331298828, 16.352689743041992, 15.735006332397461, 14.410896301269531, 15.507698059082031, 9.443138122558594, 15.92860221862793, 17.00674057006836, 15.619678497314453, 16.25920295715332, 9.580440521240234, 15.613434791564941, 17.00674057006836, 15.60543155670166, 15.742975234985352, 15.763065338134766, 17.00674057006836, 17.00674057006836, 15.811363220214844, 15.874368667602539, 17.00674057006836, 15.92860221862793, 11.60183048248291, 15.811363220214844, 17.00674057006836, 9.468624114990234, 17.00674057006836, 15.483299255371094, 17.00674057006836, 9.524507522583008, 15.742975234985352, 15.874368667602539, 15.483297348022461, 17.2056827545166, 15.92860221862793, 15.74130916595459, 15.76291561126709, 17.00674057006836, 15.613446235656738, 15.76291561126709, 11.700328826904297, 15.92860221862793, 11.684555053710938, 17.897661209106445, 15.874368667602539, 11.700328826904297, 11.700328826904297, 15.735006332397461, 17.00674057006836, 15.763065338134766, 11.660080909729004, 15.874368667602539, 15.416997909545898, 15.76291561126709, 17.00674057006836, 17.00674057006836, 13.167160034179688, 17.00674057006836, 17.00674057006836, 15.588261604309082, 15.597330093383789, 12.991905212402344, 15.705995559692383, 17.00674057006836, 15.024913787841797, 17.00674057006836, 17.00674057006836, 9.443827629089355, 17.00674057006836, 17.00674057006836, 15.92860221862793, 9.662933349609375, 17.00674057006836, 17.00674057006836, 11.660080909729004, 15.562812805175781, 15.705995559692383, 11.653555870056152, 15.92860221862793, 17.00674057006836, 11.65095329284668, 17.00674057006836, 17.00674057006836, 15.705995559692383, 17.00674057006836, 15.749494552612305, 17.00674057006836, 17.00674057006836, 14.751652717590332, 15.874368667602539, 14.979554176330566, 13.207568168640137, 15.633118629455566, 17.00674057006836, 15.701452255249023, 15.76291561126709, 11.342416763305664, 17.00674057006836, 17.00674057006836, 15.763065338134766, 15.701452255249023, 15.735006332397461, 15.585440635681152, 15.92860221862793, 13.224557876586914, 18.57618522644043, 9.468765258789062, 15.735006332397461, 15.484336853027344, 15.811363220214844, 17.00674057006836, 13.167160034179688, 15.705995559692383, 15.811363220214844, 15.874368667602539, 15.92860221862793, 11.700328826904297, 13.236074447631836, 15.92860221862793, 15.92860221862793, 15.588261604309082, 15.742974281311035, 17.00674057006836, 9.580440521240234, 9.498150825500488, 15.736257553100586, 13.236122131347656, 11.671688079833984, 15.874368667602539, 17.00674057006836, 13.177889823913574, 15.613434791564941, 13.23607063293457, 15.874368667602539, 9.498150825500488, 15.763065338134766, 11.66135311126709, 11.558618545532227, 13.177889823913574, 15.874368667602539, 13.177889823913574, 15.633118629455566, 17.00674057006836, 15.749493598937988, 9.309819221496582, 11.677820205688477, 17.00674057006836, 17.20568084716797, 11.653555870056152, 15.518141746520996, 15.518141746520996, 13.249528884887695, 15.600032806396484, 15.874368667602539, 9.443151473999023, 17.00674057006836, 17.00674057006836, 13.176441192626953, 14.979554176330566, 15.92860221862793, 15.92860221862793, 11.700328826904297, 15.74130916595459, 15.92860221862793, 15.92860221862793, 15.92860221862793, 11.677820205688477, 17.00674057006836, 13.27955436706543, 15.633118629455566, 15.811363220214844, 15.475539207458496, 15.763065338134766, 15.92860221862793, 15.518141746520996, 17.00674057006836, 17.00674057006836, 17.00674057006836, 15.874368667602539, 17.00674057006836, 15.735620498657227, 15.92860221862793, 17.00674057006836, 15.92860221862793, 17.00674057006836, 17.00674057006836, 15.76291561126709, 15.588261604309082, 15.76291561126709, 15.705995559692383, 15.763065338134766, 13.249521255493164, 15.92860221862793, 15.811363220214844, 19.964019775390625, 11.653555870056152, 17.00674057006836, 15.811363220214844, 15.7549467086792, 15.594595909118652, 13.224535942077637, 15.56279468536377, 15.874368667602539, 9.443140029907227, 17.00674057006836, 17.2056827545166, 15.74130916595459, 17.00674057006836, 17.00674057006836, 15.701452255249023, 15.736258506774902, 11.558618545532227, 17.00674057006836, 15.705995559692383, 13.20140266418457, 15.874368667602539, 15.50761604309082, 15.634844779968262, 15.296195030212402, 15.92860221862793, 17.00674057006836, 16.529769897460938, 17.00674057006836, 15.874368667602539, 15.705995559692383, 15.874368667602539, 9.52454662322998, 15.92860221862793, 15.92860221862793, 15.763065338134766, 15.271015167236328, 17.205684661865234, 9.309822082519531, 13.138240814208984, 15.763065338134766, 17.00674057006836, 17.00674057006836, 17.00674057006836, 17.00674057006836, 15.749493598937988, 11.224679946899414, 17.00674057006836, 17.00674057006836, 9.458074569702148, 15.60543155670166, 15.562812805175781, 15.735006332397461, 15.701452255249023, 17.00674057006836, 17.00674057006836, 9.309821128845215, 17.00674057006836, 13.246078491210938, 15.76291561126709, 9.469289779663086, 15.92860221862793, 17.00674057006836, 15.811363220214844, 15.701452255249023, 9.309821128845215, 15.735620498657227, 15.92860221862793, 19.964006423950195, 13.24354076385498, 15.92860221862793, 17.00674057006836, 9.580440521240234, 15.737587928771973, 15.388965606689453, 17.00674057006836, 15.633118629455566, 15.701452255249023, 15.484336853027344, 9.443143844604492, 15.811363220214844, 17.00674057006836, 15.92860221862793, 13.279550552368164, 17.00674057006836, 11.684596061706543, 17.00674057006836, 15.701452255249023, 15.811363220214844, 15.736257553100586, 15.634844779968262, 17.00674057006836, 15.92860221862793, 11.558616638183594, 15.92860221862793, 15.47735595703125, 17.00674057006836, 19.963998794555664, 9.469282150268555, 17.00674057006836, 15.613446235656738, 17.00674057006836, 15.874368667602539, 15.633118629455566, 15.92860221862793, 15.742974281311035, 11.558618545532227, 9.524499893188477, 15.92860221862793, 11.700328826904297, 15.518141746520996, 15.811361312866211, 15.92860221862793, 15.874368667602539, 9.469883918762207, 15.92860221862793, 11.55861759185791, 17.00674057006836, 15.92860221862793, 15.735006332397461, 17.00674057006836, 15.92860221862793, 17.00674057006836, 15.874368667602539, 15.749494552612305, 17.00674057006836, 13.243532180786133, 17.00674057006836, 15.74720573425293, 15.484336853027344, 11.845086097717285, 9.481948852539062, 9.458069801330566, 11.695642471313477, 15.74720573425293, 13.167160034179688, 15.92860221862793, 11.671610832214355, 15.60543155670166, 17.00674057006836, 15.92860221862793, 15.701452255249023, 16.535499572753906, 14.199304580688477, 17.00674057006836, 15.735006332397461, 15.701452255249023, 17.00674057006836, 11.601829528808594, 13.177889823913574, 15.701452255249023, 15.388965606689453, 17.00674057006836, 15.633118629455566, 15.811361312866211, 15.742974281311035, 15.92860221862793, 17.00674057006836, 17.00674057006836, 15.735006332397461, 17.00674057006836, 17.00674057006836, 11.700328826904297, 17.00674057006836, 17.00674057006836, 15.705995559692383, 13.249519348144531, 15.588261604309082, 14.751652717590332, 15.76291561126709, 11.60183048248291, 15.594595909118652, 17.00674057006836, 15.701452255249023, 15.874368667602539, 15.74720573425293, 15.749494552612305, 15.633118629455566, 15.874368667602539, 15.41700553894043, 15.701452255249023, 15.588261604309082, 15.749494552612305, 15.74720573425293, 17.00674057006836, 15.749494552612305, 15.594595909118652, 11.224679946899414, 15.633118629455566, 9.46989631652832, 11.660079956054688, 17.00674057006836, 15.613446235656738, 15.633118629455566, 15.874368667602539, 16.835603713989258, 15.735006332397461, 15.92860221862793, 15.76291561126709, 17.2056827545166, 15.613434791564941, 15.874368667602539, 11.660080909729004, 15.613434791564941, 15.588261604309082, 15.735006332397461, 11.671611785888672, 11.692178726196289, 16.529773712158203, 17.205684661865234, 17.00674057006836, 17.00674057006836, 15.024913787841797, 15.024887084960938, 15.701452255249023, 15.811361312866211, 17.00674057006836, 15.735006332397461, 15.811361312866211, 16.67470359802246, 11.661349296569824, 17.00674057006836, 15.613434791564941, 11.700328826904297, 11.65095329284668, 17.00674057006836, 15.763065338134766, 15.588261604309082, 17.00674057006836, 15.701452255249023, 17.00674057006836, 15.749494552612305, 15.530898094177246, 17.00674057006836, 15.737587928771973, 15.417070388793945, 13.186792373657227, 11.700328826904297, 17.00674057006836, 15.92860221862793, 17.00674057006836, 17.00674057006836, 15.749494552612305, 15.588261604309082, 15.417064666748047, 13.246075630187988, 17.00674057006836, 17.00674057006836, 9.468799591064453, 15.88016128540039, 13.176412582397461, 17.00674057006836, 15.634844779968262, 15.92860221862793, 17.00674057006836, 17.00674057006836, 15.633118629455566, 15.633118629455566, 15.597330093383789, 15.735006332397461, 9.44383716583252, 17.00674057006836, 15.737587928771973, 9.443140029907227, 15.92860221862793, 15.92860221862793, 17.00674057006836, 15.92860221862793, 13.186458587646484, 15.70599365234375, 15.763065338134766, 17.00674057006836, 11.700328826904297, 9.468681335449219, 15.613446235656738, 15.619678497314453, 15.70599365234375, 11.601827621459961, 15.74720573425293, 17.00674057006836, 13.023645401000977, 17.00674057006836, 11.700328826904297, 11.67781925201416, 11.217580795288086, 15.588261604309082, 16.25920295715332, 15.701452255249023, 15.74130916595459, 9.443146705627441, 15.008652687072754, 15.600031852722168, 11.558618545532227, 15.388965606689453, 15.763065338134766, 11.692181587219238, 15.811361312866211, 11.6716890335083, 15.619678497314453, 15.92860221862793, 15.811361312866211, 9.483205795288086, 17.00674057006836, 15.594595909118652, 17.897661209106445, 17.00674057006836, 15.701452255249023, 13.279544830322266, 17.00674057006836, 17.205684661865234, 17.00674057006836, 15.811361312866211, 16.63477897644043, 11.700328826904297, 15.388965606689453, 11.684596061706543, 17.00674057006836, 15.76291561126709, 15.77456283569336, 11.224679946899414, 17.00674057006836, 15.874368667602539, 15.811361312866211, 11.10782527923584, 15.613446235656738, 15.874368667602539, 17.00674057006836, 17.00674057006836, 15.701452255249023, 17.00674057006836, 9.443138122558594, 17.00674057006836, 17.79448699951172, 15.92860221862793, 9.392147064208984, 17.00674057006836, 15.811361312866211, 17.00674057006836, 18.576183319091797, 17.00674057006836, 17.00674057006836, 15.749494552612305, 15.701452255249023, 15.308034896850586, 15.92860221862793, 11.558618545532227, 15.613434791564941, 17.00674057006836, 11.692193984985352, 15.76291561126709, 15.874368667602539, 15.008674621582031, 9.309823989868164, 15.811361312866211, 15.874368667602539, 15.811361312866211, 17.00674057006836, 15.92860221862793, 15.874368667602539, 15.92860221862793, 11.65095329284668, 15.735006332397461, 15.735006332397461, 9.392148971557617, 17.00674057006836, 12.32325553894043, 15.874368667602539, 17.00674057006836, 15.594595909118652, 15.603612899780273, 11.558618545532227, 15.74130916595459, 15.74720573425293, 15.763065338134766, 15.024904251098633, 9.661970138549805, 15.633118629455566, 15.76291561126709, 17.00674057006836, 15.811361312866211, 15.811361312866211, 15.70599365234375, 11.671609878540039, 15.811361312866211, 15.701452255249023, 15.92860221862793, 17.00674057006836, 16.25920295715332, 15.633118629455566, 9.309825897216797, 15.633118629455566, 15.811361312866211, 17.00674057006836, 11.60183048248291, 17.00674057006836, 15.763065338134766, 15.74720573425293, 11.558618545532227, 15.92860221862793, 13.279549598693848, 15.701452255249023, 15.701452255249023, 9.443140029907227, 15.874368667602539, 8.979388236999512, 15.74720573425293, 11.650954246520996, 9.580439567565918, 13.19752025604248, 15.763065338134766, 15.763065338134766, 15.874368667602539, 17.00674057006836, 11.65095329284668, 15.874368667602539, 15.874368667602539, 15.92860221862793, 15.874368667602539, 15.763065338134766, 11.653555870056152, 17.897661209106445, 15.874368667602539, 15.92860221862793, 13.249517440795898, 15.735618591308594, 17.00674057006836, 15.588261604309082, 17.00674057006836, 15.92860221862793, 19.964017868041992, 9.309820175170898, 17.897661209106445, 17.00674057006836, 17.00674057006836, 13.236115455627441, 15.475537300109863, 15.811361312866211, 9.443140029907227, 17.00674057006836, 15.92860221862793, 17.00674057006836, 15.749494552612305, 16.835601806640625, 15.588261604309082, 15.92860221862793, 15.811361312866211, 15.749494552612305, 17.00674057006836, 17.00674057006836, 19.964008331298828, 15.763065338134766, 15.811361312866211, 15.634844779968262, 17.00674057006836, 17.00674057006836, 15.735006332397461, 15.92860221862793, 15.874368667602539, 15.874368667602539, 17.00674057006836, 17.00674057006836, 17.00674057006836, 11.692179679870605, 9.443826675415039, 15.701452255249023, 9.44314193725586, 15.811363220214844, 15.633118629455566, 11.66135311126709, 15.70599365234375, 17.00674057006836, 19.964004516601562, 9.44314193725586, 17.00674057006836, 17.00674057006836, 9.443838119506836, 15.416987419128418, 9.458074569702148, 11.558614730834961, 15.762916564941406, 15.741310119628906, 15.811363220214844, 9.468717575073242, 15.811363220214844, 13.243528366088867, 9.469892501831055, 15.92860221862793, 10.545327186584473, 13.236124038696289, 17.00674057006836, 11.661354064941406, 17.00674057006836, 15.701452255249023, 17.00674057006836, 17.00674057006836, 17.00674057006836, 16.835603713989258, 11.601829528808594, 15.701452255249023, 15.634843826293945, 11.700328826904297, 13.236066818237305, 15.874368667602539, 15.633118629455566, 15.128351211547852, 15.74720573425293, 15.74720573425293, 15.92860221862793, 11.558618545532227, 9.55019760131836, 9.498149871826172, 15.735006332397461, 13.224531173706055, 15.701452255249023, 11.700328826904297, 15.749494552612305, 15.024925231933594, 15.742975234985352, 15.388965606689453, 9.443138122558594, 15.92860221862793, 17.00674057006836, 17.00674057006836, 15.705992698669434, 11.700328826904297, 11.55861759185791, 17.00674057006836, 15.874368667602539, 15.634843826293945, 15.742975234985352, 17.00674057006836, 15.634843826293945, 17.00674057006836, 13.167160034179688, 11.700328826904297, 17.00674057006836, 17.00674057006836, 17.00674057006836, 17.00674057006836, 17.00674057006836, 17.00674057006836, 15.763065338134766, 13.167160034179688, 17.00674057006836, 13.186454772949219, 13.249519348144531, 15.874368667602539, 11.700328826904297, 13.197521209716797, 17.00674057006836, 9.468745231628418, 14.485234260559082, 9.481821060180664, 9.309822082519531, 13.249521255493164, 11.601829528808594, 11.65095329284668, 11.661351203918457, 15.588261604309082, 15.742975234985352, 17.00674057006836, 17.00674057006836, 15.483257293701172, 17.00674057006836, 15.735618591308594, 11.700328826904297, 15.602745056152344, 18.576183319091797, 15.92860221862793, 15.92860221862793, 11.601829528808594, 17.00674057006836, 17.00674057006836, 15.763065338134766, 15.874368667602539, 15.597330093383789, 11.650955200195312, 17.00674057006836, 11.601829528808594, 17.00674057006836, 17.00674057006836, 13.246072769165039, 15.701452255249023, 17.79448699951172, 17.00674057006836, 11.601829528808594, 15.588261604309082, 19.964004516601562, 13.186452865600586, 15.92860221862793, 15.811363220214844, 17.00674057006836, 17.00674057006836, 17.00674057006836, 17.00674057006836, 15.763065338134766, 16.63477897644043, 15.605432510375977, 15.762916564941406, 17.00674057006836, 11.661352157592773, 17.897661209106445, 9.392154693603516, 17.00674057006836, 9.443142890930176, 15.469447135925293, 11.700328826904297, 17.00674057006836, 15.92860221862793, 15.811363220214844, 17.00674057006836, 16.634777069091797, 15.92860221862793, 17.00674057006836, 17.00674057006836, 11.558618545532227, 17.00674057006836, 9.443140029907227, 13.065105438232422, 11.60183048248291, 9.498150825500488, 15.633118629455566, 15.701452255249023, 13.023639678955078, 15.762916564941406, 15.874368667602539, 15.874368667602539, 15.594595909118652, 17.00674057006836, 15.024967193603516, 13.249520301818848, 13.236116409301758, 17.00674057006836, 17.00674057006836, 15.633118629455566, 15.811363220214844, 16.83559799194336, 16.529769897460938, 17.00674057006836, 13.201393127441406, 15.735006332397461, 15.70599365234375, 15.92860221862793, 15.880189895629883, 13.24952220916748, 15.92860221862793, 15.633118629455566, 17.00674057006836, 17.00674057006836, 16.529769897460938, 15.417015075683594, 14.415735244750977, 11.677864074707031, 17.00674057006836, 14.979554176330566, 11.660080909729004, 15.874368667602539, 17.00674057006836, 15.634843826293945, 13.138254165649414, 15.762916564941406, 15.308032989501953, 11.671689987182617, 17.00674057006836, 15.874368667602539, 15.701452255249023, 15.248085975646973, 17.205684661865234, 15.811363220214844, 13.197525024414062, 15.763065338134766, 15.92860221862793, 17.00674057006836, 17.00674057006836, 15.749494552612305, 15.762916564941406, 13.065106391906738, 17.00674057006836, 17.00674057006836, 17.00674057006836, 15.633118629455566, 9.309819221496582, 15.811363220214844, 11.700328826904297, 11.601826667785645, 15.92860221862793, 11.601829528808594, 17.00674057006836, 9.443140029907227, 17.00674057006836, 17.00674057006836, 11.677818298339844, 17.00674057006836, 9.309820175170898, 17.00674057006836, 15.874368667602539, 15.874368667602539, 17.00674057006836, 15.602745056152344, 15.128350257873535, 9.580439567565918, 17.00674057006836, 17.00674057006836, 13.583412170410156, 9.469290733337402, 15.880163192749023, 15.92860221862793, 11.558618545532227, 11.65095329284668, 13.177889823913574, 15.701452255249023, 15.874368667602539, 15.874368667602539, 17.00674057006836, 15.701452255249023, 17.00674057006836, 9.392166137695312, 15.763065338134766, 14.979554176330566, 15.588261604309082, 13.246073722839355, 17.00674057006836, 15.634843826293945, 17.00674057006836, 15.811363220214844, 15.749494552612305, 15.762916564941406, 17.00674057006836, 17.006742477416992, 15.874367713928223]\n",
            "Mean Squared Error: 101.9430\n",
            "Mean Absolute Error: 6.4772\n",
            "Root Mean Squared Error (RMSE): 10.0967\n",
            "Mean Absolute Percentage Error (MAPE): 0.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6 Evaluation Of The GNN Model\n",
        "----------------------------\n",
        "Despite training the model with better edges, hypertuning the parameters, the graph neural network appears to perform only slightly better than the baseline model but still is not satisfactory for the project's purpose of identifying the IPO Prices based on the factors such as revenue, owner's equity and total assets of the company.\n",
        "\n",
        "Next, we shall explore if MLP is better than the Graph Neural Network in defining the IPO_Price."
      ],
      "metadata": {
        "id": "gv7eUNnGopYF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7 Multilayer Perceptron\n",
        "----------------------------\n",
        "Building the Multilayer Perceptron from a simple Multilayer Perceptron and explore the network deeper to see if building a deeper network will better fit the purpose of determining the IPO_Price on the company based on the factors such as revenue, owner's equity and total assets of the company."
      ],
      "metadata": {
        "id": "h8z_bZJXfhPp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7.1 Initalizing the Training/Test Set\n",
        "In the following codes below, we initalise the training set and maintain the hold out set for later purposes so that we can split the training set to validation and training set again."
      ],
      "metadata": {
        "id": "6DXs7FM8gceG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(df2, test_size=0.2, random_state=42)\n",
        "\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "## Training Set\n",
        "train_dfx = torch.tensor(train_df[['Sector_encoded', 'Industry_encoded','Shares_Offered_encoded','Total_Revenue_encoded','Total_Asset_encoded']].values, dtype=torch.float)\n",
        "train_dfy = torch.tensor(train_df['IPO_Price'].values, dtype=torch.long)\n",
        "\n",
        "\n",
        "## Test Set Hold Out Set\n",
        "test_dfx = torch.tensor(test_df[['Sector_encoded', 'Industry_encoded','Shares_Offered_encoded','Total_Revenue_encoded','Total_Asset_encoded']].values, dtype=torch.float)\n",
        "test_dfy = torch.tensor(test_df['IPO_Price'].values, dtype=torch.long)\n",
        "\n",
        "FinalTrainComparisonDataset = TensorDataset(train_dfx,train_dfy)\n",
        "FinalTrainComparisonloader = DataLoader(FinalTrainComparisonDataset,batch_size=32,shuffle=True)\n",
        "\n",
        "testDataset = TensorDataset(test_dfx,test_dfy)\n",
        "testloader = DataLoader(testDataset,batch_size=32,shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ds66rX5WSMd",
        "outputId": "d36a3f1f-7e70-4e6c-b7e5-76bc6a6f365f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7.2 Building the First MLP Regression Model\n",
        "------------------------------------------------\n",
        "In the first model, we have two layers of linear and the last layer which takes all the neurons into a single output and a relu activation function after each layer."
      ],
      "metadata": {
        "id": "k2RmwT-3goTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPRegressionModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim,dropout_rate=0.5):\n",
        "        super(MLPRegressionModel, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim,hidden_dim//2)\n",
        "        self.fcLast = torch.nn.Linear(hidden_dim//2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        x = self.fcLast(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "DxY-wQxRtAoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####7.2.1 Training the Model"
      ],
      "metadata": {
        "id": "CAahhzUNg5zI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = train_dfx.shape[1]\n",
        "print(input_dim)\n",
        "hidden_dim = 128\n",
        "output_dim = 1\n",
        "\n",
        "model = MLPRegressionModel(input_dim, hidden_dim, output_dim)\n",
        "criterion = torch.nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs =500  # Number of epochs for training\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    epoch_loss = 0  # Initialize loss for this epoch\n",
        "    total_samples = 0\n",
        "    for inputs, targets in FinalTrainComparisonloader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        outputs = model(inputs)  # Forward pass: compute predictions\n",
        "        loss = criterion(outputs, targets.view(-1, 1).float())  # Compute loss\n",
        "\n",
        "        loss.backward()  # Backward pass: compute gradients\n",
        "        optimizer.step()  # Update model weights\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        epoch_loss += loss.item()*inputs.size(0)  # Accumulate loss for this epoch\n",
        "        total_samples += inputs.size(0)\n",
        "    # Print average loss for the epoch\n",
        "    avg_loss = epoch_loss/total_samples\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n"
      ],
      "metadata": {
        "id": "sAgYtHR7M6MH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.2.2 Evaluation Of the Initial Model\n",
        "The model did better than the baseline model and the final Graph Neural Network Model as seen from the metrics such as the mean squared error,\n",
        "\n",
        "From the model below, the results from the evaluation are as follows:\n",
        "\n",
        "\n",
        "MAE:5.66\n",
        "\n",
        "MSE:96.91\n",
        "\n",
        "RMSE:9.84\n",
        "\n",
        "MAPE:0.3742\n",
        "\n",
        "Baseline Model's Results:\n",
        "\n",
        "MAE:6.40\n",
        "\n",
        "MSE:84.45\n",
        "\n",
        "RMSE:9.18\n",
        "\n",
        "MAPE:0.4531\n"
      ],
      "metadata": {
        "id": "UDGVzrO7g95L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Initialize variables to accumulate loss\n",
        "total_loss = 0\n",
        "n_batches = 0\n",
        "\n",
        "all_actuals = []\n",
        "all_predictions = []\n",
        "all_differences = []\n",
        "\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    for inputs, targets in testloader:\n",
        "        # Forward pass: Compute predicted outputs by passing inputs to the model\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets.view(-1, 1))\n",
        "        # Compute the loss\n",
        "        differences = targets.view(-1,1) - outputs\n",
        "\n",
        "        all_actuals.extend(targets.detach().cpu().numpy())\n",
        "        all_predictions.extend(outputs.detach().cpu().numpy())\n",
        "        all_differences.extend(differences.detach().cpu().numpy())\n",
        "        # Accumulate loss\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "\n",
        "all_actuals_tensor = torch.tensor(all_actuals, dtype=torch.float32)\n",
        "all_predictions_tensor = torch.tensor(all_predictions, dtype=torch.float32)\n",
        "\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "mse = mse_loss(all_predictions_tensor, all_actuals_tensor.view(-1, 1))\n",
        "print(f'Mean Squared Error (MSE) using nn.MSELoss: {mse.item():.4f}')\n",
        "\n",
        "mae_loss = torch.nn.L1Loss()\n",
        "mae=mae_loss(all_predictions_tensor,all_actuals_tensor.view(-1,1))\n",
        "print(f'Mean Absolute Error (MAE) using nn.L1Loss: {mae.item():.4f}')\n",
        "\n",
        "\n",
        "rmse = mean_squared_error(all_predictions_tensor,all_actuals_tensor.view(-1,1), squared=False)\n",
        "print(f'Root Mean Squared Error (rmse): {rmse.item():.4f}')\n",
        "\n",
        "\n",
        "\n",
        "mape = mean_absolute_percentage_error(all_predictions_tensor,all_actuals_tensor.view(-1,1))\n",
        "print(f'Mean Absolute Percentage Error (MAE): {mape.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73900fe7-13e5-47d9-ca4e-c84f3ca95215",
        "id": "l16Y29NfkjEU"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE) using nn.MSELoss: 96.9109\n",
            "Mean Absolute Error (MAE) using nn.L1Loss: 5.6575\n",
            "Root Mean Squared Error (rmse): 9.8443\n",
            "Mean Absolute Percentage Error (MAE): 0.3742\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Building A Deeper Network\n",
        "---------------------------------------\n",
        "For this network, we shall include few more layers without normalizing and dropping out. The network is as follows:\n",
        "\n",
        "X --> hidden layer --> Relu --> Hidden layer --> Relu --> Hidden layer --> Relu --> Hidden Layer --> Relu --> Output\n",
        "\n",
        "\n",
        "\n",
        "In this model, there are four hidden layers and one final layer to convert the input neurons into a single output.\n"
      ],
      "metadata": {
        "id": "afWXV97p5cqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPRegressionModel2(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim,dropout_rate=0.5):\n",
        "        super(MLPRegressionModel2, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim//2)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(hidden_dim//2)\n",
        "        self.fc3 = torch.nn.Linear(hidden_dim//2, hidden_dim//4)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(hidden_dim//4)\n",
        "        self.fc4 = torch.nn.Linear(hidden_dim//4, hidden_dim//8)\n",
        "        self.bn4 = torch.nn.BatchNorm1d(hidden_dim//8)\n",
        "        self.fcLast = torch.nn.Linear(hidden_dim//8, output_dim)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "\n",
        "        x = F.relu(self.fc3(x))\n",
        "\n",
        "\n",
        "        x = F.relu(self.fc4(x))\n",
        "\n",
        "\n",
        "        x = self.fcLast(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "9Paego-1ZDwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####7.3.1 Training The Second MLP Model\n",
        "-------------------------------------------\n",
        "Below is the code to train the MLP model with the training set."
      ],
      "metadata": {
        "id": "hKClAuF7HuJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = train_dfx.shape[1]\n",
        "print(input_dim)\n",
        "hidden_dim = 128\n",
        "output_dim = 1\n",
        "\n",
        "model2 = MLPRegressionModel2(input_dim, hidden_dim, output_dim)\n",
        "criterion = torch.nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs =500  # Number of epochs for training\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model2.train()  # Set the model to training mode\n",
        "    epoch_loss = 0  # Initialize loss for this epoch\n",
        "    total_samples = 0\n",
        "    for inputs, targets in FinalTrainComparisonloader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        outputs = model2(inputs)  # Forward pass: compute predictions\n",
        "        loss = criterion(outputs, targets.view(-1, 1).float())  # Compute loss\n",
        "\n",
        "        loss.backward()  # Backward pass: compute gradients\n",
        "        optimizer.step()  # Update model weights\n",
        "        torch.nn.utils.clip_grad_norm_(model2.parameters(), max_norm=1.0)\n",
        "        epoch_loss += loss.item()*inputs.size(0)  # Accumulate loss for this epoch\n",
        "        total_samples += inputs.size(0)\n",
        "    # Print average loss for the epoch\n",
        "    avg_loss = epoch_loss/total_samples\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "    model2.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    total_val_samples = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "wMw_xkP3EUdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####7.3.2 Evaluation Of the Second MLP Model\n",
        "\n",
        "The model did better than the baseline model and the final Graph Neural Network Model but worst than the initial simple MLP model as seen from the metrics such as the mean squared error,\n",
        "\n",
        "From the model below, the results from the evaluation are as follows:\n",
        "\n",
        "MAE:5.73\n",
        "\n",
        "MSE:80.4365\n",
        "\n",
        "RMSE:8.9686\n",
        "\n",
        "MAPE:0.3373\n",
        "\n",
        "\n",
        "Initial MLP Model's Results:\n",
        "\n",
        "MAE:5.66\n",
        "\n",
        "MSE:96.91\n",
        "\n",
        "RMSE:9.84\n",
        "\n",
        "MAPE:0.3742\n",
        "\n",
        "Baseline Model's Results:\n",
        "\n",
        "MAE:6.40\n",
        "\n",
        "MSE:84.45\n",
        "\n",
        "RMSE:9.18\n",
        "\n",
        "MAPE:0.4531"
      ],
      "metadata": {
        "id": "DuajGy-EIKeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model2.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Initialize variables to accumulate loss\n",
        "total_loss = 0\n",
        "n_batches = 0\n",
        "\n",
        "all_actuals = []\n",
        "all_predictions = []\n",
        "all_differences = []\n",
        "\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    for inputs, targets in testloader:\n",
        "        # Forward pass: Compute predicted outputs by passing inputs to the model\n",
        "        outputs = model2(inputs)\n",
        "        loss = criterion(outputs, targets.view(-1, 1))\n",
        "        # Compute the loss\n",
        "        differences = targets.view(-1,1) - outputs\n",
        "\n",
        "        all_actuals.extend(targets.detach().cpu().numpy())\n",
        "        all_predictions.extend(outputs.detach().cpu().numpy())\n",
        "        all_differences.extend(differences.detach().cpu().numpy())\n",
        "        # Accumulate loss\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "\n",
        "all_actuals_tensor = torch.tensor(all_actuals, dtype=torch.float32)\n",
        "all_predictions_tensor = torch.tensor(all_predictions, dtype=torch.float32)\n",
        "\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "mse = mse_loss(all_predictions_tensor, all_actuals_tensor.view(-1, 1))\n",
        "print(f'Mean Squared Error (MSE) using nn.MSELoss: {mse.item():.4f}')\n",
        "\n",
        "mae_loss = torch.nn.L1Loss()\n",
        "mae=mae_loss(all_predictions_tensor,all_actuals_tensor.view(-1,1))\n",
        "print(f'Mean Absolute Error (MAE) using nn.L1Loss: {mae.item():.4f}')\n",
        "\n",
        "\n",
        "rmse = mean_squared_error(all_predictions_tensor,all_actuals_tensor.view(-1,1), squared=False)\n",
        "print(f'Root Mean Squared Error (rmse): {rmse.item():.4f}')\n",
        "\n",
        "\n",
        "\n",
        "mape = mean_absolute_percentage_error(all_predictions_tensor,all_actuals_tensor.view(-1,1))\n",
        "print(f'Mean Absolute Percentage Error (MAE): {mape.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3fa2729-243b-47cf-9134-0f1cd4e12e3e",
        "id": "VdHeORoBEZMC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE) using nn.MSELoss: 80.4365\n",
            "Mean Absolute Error (MAE) using nn.L1Loss: 5.7308\n",
            "Root Mean Squared Error (rmse): 8.9686\n",
            "Mean Absolute Percentage Error (MAE): 0.3373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####7.3.3 K-Fold Cross Validation"
      ],
      "metadata": {
        "id": "w8PAPW6FIUJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "criterion = torch.nn.L1Loss()\n",
        "\n",
        "all_training_losses=[]\n",
        "all_val_losses =[]\n",
        "\n",
        "##MLP 2\n",
        "\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(train_dfx)):\n",
        "    print(f'Fold {fold + 1}/{k}')\n",
        "\n",
        "    # Prepare train and validation data\n",
        "    X_train, X_val = train_dfx[train_index], train_dfx[val_index]\n",
        "    y_train, y_val = train_dfy[train_index], train_dfy[val_index]\n",
        "\n",
        "    # Create datasets and dataloaders for each fold\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Initialize the model, optimizer\n",
        "    model2 = MLPRegressionModel2(input_dim=5, hidden_dim=128, output_dim=1)\n",
        "    optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "    # initalize lists to store losses for this fold\n",
        "    fold_train_losses = []\n",
        "    fold_val_losses = []\n",
        "\n",
        "\n",
        "    for epoch in range(100):  # Number of epochs, adjust as needed\n",
        "        model2.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model2(inputs)\n",
        "            loss = criterion(outputs.squeeze(), targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        fold_train_losses.append(avg_train_loss)\n",
        "\n",
        "        model2.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "          for inputs, targets in val_loader:\n",
        "            outputs = model2(inputs)\n",
        "            loss = criterion(outputs.squeeze(), targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "          avg_val_loss = val_loss / len(val_loader)\n",
        "          fold_val_losses.append(avg_val_loss)\n",
        "          fold_results.append(avg_val_loss)\n",
        "\n",
        "\n",
        "        # Append the losses for this fold to the overall lists\n",
        "        all_training_losses.append(fold_train_losses)\n",
        "        all_val_losses.append(fold_val_losses)\n",
        "\n",
        "\n",
        "# Average validation MAE across all folds\n",
        "mean_val_loss = np.mean(fold_results)\n",
        "print(f'Average Validation MAE across {k} folds: {mean_val_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEl87HvW8MBu",
        "outputId": "35b9e655-36ba-4592-967d-177facfa9cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "Average Validation MAE across 5 folds: 5.5743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####7.3.4 Plotting the Graph Of Training VS Validation Losses\n",
        "\n"
      ],
      "metadata": {
        "id": "6q04bdY1InRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_train_losses = np.mean(all_training_losses, axis=0)\n",
        "mean_val_losses = np.mean(all_val_losses, axis=0)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(mean_train_losses, label='Average Training Loss')\n",
        "plt.plot(mean_val_losses, label='Average Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(f'Average Training vs Validation Loss across {k} Folds')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "R8x8raBWC3tR",
        "outputId": "da445ae7-716a-4929-e478-3b0c34fa78a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.3.5 Adding In Batch Normalization And Dropout\n"
      ],
      "metadata": {
        "id": "sEMDruZSMPym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPRegressionModel3(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim,dropout_rate=0.5):\n",
        "        super(MLPRegressionModel3, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.bn1 = torch.nn.BatchNorm1d(hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim//2)\n",
        "        self.bn2 = torch.nn.BatchNorm1d(hidden_dim//2)\n",
        "        self.fc3 = torch.nn.Linear(hidden_dim//2, hidden_dim//4)\n",
        "        self.bn3 = torch.nn.BatchNorm1d(hidden_dim//4)\n",
        "        self.fc4 = torch.nn.Linear(hidden_dim//4, hidden_dim//8)\n",
        "        self.bn4 = torch.nn.BatchNorm1d(hidden_dim//8)\n",
        "        self.fcLast = torch.nn.Linear(hidden_dim//8, output_dim)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply first fully connected layer, batch norm, ReLU, and dropout\n",
        "        x = F.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply second fully connected layer, batch norm, ReLU, and dropout\n",
        "        x = F.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply third fully connected layer, batch norm, ReLU, and dropout\n",
        "        x = F.relu(self.bn3(self.fc3(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply fourth fully connected layer, batch norm, ReLU, and dropout\n",
        "        x = F.relu(self.bn4(self.fc4(x)))\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Apply the final fully connected layer\n",
        "        x = self.fcLast(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "zxttJxVuZIB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = train_dfx.shape[1]\n",
        "print(input_dim)\n",
        "hidden_dim = 128\n",
        "output_dim = 1\n",
        "\n",
        "model3 = MLPRegressionModel3(input_dim, hidden_dim, output_dim)\n",
        "criterion = torch.nn.L1Loss()\n",
        "optimizer = torch.optim.Adam(model3.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "num_epochs =500  # Number of epochs for training\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model3.train()  # Set the model to training mode\n",
        "    epoch_loss = 0  # Initialize loss for this epoch\n",
        "    total_samples = 0\n",
        "    for inputs, targets in FinalTrainComparisonloader:\n",
        "        optimizer.zero_grad()  # Zero the gradients\n",
        "\n",
        "        outputs = model3(inputs)  # Forward pass: compute predictions\n",
        "        loss = criterion(outputs, targets.view(-1, 1).float())  # Compute loss\n",
        "\n",
        "        loss.backward()  # Backward pass: compute gradients\n",
        "        optimizer.step()  # Update model weights\n",
        "        torch.nn.utils.clip_grad_norm_(model3.parameters(), max_norm=1.0)\n",
        "        epoch_loss += loss.item()*inputs.size(0)  # Accumulate loss for this epoch\n",
        "        total_samples += inputs.size(0)\n",
        "    # Print average loss for the epoch\n",
        "    avg_loss = epoch_loss/total_samples\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}')\n",
        "\n",
        "    model3.eval()  # Set the model to evaluation mode\n",
        "    val_loss = 0.0\n",
        "    total_val_samples = 0\n"
      ],
      "metadata": {
        "id": "pohUUE2pKLMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7.3.6 Evaluation Of the Batch Normalization\n",
        "\n",
        "After adding in Batch Normalization and Dropouts, the MLPRegression Deeper model has improved and is now the final model that is better than the initial MLP Model and the baseline Model.\n",
        "\n",
        "The model did better than the baseline model and the final Graph Neural Network Model but worst than the initial simple MLP model as seen from the metrics such as the mean squared error,\n",
        "\n",
        "From the model below, the results from the evaluation are as follows:\n",
        "\n",
        "MAE:5.58\n",
        "\n",
        "MSE:81.1937\n",
        "\n",
        "RMSE:9.0108\n",
        "\n",
        "MAPE:0.35\n",
        "\n",
        "\n",
        "Initial MLP Model's Results:\n",
        "\n",
        "MAE:5.66\n",
        "\n",
        "MSE:96.91\n",
        "\n",
        "RMSE:9.84\n",
        "\n",
        "MAPE:0.3742\n",
        "\n",
        "Baseline Model's Results:\n",
        "\n",
        "MAE:6.40\n",
        "\n",
        "MSE:84.45\n",
        "\n",
        "RMSE:9.18\n",
        "\n",
        "MAPE:0.4531"
      ],
      "metadata": {
        "id": "NcShkX1tO3qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model3.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Initialize variables to accumulate loss\n",
        "total_loss = 0\n",
        "n_batches = 0\n",
        "\n",
        "all_actuals = []\n",
        "all_predictions = []\n",
        "all_differences = []\n",
        "\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    for inputs, targets in testloader:\n",
        "        # Forward pass: Compute predicted outputs by passing inputs to the model\n",
        "        outputs = model3(inputs)\n",
        "        loss = criterion(outputs, targets.view(-1, 1))\n",
        "        # Compute the loss\n",
        "        differences = targets.view(-1,1) - outputs\n",
        "\n",
        "        all_actuals.extend(targets.detach().cpu().numpy())\n",
        "        all_predictions.extend(outputs.detach().cpu().numpy())\n",
        "        all_differences.extend(differences.detach().cpu().numpy())\n",
        "        # Accumulate loss\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "\n",
        "all_actuals_tensor = torch.tensor(all_actuals, dtype=torch.float32)\n",
        "all_predictions_tensor = torch.tensor(all_predictions, dtype=torch.float32)\n",
        "\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "mse = mse_loss(all_predictions_tensor, all_actuals_tensor.view(-1, 1))\n",
        "print(f'Mean Squared Error (MSE) using nn.MSELoss: {mse.item():.4f}')\n",
        "\n",
        "mae_loss = torch.nn.L1Loss()\n",
        "mae=mae_loss(all_predictions_tensor,all_actuals_tensor.view(-1,1))\n",
        "print(f'Mean Absolute Error (MAE) using nn.L1Loss: {mae.item():.4f}')\n",
        "\n",
        "\n",
        "rmse = mean_squared_error(all_predictions_tensor,all_actuals_tensor.view(-1,1), squared=False)\n",
        "print(f'Root Mean Squared Error (rmse): {rmse.item():.4f}')\n",
        "\n",
        "\n",
        "\n",
        "mape = mean_absolute_percentage_error(all_predictions_tensor,all_actuals_tensor.view(-1,1))\n",
        "print(f'Mean Absolute Percentage Error (MAE): {mape.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gg99bBcRKhrz",
        "outputId": "8352d2e8-de21-437a-dee8-6d58cbb199ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE) using nn.MSELoss: 81.1937\n",
            "Mean Absolute Error (MAE) using nn.L1Loss: 5.5887\n",
            "Root Mean Squared Error (rmse): 9.0108\n",
            "Mean Absolute Percentage Error (MAE): 0.3500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####7.3.3 K-Fold Cross Validation And Evaluating the Graph\n",
        "From the K-fold Cross validation, the training and valuation loss seems to be decreasing and remain close to each other and reaching a convergence point at around 40 epochs. The Training And Validation Loss are close to each other and shows that the model is alright."
      ],
      "metadata": {
        "id": "PKw8cSdoWvSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "fold_results = []\n",
        "criterion = torch.nn.L1Loss()\n",
        "\n",
        "all_training_losses=[]\n",
        "all_val_losses =[]\n",
        "\n",
        "##MLP 2\n",
        "\n",
        "\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(train_dfx)):\n",
        "    print(f'Fold {fold + 1}/{k}')\n",
        "\n",
        "    # Prepare train and validation data\n",
        "    X_train, X_val = train_dfx[train_index], train_dfx[val_index]\n",
        "    y_train, y_val = train_dfy[train_index], train_dfy[val_index]\n",
        "\n",
        "    # Create datasets and dataloaders for each fold\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    # Initialize the model, optimizer\n",
        "    model3 = MLPRegressionModel3(input_dim=5, hidden_dim=128, output_dim=1)\n",
        "    optimizer = torch.optim.Adam(model3.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "    # initalize lists to store losses for this fold\n",
        "    fold_train_losses = []\n",
        "    fold_val_losses = []\n",
        "\n",
        "\n",
        "    for epoch in range(100):  # Number of epochs, adjust as needed\n",
        "        model3.train()\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model3(inputs)\n",
        "            loss = criterion(outputs.squeeze(), targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        fold_train_losses.append(avg_train_loss)\n",
        "\n",
        "        model3.eval()\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "          for inputs, targets in val_loader:\n",
        "            outputs = model3(inputs)\n",
        "            loss = criterion(outputs.squeeze(), targets)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "          avg_val_loss = val_loss / len(val_loader)\n",
        "          fold_val_losses.append(avg_val_loss)\n",
        "          fold_results.append(avg_val_loss)\n",
        "\n",
        "\n",
        "        # Append the losses for this fold to the overall lists\n",
        "        all_training_losses.append(fold_train_losses)\n",
        "        all_val_losses.append(fold_val_losses)\n",
        "\n",
        "\n",
        "# Average validation MAE across all folds\n",
        "mean_val_loss = np.mean(fold_results)\n",
        "print(f'Average Validation MAE across {k} folds: {mean_val_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9z6zFwoPo48",
        "outputId": "96e0b4fa-f20f-4a63-fb4f-74d0fa911b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "Average Validation MAE across 5 folds: 5.6869\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_train_losses = np.mean(all_training_losses, axis=0)\n",
        "mean_val_losses = np.mean(all_val_losses, axis=0)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(mean_train_losses, label='Average Training Loss')\n",
        "plt.plot(mean_val_losses, label='Average Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(f'Average Training vs Validation Loss across {k} Folds')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "RpafF2MxVGK1",
        "outputId": "98f36e8c-3742-4ccf-a524-69033e286829"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0kAAAIjCAYAAADWYVDIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACfOElEQVR4nOzdd3hUZd7G8e9kMpn0AukkhNBbQERQOkgVBAFBEVRQFF3WXvZ11wb2VXRRXEVXRFFQEAEbiEFFBZEiTXqvCYSW3iaZ8/4xZCSGEgLkTOD+XNdcJGfOzPnNyZMw9zzlWAzDMBAREREREREAvMwuQERERERExJMoJImIiIiIiJxAIUlEREREROQECkkiIiIiIiInUEgSERERERE5gUKSiIiIiIjICRSSRERERERETqCQJCIiIiIicgKFJBERERERkRMoJImIVNCuXbuwWCx88MEHFXq8xWJhzJgx57Wmi9Vfz9UHH3yAxWJh165dZ3xsrVq1GDFixHmtZ8SIEdSqVeu8PqfIuSpvWz+b3x+RS5VCkkgV99Zbb2GxWLjyyivNLsVjjBkzBovFcsZb586dzS71onPfffdhsVjYtm3bKfd5/PHHsVgsrF27thIrO3spKSmMGTOG1atXm12KW0kwHzdunNmlyHGn+nvj6+tbrsfXqlXrlH+j8vPzL3D1InIq3mYXICLnZurUqdSqVYtly5axbds26tata3ZJphs4cGCp85Cdnc3f/vY3BgwYwMCBA93bo6Kizuk4CQkJ5OXlYbPZKvT4vLw8vL0vrj/Dw4YNY8KECUybNo2nnnrqpPt88sknJCUl0axZswof55ZbbmHIkCHY7fYKP8eZpKSkMHbsWGrVqsVll11W6r7//e9/OJ3OC3ZsqXrefvttAgMD3d9brdZyP/ayyy7j4YcfLrPdx8fnvNQmImfv4vrfWeQSs3PnTn799VdmzZrFXXfdxdSpU3n66acrtQan00lhYWG5PzWtDM2aNSv1Bvzw4cP87W9/o1mzZtx8882nfFx+fj4+Pj54eZWvk/1sPi0+GU86Z+fLlVdeSd26dfnkk09OGpKWLFnCzp07eemll87pOFar9azehJ5vFQ3Gcn4UFRXhdDo9KkQMGjSI8PDwCj22Ro0ap/3bJCKVT8PtRKqwqVOnEhYWRp8+fRg0aBBTp0513+dwOKhWrRq33XZbmcdlZmbi6+vLI4884t5WUFDA008/Td26dbHb7cTHx/OPf/yDgoKCUo+1WCzcc889TJ06lSZNmmC32/n2228BGDduHG3btqV69er4+fnRsmVLZs6cWeb4eXl53HfffYSHhxMUFES/fv3Yv3//Sefo7N+/n9tvv52oqCjsdjtNmjTh/fffP5fTBsDChQuxWCx8+umnPPHEE9SoUQN/f38yMzM5evQojzzyCElJSQQGBhIcHMw111zDmjVrSj3HyeYkjRgxgsDAQPbv30///v0JDAwkIiKCRx55hOLi4jLn8sTXWzJsZ9u2bYwYMYLQ0FBCQkK47bbbyM3NrfA5PNHBgwfx9vZm7NixZe7bvHkzFouFN998E3C1obFjx1KvXj18fX2pXr067du3Jzk5+bTndtiwYWzatImVK1eWuW/atGlYLBZuuukmCgsLeeqpp2jZsiUhISEEBATQoUMHfvzxx9M+P5x8ToVhGDz33HPExcXh7+9Ply5dWL9+fZnHlufnu3DhQlq1agXAbbfd5h7+VPKzPtmcpJycHB5++GHi4+Ox2+00aNCAcePGYRhGqf1KfofmzJlD06ZN3e265PfofEhLS2PkyJFERUXh6+tL8+bN+fDDD8vs9+mnn9KyZUuCgoIIDg4mKSmJ119/3X1/RdtAeX+HwPXhxJgxY6hfvz6+vr7ExMQwcOBAtm/fDpQeYjh+/Hjq1KmD3W5nw4YNAPzwww906NCBgIAAQkNDue6669i4cWOpY2RlZfHAAw9Qq1Yt7HY7kZGRdO/evVQb3bp1K9dffz3R0dH4+voSFxfHkCFDyMjIKNc5NwyDzMzMMj/v86G8betk1q9fz9VXX42fnx9xcXE899xzJ+0FXbFiBT179iQ8PBw/Pz8SExO5/fbbz/trEakq1JMkUoVNnTqVgQMH4uPjw0033cTbb7/N8uXLadWqFTabjQEDBjBr1izeeeedUp+4zpkzh4KCAoYMGQK4eoP69evHokWLGDVqFI0aNeKPP/7gP//5D1u2bGHOnDmljvvDDz8wY8YM7rnnHsLDw91vFl9//XX69evHsGHDKCws5NNPP2Xw4MF8/fXX9OnTx/34ESNGMGPGDG655Rauuuoqfvrpp1L3lzh48CBXXXWV+01lREQE8+bNY+TIkWRmZvLAAw+c8zl89tln8fHx4ZFHHqGgoAAfHx82bNjAnDlzGDx4MImJiRw8eJB33nmHTp06sWHDBmJjY0/7nMXFxfTs2ZMrr7yScePGsWDBAl599VXq1KnD3/72tzPWdMMNN5CYmMiLL77IypUree+994iMjOTf//63e5/ynsO/ioqKolOnTsyYMaNMr+P06dOxWq0MHjwYcIW2F198kTvuuIPWrVuTmZnJihUrWLlyJd27dz/lMYYNG8bYsWOZNm0al19+eanzMmPGDDp06EDNmjU5fPgw7733HjfddBN33nknWVlZTJo0iZ49e7Js2bIyQ9zO5KmnnuK5556jd+/e9O7dm5UrV9KjRw8KCwtL7bdjx44z/nwbNWrEM888w1NPPcWoUaPo0KEDAG3btj3psQ3DoF+/fvz444+MHDmSyy67jPnz5/Poo4+yf/9+/vOf/5Taf9GiRcyaNYvRo0cTFBTEG2+8wfXXX8+ePXuoXr36Wb3uv8rLy6Nz585s27aNe+65h8TERD777DNGjBhBeno6999/PwDJycncdNNNdO3a1d22Nm7cyOLFi937VLQNlOccg6tNXHvttXz//fcMGTKE+++/n6ysLJKTk1m3bh116tRxP+fkyZPJz89n1KhR2O12qlWrxoIFC7jmmmuoXbs2Y8aMIS8vjwkTJtCuXTtWrlzp/tt09913M3PmTO655x4aN27MkSNHWLRoERs3buTyyy+nsLCQnj17UlBQwL333kt0dDT79+/n66+/Jj09nZCQkDOe99q1a5OdnU1AQAD9+/fn1VdfLfeQXofDweHDh0tt8/f3x9/f/6zb1okOHDhAly5dKCoq4rHHHiMgIIB3330XPz+/UvulpaXRo0cPIiIieOyxxwgNDWXXrl3MmjWrXPWLXJQMEamSVqxYYQBGcnKyYRiG4XQ6jbi4OOP+++937zN//nwDML766qtSj+3du7dRu3Zt9/cfffSR4eXlZfzyyy+l9ps4caIBGIsXL3ZvAwwvLy9j/fr1ZWrKzc0t9X1hYaHRtGlT4+qrr3Zv+/333w3AeOCBB0rtO2LECAMwnn76afe2kSNHGjExMcbhw4dL7TtkyBAjJCSkzPFO5dChQ2We+8cffzQAo3bt2mWeJz8/3yguLi61befOnYbdbjeeeeaZUtsAY/Lkye5tw4cPN4BS+xmGYbRo0cJo2bJlqW1/renpp582AOP2228vtd+AAQOM6tWru78/m3N4Mu+8844BGH/88Uep7Y0bNy71s2revLnRp0+f0z7XqbRq1cqIi4srdR6//fZbAzDeeecdwzAMo6ioyCgoKCj1uGPHjhlRUVFlzsFfX9fkyZMNwNi5c6dhGIaRlpZm+Pj4GH369DGcTqd7v3/9618GYAwfPty9rbw/3+XLl5f5+ZYYPny4kZCQ4P5+zpw5BmA899xzpfYbNGiQYbFYjG3btpV6LT4+PqW2rVmzxgCMCRMmlDnWX+sEjFdeeeWU+4wfP94AjI8//ti9rbCw0GjTpo0RGBhoZGZmGoZhGPfff78RHBxsFBUVnfK5KtoGynuO33//fQMwXnvttTLPUfJzLHnNwcHBRlpaWql9LrvsMiMyMtI4cuSIe9uaNWsMLy8v49Zbb3VvCwkJMf7+97+fst5Vq1YZgPHZZ5+d3Qs1XOf7nnvuMaZOnWrMnDnTuP/++w1vb2+jXr16RkZGxhkfn5CQYABlbiXt/WzaVkJCQqm2/sADDxiAsXTpUve2tLQ0IyQkpNTvz+zZsw3AWL58+Vm/fpGLlYbbiVRRU6dOJSoqii5dugCuITw33ngjn376qXtY19VXX014eDjTp093P+7YsWMkJydz4403urd99tlnNGrUiIYNG3L48GH37eqrrwYoM/ypU6dONG7cuExNJ346eezYMTIyMujQoUOpIS0lQ4pGjx5d6rH33ntvqe8Nw+Dzzz+nb9++GIZRqq6ePXuSkZFx0uFcZ2v48OFlPlW12+3ueUnFxcUcOXKEwMBAGjRoUO5j3n333aW+79ChAzt27KjwY48cOUJmZiZQ/nN4KgMHDsTb27tUu1i3bh0bNmwo1S5CQ0NZv349W7duLdfznujmm29m3759/Pzzz+5t06ZNw8fHx91TZbVa3T2cTqeTo0ePUlRUxBVXXHHWP9sFCxZQWFjIvffei8VicW8/WW/j+fj5/tXcuXOxWq3cd999pbY//PDDGIbBvHnzSm3v1q1bqV6SZs2aERwcXO42cqZaoqOjuemmm9zbbDYb9913H9nZ2fz000+A6+ebk5Nz2qFzFW0D5T3Hn3/+OeHh4Sdtuyf+HAGuv/56IiIi3N+npqayevVqRowYQbVq1dzbmzVrRvfu3Zk7d26p17F06VJSUlJOWm9JT9H8+fPLDG09k/vvv58JEyYwdOhQrr/+esaPH8+HH37I1q1beeutt8r1HFdeeSXJycmlbrfeeitw9m3rRHPnzuWqq66idevW7m0REREMGzas1H6hoaEAfP311zgcjnLVLHKxU0gSqYKKi4v59NNP6dKlCzt37mTbtm1s27aNK6+8koMHD/L9998D4O3tzfXXX88XX3zhnls0a9YsHA5HqTfDW7duZf369URERJS61a9fH3ANxThRYmLiSev6+uuvueqqq/D19aVatWpERETw9ttvlxrTv3v3bry8vMo8x19X5Tt06BDp6em8++67ZeoqmWf117oq4mSvxel08p///Id69epht9sJDw8nIiKCtWvXlmt+gq+vb6k3cwBhYWEcO3asXDXVrFmzzGMB9+PLew5PJTw8nK5duzJjxgz3tunTp+Pt7V1q9b9nnnmG9PR06tevT1JSEo8++mi5l+0eMmQIVquVadOmAa55J7Nnz+aaa65xvx6ADz/8kGbNmrnnu0RERPDNN9+Uex5Iid27dwNQr169UtsjIiJKHQ/O/ed7quPHxsYSFBRUanujRo1K1Vfirz9jOLs2cqZa6tWrV2YBkr/WMnr0aOrXr88111xDXFwct99+e5l5URVtA+U9x9u3b6dBgwblWuXxr+295HU0aNCgzL6NGjXi8OHD5OTkAPDyyy+zbt064uPjad26NWPGjCkVSBMTE3nooYd47733CA8Pp2fPnvz3v/+tcHsYOnQo0dHRLFiwoFz7h4eH061bt1K32rVru1/n2bStE5W0hb/66znr1KkT119/PWPHjiU8PJzrrruOyZMnl5mTKnIpUUgSqYJ++OEHUlNT+fTTT6lXr577dsMNNwCUWsBhyJAhZGVluT9tnDFjBg0bNqR58+bufZxOJ0lJSWU+ySy5/bXH4q89LwC//PIL/fr1w9fXl7feeou5c+eSnJzM0KFDKzSRuWRi8c0333zKutq1a3fWz/tXJ3stL7zwAg899BAdO3bk448/Zv78+SQnJ9OkSZNyLft8rquunerxFTmPpzJkyBC2bNnivgbQjBkz6Nq1a6nVuTp27Mj27dt5//33adq0Ke+99x6XX34577333hmfv2Ri/Oeff47D4eCrr74iKyur1CfYH3/8MSNGjKBOnTpMmjSJb7/9luTkZK6++uoLurz2uf58z4fK+BmfSWRkJKtXr+bLL790z3m55pprGD58uHufiraBC3GOT/a7Wl433HADO3bsYMKECcTGxvLKK6/QpEmTUr0wr776KmvXruVf//qXe2GUJk2asG/fvgodMz4+nqNHj1a45spksViYOXMmS5Ys4Z577nEvmNOyZUuys7PNLk/EFFq4QaQKmjp1KpGRkfz3v/8tc9+sWbOYPXs2EydOxM/Pj44dOxITE8P06dNp3749P/zwA48//nipx9SpU4c1a9bQtWvXMkNcyuvzzz/H19eX+fPnl7p2zeTJk0vtl5CQgNPpZOfOnaU+4fzrxUcjIiIICgqiuLiYbt26Vaimipo5cyZdunRh0qRJpbanp6dXeInf86m85/B0+vfvz1133eUecrdlyxb++c9/ltmvZIXE2267jezsbDp27MiYMWO44447zniMYcOG8e233zJv3jymTZtGcHAwffv2dd8/c+ZMateuzaxZs0q1u4osY5+QkAC4ekVLPoEHV4/kX3tnyvvzPZvfhYSEBBYsWEBWVlapT/w3bdpUqr7KkJCQwNq1a3E6naV6k05Wi4+PD3379qVv3744nU5Gjx7NO++8w5NPPunumaxIGyjvOa5Tpw5Lly7F4XCc9bLqJa9j8+bNZe7btGkT4eHhBAQEuLfFxMQwevRoRo8eTVpaGpdffjnPP/8811xzjXufpKQkkpKSeOKJJ/j1119p164dEydO5Lnnnjur2gzDYNeuXbRo0eKsHncy59K2EhISTjpU8mTnDOCqq67iqquu4vnnn2fatGkMGzaMTz/9tFy/7yIXG/UkiVQxeXl5zJo1i2uvvZZBgwaVud1zzz1kZWXx5ZdfAuDl5cWgQYP46quv+OijjygqKio11A5cn7Lu37+f//3vfyc9XsmQldOxWq1YLJZSy1zv2rWrzMp4PXv2BCgzVn/ChAllnu/666/n888/Z926dWWOd+jQoTPWVFFWq7XMJ/qfffYZ+/fvv2DHPBvlPYenExoaSs+ePZkxYwaffvopPj4+9O/fv9Q+R44cKfV9YGAgdevWLfcQnP79++Pv789bb73FvHnzGDhwYKlrQ5X0ppx4rpcuXcqSJUvK/TpKdOvWDZvNxoQJE0o93/jx48vsW96fb8kb7PT09DMev3fv3hQXF7uXTy/xn//8B4vFUuqN+IXWu3dvDhw4UGrOWVFRERMmTCAwMJBOnToBZX++Xl5e7uuLlfyMK9oGynuOr7/+eg4fPlzmvMGZe9ViYmK47LLL+PDDD0v9jNatW8d3331H7969Adfw5L8Om4uMjCQ2Ntb9OjIzMykqKiq1T1JSEl5eXmd8rSf7W/T2229z6NAhevXqddrHlse5tK3evXvz22+/sWzZslL1njjaAFxDef96vktWl9SQO7lUqSdJpIr58ssvycrKol+/fie9/6qrriIiIoKpU6e6w9CNN97IhAkTePrpp0lKSnKPZS9xyy23MGPGDO6++25+/PFH2rVrR3FxMZs2bWLGjBnMnz+fK6644rR19enTh9dee41evXoxdOhQ0tLS+O9//0vdunVLzWFo2bKle3LzkSNH3MtXb9myBSj96f1LL73Ejz/+yJVXXsmdd95J48aNOXr0KCtXrmTBggUXbCjLtddeyzPPPMNtt91G27Zt+eOPP5g6dWqpHgoznc05PJ0bb7yRm2++mbfeeouePXu6J2+XaNy4MZ07d6Zly5ZUq1aNFStWuJdRLo/AwED69+/vnpf018ni1157LbNmzWLAgAH06dOHnTt3MnHiRBo3bnzWQ3xKrkX14osvcu2119K7d29WrVrFvHnzyvT+lffnW6dOHUJDQ5k4cSJBQUEEBARw5ZVXnnQeW9++fenSpQuPP/44u3btonnz5nz33Xd88cUXPPDAA6UWaTgfvv/+e/Lz88ts79+/P6NGjeKdd95hxIgR/P7779SqVYuZM2eyePFixo8f7+6NuOOOOzh69ChXX301cXFx7N69mwkTJnDZZZe5/0ZUtA2U9xzfeuutTJkyhYceeohly5bRoUMHcnJyWLBgAaNHj+a666477XFeeeUVrrnmGtq0acPIkSPdS4CHhIS4rxeWlZVFXFwcgwYNonnz5gQGBrJgwQKWL1/Oq6++CriGMN9zzz0MHjyY+vXrU1RUxEcffeT+sOZ0EhISuPHGG0lKSsLX15dFixbx6aefctlll3HXXXed9rHlcS5t6x//+AcfffQRvXr14v7773cvAV7S21jiww8/5K233mLAgAHUqVOHrKws/ve//xEcHOwOmyKXHBNW1BORc9C3b1/D19fXyMnJOeU+I0aMMGw2m3vpbKfTacTHx590GdkShYWFxr///W+jSZMmht1uN8LCwoyWLVsaY8eOLbWMLXDKpXQnTZpk1KtXz7Db7UbDhg2NyZMnu5e1PlFOTo7x97//3ahWrZoRGBho9O/f39i8ebMBGC+99FKpfQ8ePGj8/e9/N+Lj4w2bzWZER0cbXbt2Nd59991ynS/DOP0S4Cdb8jc/P994+OGHjZiYGMPPz89o166dsWTJEqNTp05Gp06d3PudagnwgICAMs95svPw15pK9jl06FCp/f663LVhnN05PJXMzEzDz8+vzHLRJZ577jmjdevWRmhoqOHn52c0bNjQeP75543CwsJyPb9hGMY333xjAEZMTEyZJaGdTqfxwgsvGAkJCYbdbjdatGhhfP3112WW1zaMMy8BbhiGUVxcbIwdO9b9c+vcubOxbt26Mssil/fnaxiG8cUXXxiNGzc2vL29S/2sT1ZjVlaW8eCDDxqxsbGGzWYz6tWrZ7zyyiulliQveS0n+x36a50nU9LmTnX76KOPDMNw/d7cdtttRnh4uOHj42MkJSWVWcp85syZRo8ePYzIyEjDx8fHqFmzpnHXXXcZqamp7n0q2gbO5hzn5uYajz/+uJGYmOj+HR80aJCxffv2Uq/5VMueL1iwwGjXrp3h5+dnBAcHG3379jU2bNjgvr+goMB49NFHjebNmxtBQUFGQECA0bx5c+Ott95y77Njxw7j9ttvN+rUqWP4+voa1apVM7p06WIsWLDgtK/TMAzjjjvuMBo3bmwEBQUZNpvNqFu3rvF///d/7qXWzyQhIeGMy6yXt22drA2tXbvW6NSpk+Hr62vUqFHDePbZZ41JkyaV+v1ZuXKlcdNNNxk1a9Y07Ha7ERkZaVx77bXGihUryvUaRC5GFsOoxFmiIiKnsHr1alq0aMHHH39cpsdBykfnUERE5PzQnCQRqXR5eXllto0fPx4vLy86duxoQkVVj86hiIjIhaM5SSJS6V5++WV+//13unTpgre3N/PmzWPevHmMGjWK+Ph4s8urEnQORURELhwNtxORSpecnMzYsWPZsGED2dnZ1KxZk1tuuYXHH3+8XBeVFJ1DERGRC0khSURERERE5ASakyQiIiIiInIChSQREREREZETXPQD151OJykpKQQFBZX7AosiIiIiInLxMQyDrKwsYmNj8fI6dX/RRR+SUlJStNKTiIiIiIi47d27l7i4uFPef9GHpKCgIMB1IoKDg02txeFw8N1339GjRw9sNpuptUjVoXYjFaW2IxWhdiMVoXYjFVXZbSczM5P4+Hh3RjiViz4klQyxCw4O9oiQ5O/vT3BwsP6ASLmp3UhFqe1IRajdSEWo3UhFmdV2zjQNRws3iIiIiIiInEAhSURERERE5AQKSSIiIiIiIie46OckiYiIiHgywzAoKiqiuLjY7FIqzOFw4O3tTX5+fpV+HVL5znfbsVqteHt7n/OlfxSSRERERExSWFhIamoqubm5ZpdyTgzDIDo6mr179+q6lHJWLkTb8ff3JyYmBh8fnwo/h0KSiIiIiAmcTic7d+7EarUSGxuLj49PlQ0YTqeT7OxsAgMDT3uBTpG/Op9txzAMCgsLOXToEDt37qRevXoVfk6FJBERERETFBYW4nQ6iY+Px9/f3+xyzonT6aSwsBBfX1+FJDkr57vt+Pn5YbPZ2L17t/t5K0KtWERERMREChUi59f5+J3Sb6WIiIiIiMgJFJJEREREREROoJAkIiIiIlLJFi5ciMViIT09vdyPGTFiBP37979gNcmfFJJERERE5KwtWbIEq9VKnz59zC7lgvrggw+wWCynve3ateusn7dt27akpqYSEhJS7se8/vrrfPDBB2d9rLOlMKaQJCIiIiIVMGnSJO69915+/vlnUlJSLuixSi64a4Ybb7yR1NRU961NmzbceeedpbbFx8e79y8sLCzX8/r4+BAdHX1Wy76HhIQQGhp6ti9BKkAhSURERMQDGIZBbmGRKTfDMM6q1uzsbKZPn87f/vY3+vTpw4cffui+b+jQodx4442l9nc4HISHhzNlyhTAtezziy++SGJiIn5+fjRv3pyZM2e69y8ZijZv3jxatmyJ3W5n0aJFbN++neuuu46oqCgCAwNp1aoVCxYsKHWs1NRU+vTpg5+fH4mJiUybNo1atWoxfvx49z7p6enccccdREREEBwczNVXX82aNWtO+lr9/PyIjo5233x8fPD393d//9hjj3H99dfz/PPPExsbS4MGDQD46KOPuOKKKwgKCiI6OpqhQ4eSlpZW5jWWDLf74IMPCA0NZf78+TRq1IjAwEB69epFamqq+zF/7eHp3Lkz9913H//4xz+oVq0a0dHRjBkzplT9mzZton379vj6+tK4cWMWLFiAxWJhzpw5J//hlsNPP/1E69atsdvtxMTE8Nhjj5UKsTNnziQpKQk/Pz+qV69Ot27dyMnJcb/u1q1bExAQQGhoKB06dGDPnj0VruVC0XWSRERERDxAnqOYxk/NN+XYG57pib9P+d8Wzpgxg4YNG9KgQQNuvvlmHnjgAUaPHg3AsGHDGDx4sPsCoQDz588nNzeXAQMGAPDiiy/y8ccfM3HiROrVq8fPP//MzTffTEREBJ06dXIf57HHHmPcuHHUrl2bsLAw9u7dS+/evXn++eex2+1MmTKFvn37snnzZmrWrAnArbfeyuHDh1m4cCE2m42HHnqoVDgBGDx4MH5+fsybN4+QkBDeeecdunbtypYtW6hWrdpZn7/vv/+e4OBgkpOT3dscDgfPPvssDRo0IC0tjYceeogRI0Ywd+7cUz5Pbm4u48aN46OPPsLLy4ubb76ZRx55hKlTp57yMR9++CEPPfQQS5cuZcmSJYwYMYJ27drRvXt3iouL6d+/PzVr1mTp0qVkZWXx8MMPn/XrO9H+/fvp3bs3I0aMYMqUKWzatIk777wTX19fxowZQ2pqKjfddBMvv/wyAwYMICsri19++cXdG9i/f3/uvPNOPvnkEwoLC/ntt9888iLKCkkiIiIiclYmTZrEzTffDECvXr3IyMhg8eLF9O7dm549exIQEMDs2bO55ZZbAJg2bRr9+vUjKCiIgoICXnjhBRYsWECbNm0AqF27NosWLeKdd94pFZKeeeYZunfv7v6+WrVqNG/e3P39s88+y+zZs/nyyy+555572LRpEwsWLGD58uVcccUVALz33nvUq1fP/ZhFixaxbNky0tLSsNvtAIwbN445c+Ywc+ZMRo0addbnIyAggPfeew8fHx/3tttvv939de3atXnjjTdo1apVqfD4Vw6Hg4kTJ1KnTh0A7rnnHp555pnTHrtZs2Y8/fTTANSrV48333yT77//nu7du5OcnMz27dtZuHAh0dHRADz//POlzunZeuutt4iPj+fNN9/EYrHQsGFDUlJS+L//+z+eeuopUlNTKSoqYuDAgSQkJACQlJQEwNGjR8nIyODaa691v8YGDRqQmZlZ4XouFIWkSvTTlkOsOmKhY0ERYTab2eWIiIiIB/GzWdnwTE/Tjl1emzdvZtmyZcyePRsAb29vbrjhBj766CN69+7t/n7q1Knccsst5OTk8MUXX/Dpp58CsG3bNnJzc8u8US8sLKRFixaltpUEnRLZ2dmMGTOGb775xv1mPC8vzz1ca/PmzXh7e3P55Ze7H1O3bl3CwsLc369Zs4bs7GyqV69e6rnz8vLYvn17uc/DiZKSkkoFJIDff/+dMWPGsGbNGo4dO4bT6QRgz549NG7c+KTP4+/v7w4PADExMWV6wf6qWbNmpb4/8TGbN28mPj7eHZAAWrduXf4XdhIbN26kTZs2pXp/2rVrR3Z2Nvv27aN58+Z07dqVpKQkevbsSY8ePRg0aBBhYWFUq1aNESNG0LNnT7p37063bt0YNGgQAQEB51TThaCQVIke/Xwdx3Kt3JCRT1ign9nliIiIiAexWCxnNeTNLJMmTaKoqIjY2Fj3NsMwsNvtZGRkEBYWxrBhw+jUqRNpaWkkJyfj5+dHr169AFfQAfjmm2+oUaNGqecu6dkp8dc3z4888gjJycmMGzeOunXr4ufnx6BBg8q9WELJ8WNiYli4cGGZ+yq6KMJf68zJyaFnz5707NmTqVOnEhERwZ49e+jZs+dpa7X95UN0i8VyxvliJ3tMSSAzg9VqJTk5mV9//ZXvvvuOCRMm8Pjjj7N06VISExOZPHky9913H99++y3Tp0/niSeeYNasWXTt2tW0mk9GCzdUIh+r63QXFpnXcEVEREQqqqioiClTpvDqq6+yevVq923VqlVER0fzySefAK7lrePj45k+fTpTp05l8ODB7jfzjRs3xm63s2fPHurWrVvqduIqcSezePFiRowYwYABA0hKSiI6OrrU8tsNGjSgqKiIVatWubdt27aNY8eOub+//PLLOXDgAN7e3mWOHx4efl7O06ZNmzhy5AgvvfQSHTp0oGHDhmfsEboQGjRowN69ezl48KB72/Lly8/pORs1asSSJUtKhbfFixcTFBREXFwc4Apq7dq1Y+zYsaxatQofHx93zyNAixYt+Oc//8mvv/5K06ZNSy3a4Sk8/+OKi4jN+3hIKlZIEhERkarn66+/5tixY4wcObLU9X2cTid9+/Zl8uTJ7gUchg4dysSJE9myZQs//vije9+goCAeeeQRHnzwQZxOJ+3bt3fPaQoODmb48OGnPH69evWYNWsWffv2xWKx8OSTT5bqNWnYsCHdunVj1KhRvP3229hsNh5++GH8/Pzcw8O6detGmzZt6N+/Py+//DL169cnJSWFb775hgEDBpQZ4lcRNWvWxMfHhwkTJnD33Xezbt06nn322XN+3rPVvXt36tSpw/Dhw3n55ZfJysriiSeeADjjYgkZGRmsXr261Lbq1aszevRoxo8fz7333ss999zD5s2befrpp3nooYfw8vJi6dKlfP/99/To0YPIyEiWLl3KoUOHaNSoETt37uTdd9+lX79+xMbGsnnzZrZu3cqgQYMu1CmoMPUkVSL1JImIiEhVNmnSJLp163bSC6D269ePFStWsHbtWsC1yt2GDRuoUaMG7dq1K7Xvs88+y5NPPsmLL75Io0aN6NWrF9988w2JiYmnPf5rr71GWFgYbdu2pW/fvvTs2bPU/COAKVOmEBUVRceOHRkwYAB33nknQUFB+Pr6Aq5wMHfuXDp27Mhtt91G/fr1GTJkCLt37yYqKupcTo9bREQEH3zwAZ999hmNGzfmpZdeYty4cefluc+G1Wplzpw5ZGdn06pVK+644w4ef/xxAPf5OJWFCxfSokWLUrexY8dSo0YN5s6dy7Jly2jevDl33303I0eOdIev4OBgfv75Z3r37k39+vV54oknePXVV7nmmmvw9/dn06ZNXH/99dSvX59Ro0YxevRobrvttgt+Ls6WxTjbhfGrmMzMTEJCQsjIyCA4ONjUWq4Z/zMbD2Tx/q2Xc3XjGFNrkarD4XAwd+5cevfuXWbcscjpqO1IRajdVJ78/Hx27txJYmLiGd+wejqn00lmZibBwcF4eXnWZ/D79u0jPj6eBQsWeNy8FzMsXryY9u3bs23btlKLRJjlQrSd0/1ulTcbaLhdJfLxVk+SiIiIyIX0ww8/kJ2dTVJSEqmpqfzjH/+gVq1adOzY0ezSTDF79mwCAwOpV68e27Zt4/7776ddu3YeEZA8mUJSJfLRnCQRERGRC8rhcPCvf/2LHTt2EBQURNu2bZk6deol2zOalZXF//3f/7Fnzx7Cw8Pp1q0br776qtlleTyFpEpUMiepQD1JIiIiIhdEydLb4nLrrbdy6623ml1GleNZg0YvcnYNtxMRERER8XgKSZVIw+1ERERERDyfQlIl0hLgIiIiIiKeTyGpEpX0JGlOkoiIiIiI51JIqkSakyQiIiIi4vkUkiqR5iSJiIiIiHg+haRKpDlJIiIiIpemXbt2YbFYWL16NQALFy7EYrGQnp5+ysd88MEHhIaGnvOxz9fzXEoUkiqRj7cFUE+SiIiIVH1LlizBarXSp08fs0u5oA4ePIjNZuPTTz896f0jR47k8ssvP+vnbdu2LampqYSEhJxriaXUqlWL8ePHl9p24403smXLlvN6nJPp3LkzDzzwwAU/TmVQSKpEupisiIiIXCwmTZrEvffey88//0xKSsoFPZZhGBQVFV3QY5xKVFQUffr04f333y9zX05ODjNmzGDkyJFn/bw+Pj5ER0djsVjOR5mn5efnR2Rk5AU/zsVEIakS2W1WQMPtRERE5CQMAwpzzLkZxlmVmp2dzfTp0/nb3/5Gnz59+PDDD933DR06lBtvvLHU/g6Hg/DwcKZMmQKA0+nkxRdfJDExET8/P5o3b87MmTPd+5cMRZs3bx4tW7bEbrezaNEitm/fznXXXUdUVBSBgYG0atWKBQsWlDpWamoqffr0wc/Pj8TERKZNm1amdyU9PZ077riDiIgIgoODufrqq1mzZs0pX+/IkSP5/vvv2bNnT6ntn332GUVFRQwbNoxvv/2W9u3bExoaSvXq1bn22mvZvn37KZ/zZMPtPvjgA2rWrIm/vz8DBgzgyJEjpR5zptffuXNndu/ezYMPPojFYnEHsJMNt3v77bepU6cOPj4+NGjQgI8++qjU/RaLhffee48BAwbg7+9PvXr1+PLLL0/5esrj888/p0mTJtjtdmrVqsWrr75a6v633nqLevXq4evrS1RUFIMGDXLfN3PmTJKSkvDz86N69ep069aNnJycc6rndLwv2DNLGT7W48PtFJJERETkrxy58EKsOcf+Vwr4BJR79xkzZtCwYUMaNGjAzTffzAMPPMDo0aMBGDZsGIMHDyY7O5vAwEAA5s+fT25uLgMGDADgxRdf5OOPP2bixInUq1ePn3/+mZtvvpmIiAg6derkPs5jjz3GuHHjqF27NmFhYezdu5fevXvz/PPPY7fbmTJlCn379mXz5s3UrFkTgFtvvZXDhw+zcOFCbDYbDz30EGlpaaXqHzx4MH5+fsybN4+QkBDeeecdunbtypYtW6hWrVqZ19u7d2+ioqL44IMPeOqpp9zbJ0+ezMCBAwkNDSUnJ4eHHnqIZs2akZ2dzVNPPcWAAQNYvXo1Xl5n7pdYunQpI0eO5MUXX6R///58++23PP3006X2yc7OPu3rnzVrFs2bN2fUqFHceeedpzzW7Nmzuf/++xk/fjzdunXj66+/5rbbbiMuLo4uXbq49xs7diwvv/wyr7zyChMmTGDYsGHs3r37pOfoTH7//XduuOEGxowZw4033sivv/7K6NGjCQsLY+DAgaxYsYL77ruPjz76iLZt23L06FF++eUXwBV8b7rpJl5++WUGDBhAVlYWv/zyC8ZZhvuzoZBUibS6nYiIiFwMJk2axM033wxAr169yMjIYPHixfTu3ZuePXsSEBDA7NmzueWWWwCYNm0a/fr1IygoiIKCAl544QUWLFhAmzZtAKhduzaLFi3inXfeKRWSnnnmGbp37+7+vlq1ajRv3tz9/bPPPsvs2bP58ssvueeee9i0aRMLFixg+fLlXHHFFQC899571KtXz/2YRYsWsWzZMtLS0rDb7QCMGzeOOXPmMHPmTEaNGlXm9VqtVoYPH84HH3zAk08+icViYfv27fzyyy8kJycDcP3115d6zPvvv09ERAQbNmygadOmZzynr7/+Or169eIf//gHAPXr1+fXX3/l22+/de/TvHnz077+atWqYbVaCQoKIjo6+pTHGjduHCNGjHAH24ceeojffvuNcePGlQpJI0aM4KabbgLghRde4I033mDZsmX06tXrjK/nr1577TW6du3Kk08+6X59GzZs4NVXX2XgwIHs2bOHgIAArr32WoKCgkhISKBFixaAKyQVFRUxcOBAEhISAEhKSjrrGs6GQlIl0pwkEREROSWbv6tHx6xjl9PmzZtZtmwZs2fPBsDb25sbbriBjz76iN69e7u/nzp1Krfccgs5OTl88cUX7oUPtm3bRm5ubqnwA1BYWOh+U1yiJOiUyM7OZsyYMXzzzTfuN855eXnuYXCbN2/G29u71EIKdevWJSwszP39mjVryM7Opnr16qWeOy8v77TD426//XZeeuklfvzxR66++momT55MrVq1uPrqqwHYunUrTz31FEuXLuXw4cM4na73e3v27ClXSNq4caO7p61EmzZtSoWkM73+8tq4cWOZMNiuXTtef/31UtuaNWvm/jogIIDg4OAyvXJnc8zrrruuzDHHjx9PcXEx3bt3JyEhgdq1a9OrVy969erlHurXvHlzunbtSlJSEj179qRHjx4MGjSo1M/1fFNIqkQ+upisiIiInIrFclZD3swyadIkioqKiI39c2igYRjY7XYyMjIICwtj2LBhdOrUibS0NJKTk/Hz83P3PmRnZwPwzTffUKNGjVLPXdKzUyIgoPT5eOSRR0hOTmbcuHHUrVsXPz8/Bg0aRGFhYbnrz87OJiYmhoULF5a573TLZNerV48OHTowefJkOnfuzJQpU7jzzjvd83769u1LQkIC//vf/4iNjcXpdNK0adOzqu1MzsfrPxs2m63U9xaLxR3+zregoCBWrlzJwoUL+e6773jqqacYM2YMy5cvJzQ0lOTkZH799Ve+++47JkyYwOOPP87SpUtJTEy8IPVo4YZKpOF2IiIiUpUVFRUxZcoUXn31VVavXu2+rVq1iujoaD755BPAtbx1fHw806dPZ+rUqQwePNj9hrtx48bY7Xb27NlD3bp1S93i4+NPe/zFixczYsQIBgwYQFJSEtHR0ezatct9f4MGDSgqKmLVqlXubdu2bePYsWPu7y+//HIOHDiAt7d3meOHh4ef9vgjR47k888/5/PPP2f//v2MGDECgCNHjrB582aeeOIJunbtSqNGjUodszwaNWrE0qVLS2377bffzur1g2vVvOLi4jMea/HixWWeu3HjxmdV89k41THr16+P1epa3Mzb25tu3brx8ssvs3btWnbt2sUPP/wAuAJau3btGDt2LKtWrcLHx8fdm3khqCepEv3Zk3ThJpmJiIiIXChff/01x44dY+TIkaWu7+N0Ounbty+TJ092z3MZOnQoEydOZMuWLfz444/ufYOCgnjkkUd48MEHcTqdtG/f3j2nKTg4mOHDh5/y+PXq1WPWrFn07dsXi8XCk08+Wapno2HDhnTr1o1Ro0bx9ttvY7PZePjhh/Hz83P3+HTr1o02bdrQv39/Xn75ZerXr09KSgrffPMNAwYMKDPE70SDBw/mvvvu46677qJHjx7uUBcWFkb16tV59913iYmJYc+ePTz22GNndW7vu+8+2rVrx7hx47juuuuYP39+qaF25Xn94LpO0s8//8yQIUOw2+0nDX6PPvooN9xwAy1atKBbt2589dVXzJo1q8xKgRVx6NAh9wVzS8TExPDwww/TqlUrnn32WW688UaWLFnCm2++yZtvvgm42tauXbvo2LEjYWFhzJ07F6fTSYMGDVi6dCnff/89PXr0IDIykqVLl3Lo0CEaNWp0zvWeknGRy8jIMAAjIyPD7FKMXzYfMBL+72ujyys/ml2KVCGFhYXGnDlzjMLCQrNLkSpGbUcqQu2m8uTl5RkbNmww8vLyzC6l3K699lqjd+/eZbYXFxcbCxYsMABjzZo1hmEYxoYNGwzASEhIMJxOZ6n9nU6nMX78eKNBgwaGzWYzIiIijJ49exo//fSTYRiG8eOPPxqAcezYsVKP27lzp9GlSxfDz8/PiI+PN958802jU6dOxv333+/eJyUlxbjmmmsMu91uJCQkGNOmTTMiIyONiRMnuvfJzMw07r33XiM2Ntaw2WxGfHy8MWzYMGPPnj1nPAejRo0yAGPGjBmlticnJxuNGjUy7Ha70axZM2PhwoUGYMyePdtdO2CsWrXqlK9x0qRJRlxcnOHn52f07dvXGDdunBESEnJWr3/JkiVGs2bNDLvdbpS81Z88eXKp5zEMw3jrrbeM2rVrGzabzahfv74xZcqUUvefWHuJkJAQY/Lkyac8N506dTKAMrdnn33WMAzDmDlzptG4cWPDZrMZNWvWNF555RWjuLjYOHbsmPHTTz8ZnTp1MsLCwgw/Pz+jWbNmxvTp0w3DcLWlnj17GhEREYbdbjfq169vTJgw4ZR1nO53q7zZwHL8JFy0MjMzCQkJISMjg+DgYFNrWbbjEDe8u4y4UF8WPdbV1Fqk6nA4HMydO5fevXuXGRsscjpqO1IRajeVJz8/n507d5KYmIivr6/Z5ZwTp9NJZmYmwcHB5VruujLt27eP+Ph4FixYQNeuev/laS5E2znd71Z5s4GG21Uiu3tO0kWdS0VERERM88MPP5CdnU1SUhKpqan84x//oFatWnTs2NHs0qQKUUiqRCVLgGt1OxEREZELw+Fw8K9//YsdO3YQFBRE27ZtmTp1qnpG5awoJFUirW4nIiIicmH17NmTnj17ml2GVHGeNWj0IlcSknQxWRERERERz6WQVIlK5iQVOw2KnZqXJCIiIq4LsYrI+XM+fqcUkipRyZwk0LwkERGRS13JHJnc3FyTKxG5uJT8Tp3LPDTNSapEJcPtwBWS/HysJlYjIiIiZrJarYSGhpKWlgaAv7+/+4KnVY3T6aSwsJD8/HyPWwJcPNv5bDuGYZCbm0taWhqhoaFYrRV/r62QVIm8vSxYMDCwUFBcDGiVFRERkUtZdHQ0gDsoVVWGYZCXl4efn1+VDXpijgvRdkJDQ92/WxWlkFSJLBYL3l7gcEKBQ8PtRERELnUWi4WYmBgiIyNxOBxml1NhDoeDn3/+mY4dO2qpbTkr57vt2Gy2c+pBKqGQVMm8LeBAy4CLiIjIn6xW63l5Y2cWq9VKUVERvr6+CklyVjy17Zg6aPTnn3+mb9++xMbGYrFYmDNnTqn7DcPgqaeeIiYmBj8/P7p168bWrVvNKfY8KZmWpIUbREREREQ8k6khKScnh+bNm/Pf//73pPe//PLLvPHGG0ycOJGlS5cSEBBAz549yc/Pr+RKzx/v40MtFZJERERERDyTqcPtrrnmGq655pqT3mcYBuPHj+eJJ57guuuuA2DKlClERUUxZ84chgwZUpmlnjclPUm6oKyIiIiIiGfy2DlJO3fu5MCBA3Tr1s29LSQkhCuvvJIlS5acMiQVFBRQUFDg/j4zMxNwTQoze0Kkw+Fwh6TcgkLT65GqoaSdqL3I2VLbkYpQu5GKULuRiqrstlPe43hsSDpw4AAAUVFRpbZHRUW57zuZF198kbFjx5bZ/t133+Hv739+i6wAb4trUuavvy0jY7OusC3ll5ycbHYJUkWp7UhFqN1IRajdSEVVVtsp78WbPTYkVdQ///lPHnroIff3mZmZxMfH06NHD4KDg02szJVcx6/7AYBml11OzyZRZ3iEiKvdJCcn0717d49a9UU8n9qOVITajVSE2o1UVGW3nZJRZmfisSGp5AJQBw8eJCYmxr394MGDXHbZZad8nN1ux263l9lus9k84pfW22IAFoqxeEQ9UnV4ShuWqkdtRypC7UYqQu1GKqqy2k55j2Hq6nank5iYSHR0NN9//717W2ZmJkuXLqVNmzYmVnZubFq4QURERETEo5nak5Sdnc22bdvc3+/cuZPVq1dTrVo1atasyQMPPMBzzz1HvXr1SExM5MknnyQ2Npb+/fubV/Q50nWSREREREQ8m6khacWKFXTp0sX9fclcouHDh/PBBx/wj3/8g5ycHEaNGkV6ejrt27fn22+/xdfX16ySz5mukyQiIiIi4tlMDUmdO3fGME69wpvFYuGZZ57hmWeeqcSqLix3T1KxQpKIiIiIiCfy2DlJF6uSnqQCh0KSiIiIiIgnUkiqZH/2JBWbW4iIiIiIiJyUQlIl08INIiIiIiKeTSGpkmnhBhERERERz6aQVMm8vVwLVeg6SSIiIiIinkkhqZLZNNxORERERMSjKSRVMvfqdloCXERERETEIykkVTIt3CAiIiIi4tkUkiqZFm4QEREREfFsCkmVrKQnqaBI10kSEREREfFECkmVTMPtREREREQ8m0JSJXMPt9PCDSIiIiIiHkkhqZKpJ0lERERExLMpJFUyb4suJisiIiIi4skUkiqZLiYrIiIiIuLZFJIqmVUhSURERETEoykkVTJdJ0lERERExLMpJFWyP6+TpJAkIiIiIuKJFJIqmXtOUrETwzDMLUZERERERMpQSKpkJcPtQNdKEhERERHxRApJlcz7hDOueUkiIiIiIp5HIamSWU/sSVJIEhERERHxOApJlczLArbjSUmLN4iIiIiIeB6FJBP4HB9zp54kERERERHPo5BkAp/jV5TVwg0iIiIiIp5HIckE6kkSEREREfFcCkkmKOlJKigqNrkSERERERH5K4UkE9i9S0KSepJERERERDyNQpIJNNxORERERMRzKSSZQCFJRERERMRzKSSZQKvbiYiIiIh4LoUkE5T0JBU4FJJERERERDyNQpIJShZuUE+SiIiIiIjnUUgygXu4neYkiYiIiIh4HIUkE2jhBhERERERz6WQZAL3nCRdTFZERERExOMoJJnArp4kERERERGPpZBkgpI5SQVauEFERERExOMoJJlAc5JERERERDyXQpIJ3D1JCkkiIiIiIh5HIckE6kkSEREREfFcCkkmUEgSEREREfFcCkkmUEgSEREREfFcCkkmKJmTVKjV7UREREREPI5Ckgl0MVkREREREc+lkGQCXUxWRERERMRzKSSZQHOSREREREQ8l0KSCXSdJBERERERz6WQZAIfbwugniQREREREU+kkGQCu7cVUE+SiIiIiIgnUkgygZYAFxERERHxXApJJtDCDSIiIiIinkshyQSakyQiIiIi4rkUkkzw5+p2upisiIiIiIinUUgyQcnCDU4DijQvSURERETEoygkmaBkuB1o8QYREREREU+jkGSCkuF2oHlJIiIiIiKeRiHJBN5WL7yOdybpWkkiIiIiIp5FIckkJfOS1JMkIiIiIuJZFJJMUnKtJPUkiYiIiIh4FoUkk+iCsiIiIiIinkkhySQlizdodTsREREREc+ikGQSe8lwO4cuKCsiIiIi4kkUkkziHm6nniQREREREY+ikGQSu+YkiYiIiIh4JIUkk2jhBhERERERz6SQZBItAS4iIiIi4pkUkkyii8mKiIiIiHgmhSSTlCwBXqCFG0REREREPIpCkkk0J0lERERExDMpJJnkzzlJuk6SiIiIiIgnUUgyiXqSREREREQ8k0KSSXSdJBERERERz6SQZBL1JImIiIiIeCaFJJPYj69uV6jV7UREREREPIrHh6SsrCweeOABEhIS8PPzo23btixfvtzsss6Ze+EGh0KSiIiIiIgn8fiQdMcdd5CcnMxHH33EH3/8QY8ePejWrRv79+83u7Rz4r6YrHqSREREREQ8ikeHpLy8PD7//HNefvllOnbsSN26dRkzZgx169bl7bffNru8c6I5SSIiIiIinsnb7AJOp6ioiOLiYnx9fUtt9/PzY9GiRSd9TEFBAQUFBe7vMzMzAXA4HDgcjgtXbDmUHN/hcGC1GADkO4pMr0s824ntRuRsqO1IRajdSEWo3UhFVXbbKe9xLIZhGBe4lnPStm1bfHx8mDZtGlFRUXzyyScMHz6cunXrsnnz5jL7jxkzhrFjx5bZPm3aNPz9/Suj5HJZdsjC1G1WGoY4+Vtj9SaJiIiIiFxoubm5DB06lIyMDIKDg0+5n8eHpO3bt3P77bfz888/Y7Vaufzyy6lfvz6///47GzduLLP/yXqS4uPjOXz48GlPRGVwOBwkJyfTvXt3kjcd4f4Za2ldK4ypI1uZWpd4thPbjc1mM7scqULUdqQi1G6kItRupKIqu+1kZmYSHh5+xpDk0cPtAOrUqcNPP/1ETk4OmZmZxMTEcOONN1K7du2T7m+327Hb7WW222w2j/mltdls+NldtTichsfUJZ7Nk9qwVC1qO1IRajdSEWo3UlGV1XbKewyPXrjhRAEBAcTExHDs2DHmz5/PddddZ3ZJ50QLN4iIiIiIeCaP70maP38+hmHQoEEDtm3bxqOPPkrDhg257bbbzC7tnCgkiYiIiIh4Jo/vScrIyODvf/87DRs25NZbb6V9+/bMnz+/ynfl2ksuJquQJCIiIiLiUTy+J+mGG27ghhtuMLuM8859MVmFJBERERERj+LxPUkXK/dwu2KFJBERERERT6KQZBIfq+YkiYiIiIh4IoUkk/i45yQVm1yJiIiIiIicSCHJJCULNziKDZxOj76er4iIiIjIJUUhySQlPUmgeUkiIiIiIp5EIckkCkkiIiIiIp5JIckkJQs3ABQ4FJJERERERDyFQpJJLBbLnyvcqSdJRERERMRjKCSZqGTxBi0DLiIiIiLiORSSTOSjkCQiIiIi4nEUkkykkCQiIiIi4nkUkkykC8qKiIiIiHgehSQTaU6SiIiIiIjnUUgykbsnSavbiYiIiIh4DIUkE7mXAFdPkoiIiIiIx1BIMtGfc5IUkkREREREPIVCkol8vK2AepJERERERDyJQpKJtHCDiIiIiIjnUUgy0Z/XSdIS4CIiIiIinkIhyUT2koUbtLqdiIiIiIjHUEgykXvhBodCkoiIiIiIp1BIMpF7TpJ6kkREREREPIZCkol8tHCDiIiIiIjHUUgyka6TJCIiIiLieRSSTORjdV0nSSFJRERERMRzKCSZyG7TcDsREREREU+jkGQiHy0BLiIiIiLicRSSTKSLyYqIiIiIeB6FJBNp4QYREREREc+jkGQiu5YAFxERERHxOApJJlJIEhERERHxPApJJnLPSdLCDSIiIiIiHkMhyUQl10lST5KIiIiIiOdQSDKRFm4QEREREfE8Ckkm0pwkERERERHPo5BkIvUkiYiIiIh4HoUkE+lisiIiIiIinkchyUQ+VvUkiYiIiIh4GoUkE9lPWALcMAyTqxEREREREVBIMpXd27UEuGFAkVMhSURERETEEygkmahkThJohTsREREREU+hkGQihSQREREREc+jkGQiq5cFq5cF0OINIiIiIiKeQiHJZLqgrIiIiIiIZ1FIMpn7WknFulaSiIiIiIgnUEgyma6VJCIiIiLiWRSSTFbSk6SQJCIiIiLiGRSSTKY5SSIiIiIinkUhyWQ+xy8oq5AkIiIiIuIZFJJM5qOeJBERERERj6KQZDK7tWR1O4UkERERERFPoJBksj8XbtAS4CIiIiIinkAhyWRauEFERERExLMoJJlMc5JERERERDyLQpLJdJ0kERERERHPopBkMh+rQpKIiIiIiCdRSDKZ3abhdiIiIiIinkQhyWQ+1uMXk9US4CIiIiIiHkEhyWRauEFERERExLMoJJlM10kSEREREfEsCkkm03WSREREREQ8i0KSyRSSREREREQ8i0KSydxzkrRwg4iIiIiIR1BIMlnJdZLUkyQiIiIi4hkUkkz258INCkkiIiIiIp5AIclkdm/XdZIUkkREREREPEOFQtLevXvZt2+f+/tly5bxwAMP8O677563wi4Vuk6SiIiIiIhnqVBIGjp0KD/++CMABw4coHv37ixbtozHH3+cZ5555rwWeLFTSBIRERER8SwVCknr1q2jdevWAMyYMYOmTZvy66+/MnXqVD744IPzWd9Fr2ThBl1MVkRERETEM1QoJDkcDux2OwALFiygX79+ADRs2JDU1NTzV90lQEuAi4iIiIh4lgqFpCZNmjBx4kR++eUXkpOT6dWrFwApKSlUr179vBZ4sdPFZEVEREREPEuFQtK///1v3nnnHTp37sxNN91E8+bNAfjyyy/dw/CkfBSSREREREQ8i3dFHtS5c2cOHz5MZmYmYWFh7u2jRo3C39//vBV3KdDCDSIiIiIinqVCPUl5eXkUFBS4A9Lu3bsZP348mzdvJjIy8rwWeLHTxWRFRERERDxLhULSddddx5QpUwBIT0/nyiuv5NVXX6V///68/fbb56244uJinnzySRITE/Hz86NOnTo8++yzGIZx3o5htpKLyRY5DZzOi+d1iYiIiIhUVRUKSStXrqRDhw4AzJw5k6ioKHbv3s2UKVN44403zltx//73v3n77bd588032bhxI//+9795+eWXmTBhwnk7htlKepJAK9yJiIiIiHiCCs1Jys3NJSgoCIDvvvuOgQMH4uXlxVVXXcXu3bvPW3G//vor1113HX369AGgVq1afPLJJyxbtuy8HcNsJddJAteQO1+b1cRqRERERESkQiGpbt26zJkzhwEDBjB//nwefPBBANLS0ggODj5vxbVt25Z3332XLVu2UL9+fdasWcOiRYt47bXXTvmYgoICCgoK3N9nZmYCrms7ORyO81ZbRZQcv1QdJwwdzMkrwL9CPxG5mJ203YiUg9qOVITajVSE2o1UVGW3nfIex2JUYILPzJkzGTp0KMXFxVx99dUkJycD8OKLL/Lzzz8zb968s33Kk3I6nfzrX//i5Zdfxmq1UlxczPPPP88///nPUz5mzJgxjB07tsz2adOmeezKe4/8ZsVhWHj68iKq2c2uRkRERETk4pSbm8vQoUPJyMg4bedOhUISwIEDB0hNTaV58+Z4ebmGjC1btozg4GAaNmxYsar/4tNPP+XRRx/llVdeoUmTJqxevZoHHniA1157jeHDh5/0MSfrSYqPj+fw4cPntZerIhwOB8nJyXTv3h2bzebefvnzP5CVX8R397cjMTzAxArFE52q3YicidqOVITajVSE2o1UVGW3nczMTMLDw88Ykio8uCs6Opro6Gj27dsHQFxc3Hm/kOyjjz7KY489xpAhQwBISkpi9+7dvPjii6cMSXa7Hbu9bHeMzWbzmF/av9Zi9/YiC3BavDymRvE8ntSGpWpR25GKULuRilC7kYqqrLZT3mNUaHU7p9PJM888Q0hICAkJCSQkJBAaGsqzzz6L03n+VmjLzc1191KVsFqt5/UYnqBk8YYCx8X1ukREREREqqIK9SQ9/vjjTJo0iZdeeol27doBsGjRIsaMGUN+fj7PP//8eSmub9++PP/889SsWZMmTZqwatUqXnvtNW6//fbz8vyeomQZcC0BLiIiIiJivgqFpA8//JD33nuPfv36ubc1a9aMGjVqMHr06PMWkiZMmMCTTz7J6NGjSUtLIzY2lrvuuounnnrqvDy/pyi5oGxhkUKSiIiIiIjZKhSSjh49etLFGRo2bMjRo0fPuagSQUFBjB8/nvHjx5+35/RE7p4khSQREREREdNVaE5S8+bNefPNN8tsf/PNN2nWrNk5F3WpKQlJBQpJIiIiIiKmq1BP0ssvv0yfPn1YsGABbdq0AWDJkiXs3buXuXPnntcCLwXuhRuKik2uREREREREKtST1KlTJ7Zs2cKAAQNIT08nPT2dgQMHsn79ej766KPzXeNFz27TcDsREREREU9R4eskxcbGllmgYc2aNUyaNIl33333nAu7lJT0JGl1OxERERER81WoJ0nOLy3cICIiIiLiORSSPIAWbhARERER8RwKSR7Arp4kERERERGPcVZzkgYOHHja+9PT08+llkuWLiYrIiIiIuI5ziokhYSEnPH+W2+99ZwKuhS55yRp4QYREREREdOdVUiaPHnyharjkuZe3U49SSIiIiIiptOcJA/w58INupisiIiIiIjZFJI8gF2r24mIiIiIeAyFJA+g6ySJiIiIiHgOhSQPoJAkIiIiIuI5FJI8QMnCDRpuJyIiIiJiPoUkD6CeJBERERERz6GQ5AHcF5PVdZJEREREREynkOQB7OpJEhERERHxGApJHkDXSRIRERER8RwKSR5Ac5JERERERDyHQpIH0HA7ERERERHPoZDkAdw9SVq4QURERETEdApJHkDXSRIRERER8RwKSR7gz4UbFJJERERERMymkOQB3NdJKnJiGIbJ1YiIiIiIXNoUkjxASU8SgKNYIUlERERExEwKSR7AfkJI0uINIiIiIiLmUkjyACULNwAUOHRBWRERERERMykkeQAvLwveXhZAPUkiIiIiImZTSPIQuqCsiIiIiIhnUEjyED4KSSIiIiIiHkEhyUPoWkkiIiIiIp5BIclDKCSJiIiIiHgGhSQPceIFZUVERERExDwKSR6iZBnwfC0BLiIiIiJiKoUkD1E7IgCAxdsOm1yJiIiIiMilTSHJQ1zbLBaAr9em4nQaJlcjIiIiInLpUkjyEJ0bRBDk682BzHyW7TpqdjkiIiIiIpcshSQP4Wuz0qtJNABfrkkxuRoRERERkUuXQpIH6XeZa8jd3D9StcqdiIiIiIhJFJI8SJva1QkPtJOe62DRtkNmlyMiIiIicklSSPIg3lYvrm0WA8CXqzXkTkRERETEDApJHqZvc9eQu+82HCSvUNdMEhERERGpbApJHubymqHEhfmRW1jMgo0HzS5HREREROSSo5DkYSwWC/2O9yZplTsRERERkcqnkOSBSla5+2nzITJyHSZXIyIiIiJyaVFI8kANo4NpEBVEYbGTb9enml2OiIiIiMglRSHJQ5X0JmnInYiIiIhI5VJI8lB9m7lC0pLtR0jLzDe5GhERERGRS4dCkoeqWd2fFjVDcRrw9VoNuRMRERERqSwKSR5Mq9yJiIiIiFQ+hSQP1qdZDF4WWL03nT1Hcs0uR0RERETkkqCQ5MEig3xpU6c6AF+tVW+SiIiIiEhlUEjycNc1rwHAF6v3m1yJiIiIiMilQSHJw/VsGo2P1YstB7PZmJppdjkiIiIiIhc9hSQPF+Jn4+qGkQC8MHcjhmGYXJGIiIiIyMVNIakKeLRXA+zeXvyy9TCfrdhndjkiIiIiIhc1haQqoE5EIA/3qA/As19vIDUjz+SKREREREQuXgpJVcTI9rW5LD6UrIIi/jXrDw27ExERERG5QBSSqgirl4VXBjXDx+rFj5sPMWulVrsTEREREbkQFJKqkHpRQdzfrR4AY79aT1pmvskViYiIiIhcfBSSqpi7OtYmqUYImflF/Gv2Og27ExERERE5zxSSqhhvqxfjBjfHZrWwYONBvlyTYnZJIiIiIiIXFYWkKqhBdBD3Xu0advf0l+s5lFVgckUiIiIiIhcPhaQq6m+d69A4Jpj0XAdPztGwOxERERGR80UhqYqyWb14ZXAzvL0sfLv+APPWHTC7JBERERGRi4JCUhXWJDaEv3WuA8BzX28gr7DY5IpERERERKo+haQqbnTnutQI9SMlI5+JP203uxwRERERkSpPIamK8/Ox8q/ejQCY+NN29qfnmVyRiIiIiEjVppB0EeidFM2VidUoKHLy4tyNZpcjIiIiIlKlKSRdBCwWC0/1bYyXBb5em8rSHUfMLklEREREpMpSSLpINIkNYUjrmgCM/WoDxU4tCS4iIiIiUhEKSReRR3o0INjXmw2pmUxfvtfsckREREREqiSFpItItQAfHuxeH4Bx320mI89hckUiIiIiIlWPQtJF5uarEqgXGcjRnELe+H6r2eWIiIiIiFQ5Hh+SatWqhcViKXP7+9//bnZpHslm9eKpvo0B+PDXXWxLyzK5IhERERGRqsXjQ9Ly5ctJTU1135KTkwEYPHiwyZV5rg71IujWKIoip8EzX2/EMLSIg4iIiIhIeXl8SIqIiCA6Otp9+/rrr6lTpw6dOnUyuzSP9kSfRvhYvfh5yyF+2JRmdjkiIiIiIlWGt9kFnI3CwkI+/vhjHnroISwWy0n3KSgooKCgwP19ZmYmAA6HA4fD3IUMSo5fGXXUCPFhRNuavPvLLl79bjMd6oSd8pyJZ6vMdiMXF7UdqQi1G6kItRupqMpuO+U9jsWoQmOxZsyYwdChQ9mzZw+xsbEn3WfMmDGMHTu2zPZp06bh7+9/oUv0KDkOGLPSSqHTwujGxTQIqTI/ahERERGR8y43N5ehQ4eSkZFBcHDwKferUiGpZ8+e+Pj48NVXX51yn5P1JMXHx3P48OHTnojK4HA4SE5Opnv37thstko55jPfbOKj3/bQvm51Jg9vWSnHlPPLjHYjFwe1HakItRupCLUbqajKbjuZmZmEh4efMSRVmeF2u3fvZsGCBcyaNeu0+9ntdux2e5ntNpvNY35pK7OWUR3rMHXpHhZtO8LWQ3k0jjU3KErFeVIblqpFbUcqQu1GKkLtRiqqstpOeY/h8Qs3lJg8eTKRkZH06dPH7FKqlPhq/vRp5hqa+O7P202uRkRERETE81WJkOR0Opk8eTLDhw/H27vKdH55jLs61gbgq7Wp7DuWa3I1IiIiIiKerUqEpAULFrBnzx5uv/12s0upkprWCKFd3eoUOw3eX7TL7HJERERERDxalQhJPXr0wDAM6tevb3YpVdaojnUA+HT5HjJytTyniIiIiMipVImQJOeuY71wGkYHkVtYzMdLd5tdjoiIiIiIx1JIukRYLBbu7uTqTZq8eBf5jmKTKxIRERER8UwKSZeQPs1iiA3x5XB2AbNX7Te7HBERERERj6SQdAmxWb0Y2cG10t3/ft6B01llriMsIiIiIlJpFJIuMUNaxRPs682OwzkkbzxodjkiIiIiIh5HIekSE2D35pY2CQC885MuLisiIiIi8lcKSZeg4W1r4WP1YuWedFbsOmp2OSIiIiIiHkUh6RIUGeTL9S1rAHDb5OU8PGMNCzen4Sh2mlyZiIiIiIj5vM0uQMxxz9X1WLrjKDsO5/D5yn18vnIfYf42rkmKoW+zWFonVsPqZTG7TBERERGRSqeQdImqEerHgoc68fueY3y1JoW5f6RyOLuQaUv3MG3pHiKD7DzQrT5Dr6xpdqkiIiIiIpVKIekS5uVloVWtarSqVY2nrm3MbzuO8tWaFOatSyUtq4DH5/xBrXB/2tYJN7tUEREREZFKozlJlSkzldhjS82u4qS8rV60rxfOvwc1Y8UT3RncMg7DgIemryE9t9Ds8kREREREKo1CUmUpyMb7vy1oteu/kL7H7GpOy8fbi7HXNaF2eAAHMvP556w/MAxdeFZERERELg0KSZXFHogR2xIAy86F5tZSDv4+3rw+pAU2q4V56w4wY8Ves0sSEREREakUCkmVyEjsBIDXjoXmFlJOSXEhPNyjAQBjvtzAjkPZJlckIiIiInLhKSRVIqN2FwAsu34GZ7HJ1ZTPqA61aVO7OnmOYh6YvprCIl1LSUREREQubgpJlciIbYHD6o8lPx1SV5tdTrl4eVl47cbmhPjZWLsvg/8s2GJ2SSIiIiIiF5RCUmXy8uZwYCPX19t/NLeWsxAT4se/r08CYOJP2/l1+2GTKxIRERERuXAUkipZWlAT1xdVKCQB9Goaw5BW8VoWXEREREQuegpJlexQsKtHhr1LoaBqLYTw5LWNSTy+LPj/fb4Wp1PLgouIiIjIxUchqZLl+ERihNQEpwN2/2p2OWclwO7N60Muw2a1MH/9QZ6fu1HXTxIRERGRi45CUmWzWNxLgbP9B3NrqYBmcaG8PKgZAJMW7eSdn3eYXJGIiIiIyPmlkGQC5/GlwNlRteYllRjQIo4n+rgWoHhp3iY+04VmRUREROQiopBkAqNWB8AChzZBZorZ5VTIHR1qc1fH2gA8NusPfth00OSKRERERETOD4UkM/iFQWwL19dVbJW7Ez12TUOuvzyOYqfB6Kkr+X33UbNLEhERERE5ZwpJZqlztevfKjrkDsBisfDS9Ul0aRBBvsPJ7R+sYOvBLLPLEhERERE5JwpJZqlTMi9pITidppZyLmxWL/477HJa1AwlI8/Bre8vIyU9z+yyREREREQqTCHJLHGtwRYAOYcgbb3Z1ZwTfx9v3h/eirqRgaRm5HPr+8s4lFVgdlkiIiIiIhWikGQWbx+o1c71dRVcCvyvwgJ8mHJ7a2JCfNmWls2Qd5eQlplvdlkiIiIiImdNIclMJfOSqvDiDSeKDfXjkzuvIjbEl+2Hcrjx3d9IzdDQOxERERGpWhSSzFRyvaQ9S8BxcYSJWuEBTL+rDXFhfuw8nMON7/zGvmO5ZpclIiIiIlJuCklmimgAQTFQlO8KSheJ+Gr+TL+rDTWr+bPnaC43vvMbe48qKImIiIhI1aCQZCaL5aIbcleiRqgf0++6isTwAPan53HjO0vYdTjH7LJERERERM5IIclsJUPuqvD1kk4lJsSPT0ddRZ2IAFIy8rnx3SVsP5RtdlkiIiIiIqelkGS22p1d/x74A7LTTC3lQogK9uXTUW2oFxnIwcwChrz7m66jJCIiIiIeTSHJbIEREJ3k+nrHT+bWcoFEBNn5dNRVNIgK4lBWAfd/uoqi4qp7AV0RERERubgpJHmCi3jIXYnqgXbevbUlgXZvlu86xuvfbzW7JBERERGRk1JI8gR1joek7T+CYZhbywWUUD2AFwa6es3e/HEbi7cdNrkiEREREZGyFJI8Qc024O0LWSmQssrsai6ofs1jGdIqHsOAB6av5lBWgdkliYiIiIiUopDkCWx+UK+H6+vP74C8dFPLudCe7tuE+lGBHMoq4KEZq3E6L97eMxERERGpehSSPEWf1yA4Do5uh89HgrPY7IouGD8fK28OvRxfmxe/bD3MOz/vMLskERERERE3hSRPERgBQ6aCtx9sWwDfjzW7oguqflQQY/o2AWDcd5v5ffcxkysSEREREXFRSPIksZfBdW+6vl78OqydYWo5F9qNreLp2zyWYqfBfZ+sIiPXYXZJIiIiIiIKSR4naRC0f9D19Zf3wv6V5tZzAVksFl4Y0JSa1fzZn57H/32+lmLNTxIRERERkykkeaKrn4R6PaEoH6bfDFkHza7oggnytfHm0BbYrBa+XX+AzuN+5L1fdpCRp14lERERETGHQpIn8rLC9f+D6vUgcz/MuAWKLt6lspvFhfLKoOaE+tvYezSP577ZSJsXv+epL9ax/VC22eWJiIiIyCVGIclT+YbATZ+APQT2LoW5j1zUF5rt36IGSx7ryosDk6gfFUhuYTFTluym66s/Mfz9ZSzcnIZxEb9+EREREfEcCkmeLLweDJoEWGDlFFjzqdkVXVB+PlZual2T+Q90ZOodV9KtUSQWC/y05RAjJi/n5klL2aGeJRERERG5wBSSPF297tDlcdfX3/4fZB0wt55KYLFYaFc3nPeGt+LHhztzW7ta2L29WLztCL3G/8J/kreQ77h4ryMlIiIiIuZSSKoK2j8IMZdBfgZ88/BFPezur2qFB/B03yZ892BHOtaPoLDYyevfb6XX+J/5Zeshs8sTERERkYuQQlJVYPV2XT/Jyxs2fQ0b5phdUaVLqB7Ah7e14r9DLycyyM6uI7ncMmkZ932yirSsfLPLExEREZGLiEJSVRGdBO0fcn0991HIPWpuPSawWCz0aRbD9w93YkTbWnhZ4Ms1KXR99Sd+2HTxLpMuIiIiIpVLIakq6fgIRDSCnEPw7WNmV2OaIF8bY/o14Yu/t6dZXAhZ+UXc/fFKft1+2OzSREREROQioJBUlXjb4br/gsUL1k6HLfPNrshUSXEhzPpbW3o0jqKwyMmdH65g9d50s8sSERERkSpOIamqiWsJV412ff3VA67FHC5h3lYvJgxtQfu64eQUFjNi8jI2H8gyuywRERERqcIUkqqiLo9DtdqQlQLJT5ldjens3lbeuaUlLWqGkp7r4JZJS9l9JMfsskRERESkilJIqop8/KHfm66vf/8AdvxkajmeIMDuzQcjWtMwOoi0rAJunrSUAxla9U5EREREzp5CUlVVqx1cMdL19Zf3XvLD7gBC/G1MGdmahOr+7D2axy2TlnI0p9DsskRERESkilFIqsq6jYHgOEjfDePqw6fDYO2MSzowRQb58vHIK4kO9mVrWjYjJi8jK99hdlkiIiIiUoUoJFVlvsEweLJrflJRvutCs7PuhFfqwrQbYfU0yDtmdpWVLr6aPx/f0ZpqAT6s3ZfBtRMW8dOWQ2aXJSIiIiJVhEJSVRffGu5dCXcvgo6PQvV6UFwIW76FOX9zBaYZt8LuX8EwzK620tSNDGLK7a2JCraz+0guw99fxt+nrtQ8JRERERE5I4Wki4HFAtFJcPUTcM9yGP0bdP4nRDYBZxFs+AImXwMTO8DKKeDIM7viStG0RggLHurE7e0S8bLAN3+k0vXVhUxatJOiYqfZ5YmIiIiIh1JIuthYLBDZCDo/BqN/hbsXw+XDwdsPDv7hWuThtUaupcOP7Ta72gsuyNfGU30b89W97WlRM5ScwmKe/XoD/d5czMo9l95QRBERERE5M4Wki110U+j3Bjy0Abo/C6E1XfOUFr8Ob1wGcx+Foot/BbgmsSF8fndbXhiQRIifjQ2pmQx861f+9vHvfL/xIA71LImIiIjIcQpJlwr/atDuPrhvNQz5BGp3BsMJy96FKf0gO83sCi84Ly8LQ6+syQ8Pd+L6y+MAmLfuACM/XEGbF7/n2a83sCEl0+QqRURERMRsCkmXGi8rNOwNt34BQ2eAPRj2LIF3O8P+lWZXVymqB9p59YbmzLu/AyPbJxIe6MPh7EImLdpJ7zd+4ZrXf+G9X3aQlqVFHkREREQuRQpJl7L6PeHOHyC8PmTudy3usGa62VVVmkYxwTx5bWOW/LMrk4ZfQe+kaHysXmxMzeS5bzZy5Qvfc/3bv/Luz9vZfSTH7HJFREREpJJ4m12AmCy8HtyxAGbdBVvmwexRcGAtdBsL1kujedisXnRtFEXXRlGk5xby1dpUZq3cx6o96fy++xi/7z7GC3M30TA6iB5NounZJIrGMcFYLBazSxcRERGRC+DSeBcsp+cbAkOmwcIX4OdXYMmbcHAdDJrsmst0CQn19+GWqxK45aoEUjPySN5wkPnrD/DbjqNsOpDFpgNZvPH9VlrUDOWDEa0J8beZXbKIiIiInGcabicuXl6u6yzdMAVsAbBjIYxvBl8/CAf+MLs6U8SE+HFrm1pMveMqfn+iG68Obk6PxlHYvb1YtSed2z9cTm5hkdllioiIiMh5ppAkpTW+Du5IhohGUJgFK96Hie3hf11h1VQozDW7QlOE+vtwfcs43r31Cr68pz3Bvt78vvsYf/t4JYVFWj5cRERE5GKikCRlRTWB0Uvg1i+hyQDw8ob9K+CL0fBaQ5j3f3B0p9lVmqZBdBCTb2uNn83KT1sO8chna3A6DbPLEhEREZHzRCFJTs5igdqdYPAH8NBG6Po0hCZAfgYsnQjvdIIj282u0jQtE8J4++bL8fay8OWaFMZ+tR7DUFASERERuRgoJMmZBUZCh4dcF6K9+XOIbgYFGTDj1kt2+B1A5waRvHpDcywW+HDJbl7/fqvZJYmIiIjIeeDxIWn//v3cfPPNVK9eHT8/P5KSklixYoXZZV2avLygbjcYOh0CIlwr4H3zMFzCPSjXXVaDZ/o1AWD8gq18+OuuU+5rGIaG5YmIiIhUAR69BPixY8do164dXbp0Yd68eURERLB161bCwsLMLu3SFhwLg96HKdfBmmlQ80poOcLsqkxzS5taHM1x8J8FW3j6y/XkFhYT6OtNanoeBzLySclw/ZuakY+vzcrfu9RhRNtEfLw9/jMKERERkUuSR4ekf//738THxzN58mT3tsTERBMrErfEjtD1KVgwBuY+6hqCV+Nys6syzX1d63I0p4APl+zm399uOuV+BUVOXpi7ienL9zK2X1Pa1wuvxCpFREREpDw8OiR9+eWX9OzZk8GDB/PTTz9Ro0YNRo8ezZ133nnKxxQUFFBQUOD+PjMzEwCHw4HD4bjgNZ9OyfHNruO8af13rHuW4rVlHsaMWyka+QP4Xbq9fP/qVR8LBr/vSScqyE5MiC9Rwb7EhPgSHWInOtiXZbuO8cp3W9h+KIebJy2lV5Mo/tmrPrGhfid9zuyCIn7feYRfD1q47Gg2sdUCK/lVSVV20f3NkUqhdiMVoXYjFVXZbae8x7EYHrwkl6+vLwAPPfQQgwcPZvny5dx///1MnDiR4cOHn/QxY8aMYezYsWW2T5s2DX9//wta76XIuyiHTpufJrAwjYPBzfit9kNg0TCy08ktgnl7vfjlgAUDCzYvgx41nFwda5DtgJ1ZFnYcv+3PAQMLADYvg07RBl1rOPH36I83RERERDxTbm4uQ4cOJSMjg+Dg4FPu59EhycfHhyuuuIJff/3Vve2+++5j+fLlLFmy5KSPOVlPUnx8PIcPHz7tiagMDoeD5ORkunfvjs1mM7WW8+rgOrw/6IWlKJ/iTv/E2f5hsyuqEjYdyGLs1xtZsTsdAH8fK7mFxWX2qxHqC4V57M91haVgX2/ubF+LW9vUxN9HaUlO7aL9myMXlNqNVITajVRUZbedzMxMwsPDzxiSPPodVkxMDI0bNy61rVGjRnz++eenfIzdbsdut5fZbrPZPOaX1pNqOS/iWkCf1+CL0Vh/eglrzdZQ52qzq/J4SfHV+Ozutny5JoXnv9lIWlYBXhZoFBNMq1rVuKJWGFckVKO6v5VvvpmLb+0r+M/329l8MItXF2zjw9/2cl/XugxpVVOLQMhpXXR/c6RSqN1IRajdSEVVVtsp7zE8OiS1a9eOzZs3l9q2ZcsWEhISTKpITqnFMNi7FFZ+CB8PgqgmEN8a4lpD3BVQrbbrArVSisVi4brLatC9cRRbD2ZTOyKAIN/Sv7wOhwOLBbo2iqR701i+XLOf15K3sPdoHk99sZ7//bKD29omMqBFDcICfEx6JSIiIiIXD48OSQ8++CBt27blhRde4IYbbmDZsmW8++67vPvuu2aXJidzzcuQvgd2/AgH1rpuy99z3ecfDnGtILweOIugqMB1Ky6AonwoKoTACOjyBATHmPs6TODv403z+NAz7mf1sjCgRRx9kmKZvnwPb/ywjb1H83jm6w28NG8T3ZtEccMV8bSvG47VS6FUREREpCI8OiS1atWK2bNn889//pNnnnmGxMRExo8fz7Bhw8wuTU7G5gu3zoGMfbBvOexdDvuWQeoayD0MW+a5bqezbwWM+AYCtDT26fh4e3FLm1pc3zKOmb/vY/ryvaxPyeSbtal8szaV2BBfBrWMY/AV8YT620jLKiAts4C0rHwOZRWQllXA0ZxCOtaPoF/zWLNfjoiIiIhH8eiQBHDttddy7bXXml2GnI2QONetyQDX90UFkLrWFZgy9oG3Hax2178lNy9vWPhvOLQJPhoAw78Cv1BTX0ZV4O/jza1tanFrm1qs25/BZyv2Mmd1CikZ+bzxwzbe+GHbaR8/8/d9rNh1lCevbYzNqnlNIiIiIlAFQpJcBLztEN/KdTudhHYw+RrXML2pg+GW2WDXdYHKq2mNEJrWCOGfvRvx3YaDfLZiL4u2HcYwINDuTWSQnYggO5HBvkQG2SkoKubj3/YwZclutqVl89+hl2tOk4iIiAgKSeJJwuvBLXPggz6uXqdPhsCwz8B28gutysn52qz0ax5Lv+axZOY78PaynHKp8I71Inhw+mp+3X6E/m8t5r1br6BeVFAlVywiIiLiWTS+RjxLdFO4eRb4BMKuX2DGcNeiDlIhwb62015LqUeTaD4f3Za4MD92H8llwFu/8sOmg6fcPy0zn2/XpfLrtsN48CXWRERERM6JepLE88S1hKEz4OPrYet8mHUnXD8JrGquF0LD6GC+vKc9f/v4d5buPMrID1fwf70ackf7RDYfzGLl7mOs2H2M33cfY9+xPPfjWteqxhPXNqJZXKh5xYuIiIhcAHrXKZ6pVjsY8jFMGwIb5oDNH/q+Dt6aM3MhVAvw4aORVzLmq/VMW7qHl+Zt4j/JWygocpbaz8sC9aOC2HUkh2W7jtLvzcUMaFGDR3s2IDZUwyJFRETk4qCQJJ6rbjcYPNk15G7NNNg8Fxr3g6bXQ60O4GU1u8KLio+3F8/3b0rD6CDGfrWBgiIngXZvWtQMpWVCGC0TwrgsPpQgXxsp6XmMm7+ZWav2M3vVfuatS2VUh9rc1akOAfayf1Yy8hykpOdRVGzQODZY13ASERERj6aQJJ6tUV8Y9D7M+wdkH4SVU1y3gEho0h+aDIT4K8FL0+vOB4vFwq1tatGzSTTHcgupFxl00kATG+rHazdexoh2tXju640s23WUN37YxifL93LDFXGk57pCUUp6PinpeWQVFLkfGxFkp3fTaK5tHkvLmmF4KTCJiIiIh1FIEs/XpL8rLO1eDOs+hw1fQE4aLHvXdQuOg06PQotbFZbOk6hgX6KCfc+4X7O4UKbfdRXz1x/gxXmb2H0kl//+uP2k+4b52ygqNjiUVcCHS3bz4ZLdRAXb6Z0Uw7XNYmgRr8AkIiIinkEhSaoGLyskdnTdeo+DHQtdgWnTN5C5D766H9ZMd81biqhvdrWXFIvFQq+mMXRpGMknS/ewPiWT6BBfYkP9qBHqR2yoH7Ghvvj7eFNY5GTxtsN8vTaV7zYc4GBmAZMX72Ly4l1EBdtpHhdKo5hgGsUE0yQ2mLgwPyyWswtORcVONh3IYuWeY6zZm0Gg3UrtiEBqRwRQJyKQ6GBfhTERERE5LYUkqXqsNqjX3XVz5MOKSfDD87DnV5jYDjo8DO0fdF3EViqN3dvKiHaJp93Hx9uLLg0j6dIwkoKipizaephv1qby3YaDHMws4LsNB/luw59LkAfZvWkYE0TD6GAiguyE+tsI9fch1M9GmL8Pof42fG1WNqZmsmL3MVbuPsaqPcfIKSw+ZQ1+NiuJ4QHUiQykVa0w+iTFUD1QbUVERET+pJAkVZvNF9r83TUc75uHYet3sPBFWDfL1auU0MbsCuUU7N5WujaKomujKPIdxazcc4yNqVlsTM1kY2omWw9mk1VQxPJdx1i+69hZPXeQ3ZsWCWG0iA8lv6iYHYdy2H4omz1HcslzFLMhNZMNqZl8tSaFsV9toGO9cPq3qEH3xlGnva5UidzCIvYfy2Nfeh77j+Wx//i/+47lYvWy0O+yGlx3WSzBvraKnh43wzDYdyyPsAAfAk+yKMaFkJaVj9MJ0SFnHnJZXoZhkFVQdF7OiYiIyIWmkCQXh9CarmsrrZ8N8/4PDm+Gyb2g5Qio2QbyM6Eg4/i/ma5/C7MhNAESO0BCewiobvaruGT52qy0rRNO2zrh7m2OYifbD2W7A9Ox3EKO5ThIzyskPdfhuuUVku9wEl/Nj5Y1w2hZqxpXJIRRP+rkC044ip3sPZrLjkM5bD6Yxfz1B1i7L4MfNx/ix82H8Pex0qtJNNe1qEGdiAD2Hs1j79Fc9pxw23s0lyM5p7/A8fJdx3jhm430ax7LTVfWpHlcyFkPG8x3FPPF6v1MXryLTQeyAIgKtlM7PJA6kQHH/w2kTkQANULPfljiXxUWOVmw8SCfLt/LL1sPYRjQqlYY118eR+9mMecUbjakZPL0l+tYvusY3RpFcu/V9WgeH3pO9YqIiFxICkly8bBYoOlAqNMFkp9yrYL3+weu2+ks/5/r38gmrsBUqwMktAX/ahe6YjkNm9WLhtHBNIwOPu1+hUVOfLzLt2CHzep1fH5SIN0aR/H3LnXZfiibL1btZ87qFPYczWXWqv3MWrX/jM8VZPemRphr3lVcmN/xr/05kJnPJ8v2sC0tm+kr9jJ9xV4axwRz05U1y9W7lJKex0e/7eaTZXtIz3UAYPWyUOw0OJhZwMHMApbsOFLqMdUCfLi8pmuZ9itqhZFUIwRfW/mWyN96MIvpy/cya9V+jp4Q/iwW3L14T3+5nh5Nohl4eQ061A3H21q+852Z7+C177YwZckunIZr24KNaSzYmEbH+hHcd3Vdrqh1br9nOQVFWL0sZ3y9aZn5rEvJYP3+TNalZLAhNZNqAXZuahVPv8tiy9WDKCIilw6LYRiG2UVcSJmZmYSEhJCRkUFw8OnfbF1oDoeDuXPn0rt3b2w2DTm54HYtgsWvQ7EDfIPBHgy+Icf/DQabHxxY59rv0Ma/PNgCcVdAkwHQuD+E1DDjFQBqN5XFMAxW7knni9X7+XptKln5DuLC/Imv5k/Nan7Eh/lTs5rr+/gwf0L8T/2zMAyDFbuP8cnSPXz9RyqFxy/K6+1lIbYkVIX6ERfm7w5YxU6DqUt3M3/9QYqPJ4q4MD+Gt6nFDVfEgwV2HMp2Dx3cfvzrXUdycBSX/jNus1poWiOEFnEhHN63g4aNGmL1snLiXkXFTn7cfIjfd/85lDEyyM7gK+K44Yp47N5W5qzez+e/72NrWrZ7n4ggO32SYmhXN5zWtaqd9DwYhsHsVft5Ye4mDmcXANAnKYZb2yQwfcVevlid4n6NbWpX576u9biqdrXT9oYVFTvZdSSXTQcy2ZSaxaYDmWxMzWJ/eh7gmmsWVjJnzd81Zy3Yz8aBjDzWpWRyKKvglM8d5OvNoJZx3HxVAnUiAk+536WiMv/mHM4uYGNqJvuP5dGubjjx1fwv6PHM4HQazF9/gAUb02gcG0zfZjFElmP10KrmQrQbp9NgQ2omkUH2i/KciUtlv88pbzZQSKpEerPrwbIPwa5fXIFp1y9weEvp++OvcvVSNb4OgqIrtTS1m8pnGAaGwXlZBS89t5BZK/cz7XjvUnm0rVOdEW1r0bVR1BkvvFtY5GRdSgYrdx9jxa5jrNh9zB1MysPqZeHqhpEMaRVPp/oRZXqJDMNg3f5MPl+5jy9W7+fY8d4tcPU2NY4J5srE6lxVuxqtE6txIDOfp+asZ9muowDUDg9g7HVN6FAvwv24PUdyefunbcz8fZ874F0WH0pMiC+OYgNHsZMipxNHkYHD6SSvsJidh3MoOB42K8LLAnUiAmlaI4QmscE0jglmfUomHy/dze4jue792tapzi1XJdCtcRS2cvaYnY6j2LWi45erU1izL50msSG0rxtOu3rh1Aj1O+fnLzlGWlYBBzLyOJBRQGpGHgcy8jmQmU9kkC+dG0RwZe1q2L3L17t4qr85hUVOlu08yqYDmTgNg2Inx/913UreTvj5eBNotxJg9ybA7k3g8X+tFgvbDmWdMPcwq1Rb9bF6cWubBO65ui6h/j7n5dyYqdhpMPePVCb8sJUtB//83bdY4KrE6vS7LJZrmkaf1WvNyHWw64jrw5Fdh3PZfTSHav4+XFW7Oq0SqxHid+H/j8h3FHMkp5Aj2QUczi7gcHYhR7ILOZSZx6ZtO+nTpikdG0RVOPAahsH6lEy+XJPCl6tTOJCZj7eXhT7NYritXSKXaajuGRUWOdlzNJe0zHwy8x1k5DnIzCs64WsHvjYr/S6L5arE6qav+KqQZBKFJKmQzFTY+BWsnwV7lpxwhwUS2rmu3dSgd6X0MKndXBwMwyA1I599x/LYn57LvqN5x792LfiQlV9EjyZRDG9b64xDDM90nL1H81ix+ygrdx9l847dxNWogcXLCwuu/whLOmzqRgYysEWNcn9CW1jkZOHmNBZuOcRvO46w41BOqfstFrAATsPVs3Nv17qMbJ94yjfn+9PzeOen7Xy6fK+7t+10/H2sNIh2rXbY6Piqhw2igrB4QXqOwzVvLbeQjDwHx3IKSc9zUC3Ah6Y1QmgUHYyfT9k6nE6DX7Yd5qMlu/lh00H3sMCoYDs3ta7J0NY1z/oTbFev5DG+WJ3CN2tTTzmHLTE8gHZ1q9O+bjitE6vjNAzScws5muPgaE6h6+tc1xy8rHwH2QXF5BQUkZ1fRHZBETmFrq+P5hZypv/J/WxW2tUNp0vDCLo0iCT2NAHtxL852YUGP25O4/uNafy85VCpC0OfDxYL1KoeQJCvN2v3ZQAQ4mfjni51ubVtQrmDHbjO+5GcQralZbMtzdXbWlDkJNTPRshfb/42guw27DYvfKxe2G1e2L2tpT6UKCxykp5XSEaug/S84/MgcwspdhrUiQykfmTQSXtSi50GX69NYcIP29wfjATZvRlweQ3W7c9g5Z509742q4WO9SLo0yyGED8bWflFZOU7yMx3vaHNyi8iM8/BvmN57DqS4x6Ce6pz2SQ2mKsSq9Omjis0Bdm9ycwv4kBGPinHw3NqRj6p6Xn4+Vjp2SSaKxOrnXEIbU5BEd/8kcqM5XtZsbt8C+nUrOZPu7rVaVc3nDa1q59xFdGdh3P4cnUKX67Zz/YT/rb42rzId/z596FlQhi3t0ukZ5Oocg/9LVFU7OSnLYeYvnwve47mEhfmT63q/tQKD6BW9QBqhfsTE+J3xg+nzlZ2QRGbD2RSN+LkbaYiDMNgf3oe2w/lsOtwDjtPuO07luv+W3YmCdX9ubFVPINaxhEZdOF66xzFToqKjZP+HVZIMolCkpyzjP2wYY5rUYh9y0vfF9sCGvaBhtdCRMM/332eTEGWa8nygPDT7/cXajdSURe67aRl5vPbzqP8tuMIS3cccb+xuaZpNE9c27jcPSUHM/NZsPEgTqeBzeqFt9ULm9Xi+trLgo+3F4nhAcSH+V/QTzz3p+fxydI9fLp8D4ezXcHG28tCzybR3HxVwmmHBB7LKWRDaqar12hNCvuO5bnvqx7gQ59mMbStE876lAwWbTvM2n0Z7iGH54PNaiEq2JeYEF/3v5FBvmxLy+bHzWmk/WW4YcPoIJrFhbh6e3y88bdbXf/6WLFbYcGSVaRYqrNyT3qpN1vhgT60TqyGr7cVLy8LVovF9a8XWC0WDCDneJjLKSwiK7/I9XVBEYXFThLDA2gUE+wOug2ig/D38cYwDH7acoiX5m1yL1QSF+bHoz0b0LdZbKmfe3ZBkXtBlb1Hc9l+KNsdjI6dJkSUh7eXBbu3FwaQe5pLCZSIDvalfnQQDaICqR8VhNMweOenHew47PpdCPb1ZmT72oxoV8vdy7P3aC5frU3hqzWpbEzNPOsao4LtJFQPoFZ1fxKqB7A/PY/fth9xH7OEl8W1KM6ZXkf1AB96NY2mT7MYrkys7g4IJWF/xvJ9fL02pdSlFXysXlQP9HHdAuxUD/QhzM+b3Tt3csy7Gmv2ZVD0l/bdICqIYD9vDMPVA2kAhuE6Tm5hcalhvXZvL7o1iqJv81g6N4hgW1o27y/eyVdrUty9z7EhvgxvW4t+l8USHex72uG6e47kMmPFXj77fS8HM0/f0+5j9SKhuj/1o4KoHxVEg+hA6kUFkVDNv9yhLCvfwYpdx/htxxF+23mUdftdv+8+Vi+6NY5kQIs4OtWPKNd8WsNwzUfdfDCLrQez2HIwi80Hs9l2MOu0l7vw97ESG+pHqJ+NYD8bwb7ehLi/trHziCuUZh//4MPqZaFrw0hual2TjvVdPf/puYUcySnkcFYBh4//W1DkpGeTKGqXc2hyQVExM3/fx9sLt3P95XE82L3stSwVkkyikCTnVfoeV1ja9A3sXQYnzvIIS3QFpqgmkJkCmfshY58rZGXug3zXp6T4BEF4Pdeter0/v65Wx7Wk+V+o3UhFVXbbScvKJ6+wmITqARf8WBdSYZGTb9cf4KMlu0otP18vMpBb2iTQvm44W9Oy2ZCSyfqUTDakZJCSkV/qOQJ8rPRsGk2/5rG0qxteZuheZr6DpTuOsnjbYRZtO+zucQjxs1EtwIew4/Oqwo5/HeRrI8DuTdDxoWsBdqt7GFt4oJ3qAT6nDJCG4ZrX8eOmNH7cfIhVe46V+1NmcAWqbo2i6NookuZxoRc0qBY7DT5fuY9Xv9vsfjObVCOExPCAcq0uabG4wlW9yCDqRgbiZ7O6hxdl5Ll6hDKO33IKiigscpZ5M//X5wvxs7l6o45fow1gW1q2ez7cyYT627izQ21ubZNA0GkWa9l6MIuv1qTw4+ZDWCyu+XHBvjaCfL0JOuHf2BBfaoUHkFDd/5SLjBzMzHe9Kd9xhN92HGXnCaEpzN9GdIgfMSG+RIf4EhPsS0pGPt+uSy0VLMMD7fROiiYq2JdZK/eV6tFJDA9g8BVx9L+sBjEhZUPJiX9vCpwWlu08wuJtR1i87bA7+J6O1ctCu7rhXNc8lh5Nok563tKy8vn4tz1M/W13qXYQaPcmMTyA2hGulT9rR7i+3n4oh+nL97B425+L3VQL8OH6y2twVe3qpKTnsfNwLruP5LDzSA57j+aWmeNZwsfbi7rHL0zua7Pi7WXB22rB28vr+NdeFBQVs3L3Mf7Yn1HmdyzM31bqXIf52+jbPJaBl8e5V0DNKyxm88EsNqVmsulAFhtSM9mUmklm/sl7cG1WCwnVA1yvPTyAWuGurxPDA4gMsp9x1dOSHsJPl+0p1cPp72Ml31F8yr8TFgtc3SCSke0TaVOn+kmPk+8oZvryvUz8aTupx/8+1okIIPnBTmX+higkmUQhSS6Y7DTYPM8VmHYshOLyzAOxUCpYlbrLyxWUohpDVFOIbAxRjXEE1mDuvG8rp90UF7mGGP72FviHQ++XoVrtC3tMuWD0N+fcbUzN5OPfdjN71f4zfiJfs5o/STVCuCYpmq4No046rORUcgqKsHt7nfXwoYo4llPIz1sPse9YHjkFReQWFv/5b2ER2fkOcjOOMrh9Y7o3iSEurPIXU8grLGbSoh1M/GmH+5PuE4X529yLqNSOCKBuZCB1IwOpHR54VucdXEOwCoudFDicFBQ5KShy/ZxD/XwI8vU+ZSjMzHew9WC265P9A65P+I/mFNK/RQ1uviqh0q5rdioHM/PJLSwmJsT3lKs/OoqdLNl+hG/WpvLt+gNk5JXuifOzWemdFMONreJpVSvstG+6T/f35lBWAWv2puModh4fSGHBYgEvi2sQsNXLtdhMRFD5Luyd7yjmyzUpfLRkN+tTygaSv7JYoEO9CIa0iqdbo6hT9uAUOw1S0vPYfiibrQez2Xy852bLwaxSQ/7KI6G6P1clVufK2tW4snZ1YkN82ZCayeyV+/liTUqpxWQSwwOwADuP5Jx06KzVy0Kt6v40iA6iXmTQ8V6uQGqFB5yXuZMAWw5m8emyvcxata/UsM4wfxvVA+2EB/pQPdBOVn4RP2855L6/UUwwt7dz9ejZva3kFRYzdelu3v15h7sHOyrYzt2d6nBT65onbYsKSSZRSJJKUZAN27+HTXMhKwWC4yAkzjVnKSQOQuIhuAZYbXB0JxzZ6loc4vDWP28FGSd9asMWQLotkpComnj5BLhW5bP5gc3f9a9PENRq51pcwquCfyyLCmDNJ7DoP3Bs15/bvf2g61Nw5V3gdXZvPMR8+ptz/mTmO5i9cj8f/7abXUdyqBsZ5F4AoklsMI1igy+aC+V6Urs5nF3AZyv2YfXiz9Ulq/lfNOfak5QsMvL12lQOZRXQq2k01zaLOW1PWKnHm9RuCoqK2XMkl+2Hcthx2LXq545D2ew4nEOQrzcDW8Qx+Iq4cwr7xU6Dfcdy2Xwgiz3He5uKil29kEVO11ybkh7JpjVci9mcbt5fUbGTRdsOM3vVfuavP1AqgFUP8KFRjGs4asnQ1DqRAWc1P+9cFBQVs/tILqF+NsICfE4awnYcymby4l3M/H0feQ7XhwrhgXZ6NIniu/UH3MOVa4T6cXfnOgxuGXfayzQoJJlEIUmqBMOArAOQth4OboC0DXBwPRzaXM4eKiAo1rWgRJOBruXLyzPvqTAXVn4Ii99whTsA/+rQ+i7YvQh2/uzaFtcarvsvRJQdS3zJSV0DOYehztVnNbfsnBhGhY6lvzkXhtNpmL4a1IWkdiMVoXZTMdkFRSzaegh/H28axgRd0MUTzrf03EI+WbaXD3/dxYHMP4ccx1fz4++d6zLw8rhyzbvy1JCkq+eJeAKLBYJjXLe63f7cXlyEI20zK7+bTstmDfF2FoIjDxy5rn8LcyH7AGxNdoWc395y3UJqHg9MA1yhJz8DCjJd/+ZnQH6ma/9VUyH3sOtYQTHQ9j5oORx8AsD4hytAzX8C9i2Die2h8/9B2/vBegn+6XDkw4/Pw68TAMN10eFrXnYNj7xQDm+DX8bBhi/gituh+zPm9+ilrHLNtWvQu3JqyT3qGtJa52pTr1d2oos5IIlI5Qq0e9OraYzZZVRIqL8Pf+tchzs6JDL3j1QWbT1M68Rq9G9R47wNAzTTJfhOR6QKsXpDeH0OhLbEaNobTvUJiyPfNdxv3SzXPKmMPfDrG67bmYTVgnYPwGVDwfuE8eAWC7Qc4QptXz0A25Lh+2dcb9h7/RtqtATvqn8tk3JJXQuz73L18AF42VzX05rYHq682xUefUPO3/EObYafx8G6mWAcH4ax5E3XsMxBk8AedP6OVV65R+H7sfD7B67vIxu7QlvdbhemRy37kOs1L/sfOHLANxT6v+VaHOViUuyAbQtcvbbV60JiJ6he59zPaXGRq416eUP8lZfO76qIVDqb1YvrLqvBdZd5xgdZ54tCksjFwOZ7fCnyPq7epW3JrsC09TvXUC3fEPANdv1rD/7z+4T2rt6m0/UMhcTBsM9g7XSY93+u4WaTe7mCQkRDiG4K0UmuW1RT8K/m6uXKPQp5R0v/68gFn0DXm3x70AlfB7rmPxXmuHq8CrNdS6aX3By5rjlY9uA/H1ty8w0Bv2rlm49VVOiaC3bgDzi0EQIioF4PCK9/8jelxUXw6+vw44vgdLj27/uGawXD+f+CTV//f3v3HhdlmfYB/DcwMAznkwwgomioeEzFA2pbqSlmlmW7q0sua+3rq2Gr9W5b62bax1qrbdt9a4sOm+7uqyutvaupm5Vp2errAfGEgWipiCKgAoKcmbnfP645cpIQZwb9fT+f5wPMPMD9zFwzc1/3dT/3A+x9C8heLwnD0FnX17ktyQW+/p08d5YFPvomA3E/kAT15GfAB1OAn3wIBPfo+P/5PpQCjv5DjtdSdfT2l4Rx7cPStntWANG3d87/qyyW5P7AKnneAUAXBNSWAxk/AUYvkMe6K3f6lZKK3JEMSYSrLzveHxAtj2vcD4C4O4Dg2Pb/7coi4ODfJJmtOC+3eftL8nXbRElqQ3p22qEQUTvUV8nnTkRC5w6o0Q3FJInoZuPtCwx4QLYOnsvSjEYjCUDvu4HPn5Pkq7YcKM6W7cg6275aH6CxttU/dUN4aKVjGdQdCIyWRTICuwMBBlmOvSgbKDoGXDwuyY69z58DgnsCfadIwtTrDkk6L38HbJgvUw0BuRbW9P+W61wBwKy1UgHY+gxw+Vtg43zpmE5cKlMXtTp5LDy9zV/NVcDaK9IptiaPl2Ur2C8XMLYkR/3vA37wtC356DEGWDdLzlt7fwIwOwOIGXFjH9dL3wL/ego4vVN+Du8H3PcHmWL4798D+96VCsh7dwJDfgxMeM7WoTeZgNJTQNFR85Yt0/QCoqSTHhwrj3tIL/lqapBz47JW2+Inejhw5zMy1W77C1JZ2pcOFOwFHl4NhMbd2OPvbFfOScJ5JAO4lGe73d8gyXDpKaBgn0yFPZohGyCPUfdE82DEICByCOAfYft9paRqlPlnmZpoMq8Gpw+VKZFVF4G8f8kGyKUHbpsk5y7qQ2RgQx8q37uiSmkyyfuJPsR55/k5m8kor3O/bp13jErJ68rDUwasOmP6a3WpDIhVX5aZBEEx1/83O8pkBHI3AYfWyLTx4T+Vi7l3lRgxmYD83fL5mPOxDP75RciqsQNm3PjjaKiRVXO17VsxkJrjwg1OxJMaqSPcMm6Ukg5fUbZ5OwoUH3NcGc9DKx0vSwfMN1SqQc2qRVfl+4Yac2XJv0mVKUB+r6Fafs++wlRXKb/7feiCpBJkGCArDZ7Z5bg4hlYP9BwLnN0j/1MXKOcetVYlaqyX88B2virTwtqi8bBNn2tNwv2SHEUNaX5feYEkSsXHJPGakQ4Meshxn4pC4Lsvge92QJ35N+pqa6ELi4UmIFI64wEG+epvkBFNL19JCi2rJWr1Ulncmy6JkLFe/tcPnpZz1uwrOGX5wI4VUkkDAE+dVDOvnJOFR671eLQmZiRw57NS+bB/zPO2SuJaWy7Py/1vyrl3HVFfLdc9K8+X4yjPd/xeq5fEOWG6VHQ62tFQCsj/P0nw8rbCmgRrfSQRHjob6H2XrZrbUCOJ0umvgdP/Bs5nAaqFpcf9DZI0hcXLVNtLJ2z39RgDjHxMBko8vGQg4+Q24Nvt8rdb+nsWHloofQgua8IQ/MBKaOMndOy4mzI2AlcKgLLTkgyWWr6ekveNxlogYqC0e8iP5X2gK7K8N148LtXWklz5evEE0Fgjgzfx98iATNyd3/84LVXIbzYA32yUqdUA4OUnFzfvPlyS3+6JMmDUno64UjJIc2CV/F3L+6GXL3DHfwFjn2hX/Lf5WWUyymMS1ENmMbSlsV4Std1/lAEoe2HxkrwNnQ34hV372Fzh0rcywHHkQ9vzA8h7SqP52lr97gXufe3a51mWHJe/VXulhVkY/rK6bUOV+ZqM5+Q9zfJ99SV5Dgf/UF5XUUNv3DEX58h7XO4WGUg0DJAp2ZYttHebM1bcdeEGJklO5JadXXJ7XSpuaq8ANWXmEelA54z4GRtl8YqKQvlgqCg0b+dk6pF/BGAwj8AbBkkFw75d9VXSIT3xmVTILFOUAOkcP/B2+6a2VRQCXyyXjmhjnXQ0jK1c9NLLT0ZGfUNtX/0Ncl6YYWDb/6euEvjoMZl6BwB3/0Y6RubEyHreVGfpMxGY9lrb18sqPAR8vlSqGfa0ejkey3TMkJ7ynJSftSUjZflA5QUACohNkspR77taj53yAuB/H5POPgCM/LlspkY5v8fUaPd9g6xEeOWc+eLO5+VrxXmJ0/bSBUqnNuE+4LZ72texNTYCORul41B4yHZ7z/GScA944NqdRUCe77P7gKIjUg0tyjZ3HJt8dHv7A0N+BCQ+JrHemtorwKmdUgUtPWWuaJZJVbOlCnD8FJneGNH/2m1tqJXpuGVnHBPOsnx5zNtKzuzpAuW1MPLncqHtzlJfLZdfuJgnSWVDjfkSDTGSvAT1kA5ee9+36qslCSrOtj03JTkymNMent5SGYmfLIlTYHe5rWln0lIx+maDbGWnbfd5+Ul7Wxos8o+UwZbQ3o5bcKxUtmuvSGXzwGqpUFsYBsuAiaWKHtobSH5ZBg3a0OJnVdkZWSDo8FqJAY0HEHU70Gu8bLFjbNPP6quArL/Ka8byPuwTLHFQdRHI/sg28OLpLQNKI34m7zHW1/g5x9e7byiQOFfex67n88hkkv9dVykLH9nPAqi6JK+j6ssSX+ezbL+nC5Qp7bf/RI571+vAv1+X9ybvAOCe5cCIRx2nizfUSOUp6y8yWNdZuidKsjTwQXl+WzrG8nyJ4apLMoshYmDryY1Sco3IPX+S95O2eOpkddzedwOTVzS7m0mSizBJoq6OceNESkkF5LsdMi1s0MyOX3sKkA8dY50kTY11UkXSh0jl5nqYjJKU7H2rhTs1kjT1mYDGnndg1/7DGH97X2hrLgFXi+UiyFeL5Nwfy/lejbW2FRMtiV1AFDDlt/KB2t7R6G+3A+cypWMbOUQWIGjPFKDGOumwtXcqkrEB2PGijDRfD10QEGKe9hfc0zwN0Py18oKMih7/lzxeFp46qTSG3Sb7hfSyTRn0CZQO1KH/kUrclQL5Ha2PjHwnpXVOp7++SkZui45Kh79bP0mQrneqXEMNUFOGhitFKPj4JcSVfgWNqVE6tsN/Ctz1ayAg0vF3aitkcCF3s1Sr2qoeeupkiqSlsx7Sy/a9LlCqB5l/Bkq/s/1O77uAkf8h0xG/z6qaxgZ57s5lmpOiPEmwW7uYt30bg7rLtCittzlp8ZakwvJ9fZW8T5R+13Jl2EMrFY+IBPNIen/5GhAJ5O+RAY4Tn0mHtCUaD2mHp7e0QSnb+YCADD70S5bXZvxk2e/SCeDcAemgnz8g8dFaUqrxlMSw6qLtvD+tXt7vEufKojyAVIg/X2qL/77JQPLKVgdMrJ9VkyfA69vP5Nw4y1Rdy2Pb9JIWGg95r4gcLM9XTanc7h8JjF0oSZAlrusqJVHKWi3J+PcR3lcW2Rk6S1ZvbUndVRkwO/m5vLasMxbMsx2uFTv2x9Rnovyv/tOaJyTFOcDmX0hsAlL5vf9Neb6y/irT82rLzX/LE+g3VR4fy8wJ+zbVVUqVL6iHPKfBsbbEPyhGkvjMPwM5m2xTzfUhwO0pMo259JTMTCj+RvZtmmx7+0tlPzZJEtqYRIm3Y/+UlV6Ls23HnDAdGDVPPkMsFdTiHKkgWuKs373A7HVoikmSizBJoq6OcUOtyvwA+PRZSS76TJCt910yeooOxo7JaF4ow+/6EkRnOPmFHH9NqXRMPbykI2353kMrj0VQjO1cNWvVoHv7TqA2maTTmbtZFuooPdX6vvpQ6ZzXV8rPvuHSaRj5mO1cti7AGjej+8LrqxfluAGJibFPALfPlmpU7mbpBNtXTP0iJGlrmnQG95Rq6bViymQCTn0pHbsTn9qSkIBoYNgjwPA5bS9kUV0qnej9f7Zd+82eb5icW9etrxxPxTmpOFw5J4MI7e0IW4+3m1SoIwfZKtZh8ddeWEQpWa3y5Oey5f9f8/Ml7XnqpNo06CFJVlrr6FvUV0sicSlPYvbyd7YpjpYpX4A8FomPAkN/LJ3npuoqZSrx3relQuvpLUlrYDQAJc+PUgAUjI2NOHtsD3pdzYLG0skHpHowfA7Qb5oke2d2y3X4zuxq/noKiQPGL5ZBhbam+BUekkpL9kfSsfcNN1+8vYfd6zxaEpFDa22vSZ8gYHgqMOo/ZN/L3zo+B61V/y00npK0+YY5bn6Wr+b34qaDCU2ZjPL+vf0Fab/G0zGpDYoFRvwUuP0RuTTI9bpaIoM3B1bbBm9a4uktr199qDzGTauiGk8ZDLJU4r18gWFzgDELWj9H1GQCys9I4qQLkBkaTTBJchEmSdTVMW6oTY118sHWQgWGsdPJlJIP+nOZ5mlkZ2yb/Qp14X2lajTkxy1Pa3FzzeImf48scHL+QMu/EBYvo8gJ98liG501zbYsX86TOfQ/do+vRs5VG54qI+yWBVEu5kn17kiGLQnwi5Bz1iISzIlRv7aT1cZ6SayunJfOvLFBOs3GesfvPbRAtwQZ3Q8wdM6xGhulomv5H411tq+mBiC0T/umZ16L5cLlpd9JbLb3+bp4Atj6K0lg2yMwBhiWIhWLtlZTrCiUpOnCYTmnasCM71cxbKyX5KKt11lthUz32/eubaqixkMS74pzjvsG9zQv4DNeYsX+PCBdgFSFO3MaeXmBLI5z8nNb1WjEXKDP3TfmOnQmo1R8D6yS6mO3fjJdMWKAJPthfWyvKZNR3u/O7gHO7pXN8nj5RQCj/1MSbPOg3PVgkuQiTJKoq2PcUEcxdpyotkISp8Z66ey5exWuDS3GjVJyPsz2FyQpjLpdkqKE+6WjdSM11kk1K+uvjtO3/CJkmuHF447nREQOBsakSdWFK3t1HqVkStzxLeYKn8acMGgAjQdMSiH/fBF6TJoHbd97XH/h66ZMRklG9qbb4sh6Xph5MY2w25y/ep5SMk0yKObaFShXKy+Qc0pjEjv1teWuSRKXACciIrpePoHSOb9ZaTSSdAx4QM7J6YyqRntpdXK+zKCZMj3r4N9kClVViZw0Lg2U8z/GLOhay0R3JRqNOTG+r8W7jQ0NOPrJJ4jpM9H9EiRA2tRvqmwX82Rxh5hRrl9JUaORpKMrCO7hvGv0uQEmSURERNQ+Hp7OTZCaCu0NTFouqzqe+FSWwfY3AKN+3vYKjET2uvW78RVQ6vKYJBEREVHX4ullPg9quqtbQkQ3qa47aZqIiIiIiOgGYJJERERERERkh0kSERERERGRHSZJREREREREdpgkERERERER2WGSREREREREZIdJEhERERERkR0mSURERERERHaYJBEREREREdlhkkRERERERGSHSRIREREREZEdJklERERERER2mCQRERERERHZYZJERERERERkh0kSERERERGRHSZJREREREREdpgkERERERER2WGSREREREREZEfr6gbcaEopAEBFRYWLWwI0NDSguroaFRUV8PLycnVzqItg3FBHMXaoIxg31BGMG+ooZ8eOJSew5AituemTpMrKSgBAjx49XNwSIiIiIiJyB5WVlQgKCmr1fo26VhrVxZlMJhQWFiIgIAAajcalbamoqECPHj1QUFCAwMBAl7aFug7GDXUUY4c6gnFDHcG4oY5yduwopVBZWYno6Gh4eLR+5tFNX0ny8PBATEyMq5vhIDAwkG8g9L0xbqijGDvUEYwb6gjGDXWUM2OnrQqSBRduICIiIiIissMkiYiIiIiIyA6TJCfS6XRYtmwZdDqdq5tCXQjjhjqKsUMdwbihjmDcUEe5a+zc9As3EBERERERfR+sJBEREREREdlhkkRERERERGSHSRIREREREZEdJklERERERER2mCQ50VtvvYVevXrBx8cHo0ePxv79+13dJHIjK1euxMiRIxEQEICIiAjMmDEDeXl5DvvU1tYiLS0NYWFh8Pf3x8yZM1FcXOyiFpM7evnll6HRaLB48WLrbYwbasn58+fxyCOPICwsDHq9HoMHD8aBAwes9yul8PzzzyMqKgp6vR6TJk3CyZMnXdhicgdGoxFLly5FXFwc9Ho9+vTpgxUrVsB+HTDGDn399deYPn06oqOjodFosHHjRof72xMjpaWlSElJQWBgIIKDg/HYY4/h6tWrTjsGJklO8uGHH+Kpp57CsmXLcPDgQQwdOhRTpkxBSUmJq5tGbmLnzp1IS0vD3r17sW3bNjQ0NGDy5Mmoqqqy7vPkk09i8+bNWL9+PXbu3InCwkI89NBDLmw1uZPMzEy8++67GDJkiMPtjBtqqqysDOPGjYOXlxe2bt2KnJwc/P73v0dISIh1n1dffRVvvPEG3nnnHezbtw9+fn6YMmUKamtrXdhycrVXXnkF6enp+NOf/oTc3Fy88sorePXVV/Hmm29a92HsUFVVFYYOHYq33nqrxfvbEyMpKSn45ptvsG3bNmzZsgVff/015s2b56xDABQ5xahRo1RaWpr1Z6PRqKKjo9XKlStd2CpyZyUlJQqA2rlzp1JKqfLycuXl5aXWr19v3Sc3N1cBUHv27HFVM8lNVFZWqvj4eLVt2zZ15513qkWLFimlGDfUsmeeeUaNHz++1ftNJpOKjIxUv/vd76y3lZeXK51Op9atW+eMJpKbmjZtmnr00UcdbnvooYdUSkqKUoqxQ80BUBs2bLD+3J4YycnJUQBUZmamdZ+tW7cqjUajzp8/75R2s5LkBPX19cjKysKkSZOst3l4eGDSpEnYs2ePC1tG7uzKlSsAgNDQUABAVlYWGhoaHOKof//+iI2NZRwR0tLSMG3aNIf4ABg31LJNmzYhMTERP/zhDxEREYFhw4bh/ffft95/+vRpFBUVOcRNUFAQRo8ezbi5xY0dOxbbt2/HiRMnAABHjhzBrl27MHXqVACMHbq29sTInj17EBwcjMTEROs+kyZNgoeHB/bt2+eUdmqd8l9ucZcuXYLRaITBYHC43WAw4Pjx4y5qFbkzk8mExYsXY9y4cRg0aBAAoKioCN7e3ggODnbY12AwoKioyAWtJHeRkZGBgwcPIjMzs9l9jBtqyalTp5Ceno6nnnoKS5YsQWZmJn7xi1/A29sbqamp1tho6XOLcXNre/bZZ1FRUYH+/fvD09MTRqMRL730ElJSUgCAsUPX1J4YKSoqQkREhMP9Wq0WoaGhTosjJklEbigtLQ3Hjh3Drl27XN0UcnMFBQVYtGgRtm3bBh8fH1c3h7oIk8mExMRE/Pa3vwUADBs2DMeOHcM777yD1NRUF7eO3Nk//vEPrF27Fn//+98xcOBAHD58GIsXL0Z0dDRjh24qnG7nBOHh4fD09Gy2mlRxcTEiIyNd1CpyVwsXLsSWLVvw5ZdfIiYmxnp7ZGQk6uvrUV5e7rA/4+jWlpWVhZKSEgwfPhxarRZarRY7d+7EG2+8Aa1WC4PBwLihZqKiojBgwACH2xISEnD27FkAsMYGP7eoqaeffhrPPvssZs2ahcGDB2POnDl48sknsXLlSgCMHbq29sRIZGRks8XNGhsbUVpa6rQ4YpLkBN7e3hgxYgS2b99uvc1kMmH79u1ISkpyYcvInSilsHDhQmzYsAE7duxAXFycw/0jRoyAl5eXQxzl5eXh7NmzjKNb2MSJE5GdnY3Dhw9bt8TERKSkpFi/Z9xQU+PGjWt2iYETJ06gZ8+eAIC4uDhERkY6xE1FRQX27dvHuLnFVVdXw8PDsfvo6ekJk8kEgLFD19aeGElKSkJ5eTmysrKs++zYsQMmkwmjR492TkOdsjwEqYyMDKXT6dRf/vIXlZOTo+bNm6eCg4NVUVGRq5tGbmLBggUqKChIffXVV+rChQvWrbq62rrP/PnzVWxsrNqxY4c6cOCASkpKUklJSS5sNbkj+9XtlGLcUHP79+9XWq1WvfTSS+rkyZNq7dq1ytfXV61Zs8a6z8svv6yCg4PVxx9/rI4ePaoeeOABFRcXp2pqalzYcnK11NRU1b17d7VlyxZ1+vRp9c9//lOFh4erX/3qV9Z9GDtUWVmpDh06pA4dOqQAqNdff10dOnRI5efnK6XaFyPJyclq2LBhat++fWrXrl0qPj5ezZ4922nHwCTJid58800VGxurvL291ahRo9TevXtd3SRyIwBa3FavXm3dp6amRj3++OMqJCRE+fr6qgcffFBduHDBdY0mt9Q0SWLcUEs2b96sBg0apHQ6nerfv7967733HO43mUxq6dKlymAwKJ1OpyZOnKjy8vJc1FpyFxUVFWrRokUqNjZW+fj4qN69e6vf/OY3qq6uzroPY4e+/PLLFvs0qampSqn2xcjly5fV7Nmzlb+/vwoMDFRz585VlZWVTjsGjVJ2l0gmIiIiIiK6xfGcJCIiIiIiIjtMkoiIiIiIiOwwSSIiIiIiIrLDJImIiIiIiMgOkyQiIiIiIiI7TJKIiIiIiIjsMEkiIiIiIiKywySJiIiIiIjIDpMkIiIiOxqNBhs3bnR1M4iIyIWYJBERkdv42c9+Bo1G02xLTk52ddOIiOgWonV1A4iIiOwlJydj9erVDrfpdDoXtYaIiG5FrCQREZFb0el0iIyMdNhCQkIAyFS49PR0TJ06FXq9Hr1798ZHH33k8PvZ2dmYMGEC9Ho9wsLCMG/ePFy9etVhn1WrVmHgwIHQ6XSIiorCwoULHe6/dOkSHnzwQfj6+iI+Ph6bNm2y3ldWVoaUlBR069YNer0e8fHxzZI6IiLq2pgkERFRl7J06VLMnDkTR44cQUpKCmbNmoXc3FwAQFVVFaZMmYKQkBBkZmZi/fr1+OKLLxySoPT0dKSlpWHevHnIzs7Gpk2bcNtttzn8jxdeeAE/+tGPcPToUdx7771ISUlBaWmp9f/n5ORg69atyM3NRXp6OsLDw533ABAR0Q2nUUopVzeCiIgIkHOS1qxZAx8fH4fblyxZgiVLlkCj0WD+/PlIT0+33jdmzBgMHz4cb7/9Nt5//30888wzKCgogJ+fHwDgk08+wfTp01FYWAiDwYDu3btj7ty5ePHFF1tsg0ajwXPPPYcVK1YAkMTL398fW7duRXJyMu6//36Eh4dj1apVN+hRICIiV+M5SURE5FbuvvtuhyQIAEJDQ63fJyUlOdyXlJSEw4cPAwByc3MxdOhQa4IEAOPGjYPJZEJeXh40Gg0KCwsxceLENtswZMgQ6/d+fn4IDAxESUkJAGDBggWYOXMmDh48iMmTJ2PGjBkYO3Zsh46ViIjcE5MkIiJyK35+fs2mv3UWvV7frv28vLwcftZoNDCZTACAqVOnIj8/H5988gm2bduGiRMnIi0tDa+99lqnt5eIiFyD5yQREVGXsnfv3mY/JyQkAAASEhJw5MgRVFVVWe/fvXs3PDw80K9fPwQEBKBXr17Yvn37dbWhW7duSE1NxZo1a/DHP/4R77333nX9PSIici+sJBERkVupq6tDUVGRw21arda6OML69euRmJiI8ePHY+3atdi/fz8++OADAEBKSgqWLVuG1NRULF++HBcvXsQTTzyBOXPmwGAwAACWL1+O+fPnIyIiAlOnTkVlZSV2796NJ554ol3te/755zFixAgMHDgQdXV12LJlizVJIyKimwOTJCIiciuffvopoqKiHG7r168fjh8/DkBWnsvIyMDjjz+OqKgorFu3DgMGDAAA+Pr64rPPPsOiRYswcuRI+Pr6YubMmXj99detfys1NRW1tbX4wx/+gF/+8pcIDw/Hww8/3O72eXt749e//jXOnDkDvV6PO+64AxkZGZ1w5ERE5C64uh0REXUZGo0GGzZswIwZM1zdFCIiuonxnCQiIiIiIiI7TJKIiIiIiIjs8JwkIiLqMjhDnIiInIGVJCIiIiIiIjtMkoiIiIiIiOwwSSIiIiIiIrLDJImIiIiIiMgOkyQiIiIiIiI7TJKIiIiIiIjsMEkiIiIiIiKywySJiIiIiIjIzv8DoClcpGCrGi4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'lr':[0.001,0.005,0.01],\n",
        "    'hidden_dim':[64,128],\n",
        "    'dropout_rate':[0.3,0.5]\n",
        "\n",
        "}\n",
        "\n",
        "# Generate all combinations of hyperparameters\n",
        "param_combinations = list(itertools.product(param_grid['lr'], param_grid['hidden_dim'], param_grid['dropout_rate']))"
      ],
      "metadata": {
        "id": "ZKd9IaUWYqp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_evaluate(input_dim, hidden_dim, output_dim, dropout_rate, lr, num_epochs=100, k=5):\n",
        "    # Initialize KFold cross-validation\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "    # Criterion and optimizer setup\n",
        "    criterion = torch.nn.L1Loss()\n",
        "\n",
        "    # Store losses for cross-validation\n",
        "    all_training_losses = []\n",
        "    all_val_losses = []\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_index, val_index) in enumerate(kf.split(train_dfx)):\n",
        "        print(f'Fold {fold + 1}/{k}')\n",
        "\n",
        "        # Split training data into fold-specific training and validation sets\n",
        "        X_train, X_val = train_dfx[train_index], train_dfx[val_index]\n",
        "        y_train, y_val = train_dfy[train_index], train_dfy[val_index]\n",
        "\n",
        "        # Create TensorDatasets and DataLoaders for this fold\n",
        "        train_dataset = TensorDataset(X_train, y_train)\n",
        "        val_dataset = TensorDataset(X_val, y_val)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "        # Initialize the model with the current hyperparameters\n",
        "        model3 = MLPRegressionModel3(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout_rate=dropout_rate)\n",
        "        optimizer = torch.optim.Adam(model3.parameters(), lr=lr)\n",
        "\n",
        "        # Lists to store fold-specific training and validation losses\n",
        "        fold_train_losses = []\n",
        "        fold_val_losses = []\n",
        "\n",
        "        for epoch in range(num_epochs):  # Train for a certain number of epochs\n",
        "            model3.train()\n",
        "            running_loss = 0.0\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model3(inputs)\n",
        "                loss = criterion(outputs.squeeze(), targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            avg_train_loss = running_loss / len(train_loader)\n",
        "            fold_train_losses.append(avg_train_loss)\n",
        "\n",
        "            # Validation step\n",
        "            model3.eval()\n",
        "            val_loss = 0.0\n",
        "            with torch.no_grad():\n",
        "                for inputs, targets in val_loader:\n",
        "                    outputs = model3(inputs)\n",
        "                    loss = criterion(outputs.squeeze(), targets)\n",
        "                    val_loss += loss.item()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            fold_val_losses.append(avg_val_loss)\n",
        "            fold_results.append(avg_val_loss)\n",
        "\n",
        "        # Append fold losses to overall results\n",
        "        all_training_losses.append(fold_train_losses)\n",
        "        all_val_losses.append(fold_val_losses)\n",
        "\n",
        "    # Average validation MAE across all folds\n",
        "    mean_val_loss = np.mean(fold_results)\n",
        "    print(f'Average Validation MAE across {k} folds: {mean_val_loss:.4f}')\n",
        "\n",
        "    return mean_val_loss  # Return validation loss for hyperparameter tuning"
      ],
      "metadata": {
        "id": "T8aeRq1OZWzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hypertune(input_dim, output_dim, num_epochs=100, k=5):\n",
        "    best_params = None\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Iterate over all combinations of hyperparameters\n",
        "    for lr in param_grid['lr']:\n",
        "        for hidden_dim in param_grid['hidden_dim']:\n",
        "            for dropout_rate in param_grid['dropout_rate']:\n",
        "                print(f'Testing with lr={lr}, hidden_dim={hidden_dim}, dropout_rate={dropout_rate}')\n",
        "\n",
        "                # Call the train_and_evaluate function with the current hyperparameters\n",
        "                val_loss = train_and_evaluate(input_dim, hidden_dim, output_dim, dropout_rate, lr, num_epochs=num_epochs, k=k)\n",
        "\n",
        "                # Track the best hyperparameters based on validation loss\n",
        "                if val_loss < best_val_loss:\n",
        "                    best_val_loss = val_loss\n",
        "                    best_params = (lr, hidden_dim, dropout_rate)\n",
        "\n",
        "    print(f'Best Hyperparameters: lr={best_params[0]}, hidden_dim={best_params[1]}, dropout_rate={best_params[2]}')\n",
        "    print(f'Best Validation Loss: {best_val_loss:.4f}')\n",
        "    return best_params"
      ],
      "metadata": {
        "id": "GULfAmTCc6xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 5\n",
        "output_dim = 1\n",
        "\n",
        "# Run the hypertune function to find the best hyperparameters\n",
        "best_params = hypertune(input_dim, output_dim, num_epochs=100, k=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_Y2r93UdFH7",
        "outputId": "d9290617-05eb-481d-dbc7-bc59400718a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing with lr=0.001, hidden_dim=64, dropout_rate=0.3\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "Average Validation MAE across 5 folds: 6.4489\n",
            "Testing with lr=0.001, hidden_dim=64, dropout_rate=0.5\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "Average Validation MAE across 5 folds: 6.8920\n",
            "Testing with lr=0.001, hidden_dim=128, dropout_rate=0.3\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "Average Validation MAE across 5 folds: 6.0173\n",
            "Testing with lr=0.001, hidden_dim=128, dropout_rate=0.5\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "Average Validation MAE across 5 folds: 6.3291\n",
            "Testing with lr=0.005, hidden_dim=64, dropout_rate=0.3\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "Average Validation MAE across 5 folds: 5.7747\n",
            "Testing with lr=0.005, hidden_dim=64, dropout_rate=0.5\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "Average Validation MAE across 5 folds: 5.9738\n",
            "Testing with lr=0.005, hidden_dim=128, dropout_rate=0.3\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "Average Validation MAE across 5 folds: 5.6559\n",
            "Testing with lr=0.005, hidden_dim=128, dropout_rate=0.5\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "Average Validation MAE across 5 folds: 5.7873\n",
            "Testing with lr=0.01, hidden_dim=64, dropout_rate=0.3\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "Average Validation MAE across 5 folds: 5.7054\n",
            "Testing with lr=0.01, hidden_dim=64, dropout_rate=0.5\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "Average Validation MAE across 5 folds: 5.8713\n",
            "Testing with lr=0.01, hidden_dim=128, dropout_rate=0.3\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "Average Validation MAE across 5 folds: 5.6329\n",
            "Testing with lr=0.01, hidden_dim=128, dropout_rate=0.5\n",
            "Fold 1/5\n",
            "Fold 2/5\n",
            "Fold 3/5\n",
            "Fold 4/5\n",
            "Fold 5/5\n",
            "Average Validation MAE across 5 folds: 5.7383\n",
            "Best Hyperparameters: lr=0.01, hidden_dim=128, dropout_rate=0.3\n",
            "Best Validation Loss: 5.6329\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3.eval()  # Set the model to evaluation mode\n",
        "\n",
        "# Initialize variables to accumulate loss\n",
        "total_loss = 0\n",
        "n_batches = 0\n",
        "\n",
        "all_actuals = []\n",
        "all_predictions = []\n",
        "all_differences = []\n",
        "\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    for inputs, targets in testloader:\n",
        "        # Forward pass: Compute predicted outputs by passing inputs to the model\n",
        "        outputs = model3(inputs)\n",
        "        loss = criterion(outputs, targets.view(-1, 1))\n",
        "        # Compute the loss\n",
        "        differences = targets.view(-1,1) - outputs\n",
        "\n",
        "        all_actuals.extend(targets.detach().cpu().numpy())\n",
        "        all_predictions.extend(outputs.detach().cpu().numpy())\n",
        "        all_differences.extend(differences.detach().cpu().numpy())\n",
        "        # Accumulate loss\n",
        "        total_loss += loss.item()\n",
        "        n_batches += 1\n",
        "\n",
        "\n",
        "all_actuals_tensor = torch.tensor(all_actuals, dtype=torch.float32)\n",
        "all_predictions_tensor = torch.tensor(all_predictions, dtype=torch.float32)\n",
        "\n",
        "mse_loss = torch.nn.MSELoss()\n",
        "mse = mse_loss(all_predictions_tensor, all_actuals_tensor.view(-1, 1))\n",
        "print(f'Mean Squared Error (MSE) using nn.MSELoss: {mse.item():.4f}')\n",
        "\n",
        "mae_loss = torch.nn.L1Loss()\n",
        "mae=mae_loss(all_predictions_tensor,all_actuals_tensor.view(-1,1))\n",
        "print(f'Mean Absolute Error (MAE) using nn.L1Loss: {mae.item():.4f}')\n",
        "\n",
        "\n",
        "rmse = mean_squared_error(all_predictions_tensor,all_actuals_tensor.view(-1,1), squared=False)\n",
        "print(f'Root Mean Squared Error (rmse): {rmse.item():.4f}')\n",
        "\n",
        "\n",
        "\n",
        "mape = mean_absolute_percentage_error(all_predictions_tensor,all_actuals_tensor.view(-1,1))\n",
        "print(f'Mean Absolute Percentage Error (MAE): {mape.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIbFN_RDsROw",
        "outputId": "f3848d5c-2db8-4cee-e567-4e80dabf6b94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE) using nn.MSELoss: 79.6893\n",
            "Mean Absolute Error (MAE) using nn.L1Loss: 5.5760\n",
            "Root Mean Squared Error (rmse): 8.9269\n",
            "Mean Absolute Percentage Error (MAE): 0.3458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8 Conclusion\n",
        "Comparing between the baseline model, The Graph Neural Network and the Multilayer Perceptron Network, The Multilayer Perceptron\n",
        "\n",
        "The graph neural network has shown similar results to that of the baseline model even after hypertuning the parameters and the Multilayer Perceptron Network could predict the IPO_Prices much better than the two models stated above.\n",
        "\n",
        "As seen from the evaluation metrics and the graphs, the Multilayer Perceptron has shown the best results which are displayed below:\n",
        "\n",
        "----MLP----\n",
        "\n",
        "Mean Squared Error: 79.6893\n",
        "\n",
        "Mean Absolute Error: 5.5760\n",
        "\n",
        "Root Mean Squared Error (RMSE): 8.9269\n",
        "\n",
        "Mean Absolute Percentage Error (MAPE): 0.3458\n",
        "\n",
        "\n",
        "---GNN---\n",
        "\n",
        "Mean Squared Error: 99.0291\n",
        "\n",
        "Mean Absolute Error: 6.3832\n",
        "\n",
        "Root Mean Squared Error (RMSE): 9.9513\n",
        "\n",
        "Mean Absolute Percentage Error (MAPE): 0.39\n",
        "\n",
        "---Baseline---\n",
        "\n",
        "Mean Squared Error: 84.45\n",
        "\n",
        "Mean Absolute Error: 6.40\n",
        "\n",
        "Root Mean Squared Error (RMSE): 9.18\n",
        "\n",
        "Mean Absolute Percentage Error (MAPE): 0.4531\n",
        "\n"
      ],
      "metadata": {
        "id": "RN-Vg8EYXPYp"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}